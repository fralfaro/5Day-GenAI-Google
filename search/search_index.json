{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"5-Day Gen AI Intensive Course with Google","text":"<p>Welcome to our 5-Day Gen AI Intensive Course with Google! This was a live event from November 11\u201315, 2024, now made available as a self-paced learning guide for anyone interested in learning the fundamental technologies and techniques behind Generative AI.</p> <p>Our 2025 new course is currently open for registration! \ud83d\udc49 Register here</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<p>Day 1 \u2013 Foundations</p><p>Learn about LLMs and prompt engineering techniques.</p> <p>Day 2 \u2013 Embeddings</p><p>Explore embeddings and vector databases in Gen AI.</p> <p>Day 3 \u2013 AI Agents</p><p>Build agents with LLMs and real-world tools.</p> <p>Day 4 \u2013 LLMs</p><p>Create and fine-tune specialized models.</p> <p>Day 5 \u2013 MLOps</p><p>Deploy Gen AI apps using MLOps and Vertex AI.</p>"},{"location":"__init__/","title":"init","text":""},{"location":"day_01/","title":"Day 1 \u2013 Foundational Large Language Models &amp; Prompt Engineering","text":"<p>Explore the evolution of LLMs, from transformers to techniques like fine-tuning and inference acceleration. Practice prompt engineering for optimal interaction with LLMs.</p> <p>Assignments:</p> <ol> <li> <p>Intro Unit: \u201cFoundational Large Language Models\u201d </p> <ul> <li>\ud83c\udfa7 Podcast (Optional) </li> <li>\ud83d\udcc4 Whitepaper</li> </ul> </li> <li> <p>Unit 1: \u201cPrompt Engineering\u201d </p> <ul> <li>\ud83c\udfa7 Podcast (Optional) </li> <li>\ud83d\udcc4 Whitepaper </li> <li>\ud83d\udcbb Code lab (Phone verification required) </li> <li>\ud83e\udde0 Case Study (Optional)</li> </ul> </li> <li> <p>\ud83c\udfa5 YouTube Livestream Recording</p> </li> </ol>"},{"location":"day_01/day-1-prompting/","title":"Day 1 - Prompting","text":"In\u00a0[2]: Copied! <pre># @title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # @title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <p>To run this notebook, as well as the others in this course, you will need to make a copy, or fork, the notebook. Look for the <code>Copy and Edit</code> button in the top-right, and click it to make an editable, private copy of the notebook. It should look like this one:</p> <p></p> <p>Your copy will now have a \u25b6\ufe0f Run button next to each code cell that you can press to execute that cell. These notebooks are expected to be run in order from top-to-bottom, but you are encouraged to add new cells, run your own code and explore. If you get stuck, you can try the <code>Factory reset</code> option in the <code>Run</code> menu, or head back to the original notebook and make a fresh copy.</p> <p></p> <p>Next, you will need to add your API key to your Kaggle Notebook as a Kaggle User Secret.</p> <p> </p> In\u00a0[3]: Copied! <pre>!pip uninstall -qqy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n!pip install -U -q \"google-genai==1.7.0\"\n</pre> !pip uninstall -qqy jupyterlab  # Remove unused packages from Kaggle's base image that conflict !pip install -U -q \"google-genai==1.7.0\" <p>Import the SDK and some helpers for rendering the output.</p> In\u00a0[4]: Copied! <pre>from google import genai\nfrom google.genai import types\n\nfrom IPython.display import HTML, Markdown, display\n</pre> from google import genai from google.genai import types  from IPython.display import HTML, Markdown, display <p>Set up a retry helper. This allows you to \"Run all\" without worrying about per-minute quota.</p> In\u00a0[5]: Copied! <pre>from google.api_core import retry\n\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\ngenai.models.Models.generate_content = retry.Retry(\n    predicate=is_retriable)(genai.models.Models.generate_content)\n</pre> from google.api_core import retry   is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})  genai.models.Models.generate_content = retry.Retry(     predicate=is_retriable)(genai.models.Models.generate_content) In\u00a0[6]: Copied! <pre>from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n</pre> from kaggle_secrets import UserSecretsClient  GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\") <p>If you received an error response along the lines of <code>No user secrets exist for kernel id ...</code>, then you need to add your API key via <code>Add-ons</code>, <code>Secrets</code> and enable it.</p> <p></p> In\u00a0[7]: Copied! <pre>client = genai.Client(api_key=GOOGLE_API_KEY)\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=\"Explain AI to me like I'm a kid.\")\n\nprint(response.text)\n</pre> client = genai.Client(api_key=GOOGLE_API_KEY)  response = client.models.generate_content(     model=\"gemini-2.0-flash\",     contents=\"Explain AI to me like I'm a kid.\")  print(response.text) <pre>Okay, imagine you have a really, REALLY smart robot. But instead of being made of metal and wires, this smartness lives inside a computer!\n\nThis smartness is called Artificial Intelligence, or AI for short.\n\nThink of it like this:\n\n*   **A puppy learning tricks:** You teach a puppy by showing it what to do and rewarding it when it does it right. AI learns in a similar way, but with lots and lots of examples and by getting \"rewards\" for making the right choices.\n*   **A super-powered calculator:** A calculator can do math really fast. AI is like a super-powered calculator that can do lots more than just math! It can see pictures, understand words, and even play games.\n*   **Playing hide-and-seek:** If you play hide-and-seek a lot, you get better at finding good hiding spots. AI is the same. The more it \"plays\" (or works), the better it gets at things!\n\n**So, what can AI do?**\n\n*   **Help you find videos you like:** Have you ever noticed that YouTube suggests videos you might enjoy? That's AI! It's learned what kind of videos you watch and tries to guess what you'll like next.\n*   **Help you spell words:** When you're typing and you misspell a word, sometimes your computer fixes it. That's AI helping you!\n*   **Help doctors find diseases:** AI can look at X-rays and other medical images to help doctors find problems that might be hard to see with their own eyes.\n*   **Drive cars:** Some cars are starting to drive themselves using AI! They can see the road, other cars, and even pedestrians.\n\n**AI is still learning and growing, just like you!** It's a tool that can help us do amazing things, but it's important to remember that it's not perfect. We still need people to help it learn and make sure it's used for good.\n\nDoes that make sense? Do you have any other questions about AI?\n\n</pre> <p>The response often comes back in markdown format, which you can render directly in this notebook.</p> In\u00a0[8]: Copied! <pre>Markdown(response.text)\n</pre> Markdown(response.text) Out[8]: <p>Okay, imagine you have a really, REALLY smart robot. But instead of being made of metal and wires, this smartness lives inside a computer!</p> <p>This smartness is called Artificial Intelligence, or AI for short.</p> <p>Think of it like this:</p> <ul> <li>A puppy learning tricks: You teach a puppy by showing it what to do and rewarding it when it does it right. AI learns in a similar way, but with lots and lots of examples and by getting \"rewards\" for making the right choices.</li> <li>A super-powered calculator: A calculator can do math really fast. AI is like a super-powered calculator that can do lots more than just math! It can see pictures, understand words, and even play games.</li> <li>Playing hide-and-seek: If you play hide-and-seek a lot, you get better at finding good hiding spots. AI is the same. The more it \"plays\" (or works), the better it gets at things!</li> </ul> <p>So, what can AI do?</p> <ul> <li>Help you find videos you like: Have you ever noticed that YouTube suggests videos you might enjoy? That's AI! It's learned what kind of videos you watch and tries to guess what you'll like next.</li> <li>Help you spell words: When you're typing and you misspell a word, sometimes your computer fixes it. That's AI helping you!</li> <li>Help doctors find diseases: AI can look at X-rays and other medical images to help doctors find problems that might be hard to see with their own eyes.</li> <li>Drive cars: Some cars are starting to drive themselves using AI! They can see the road, other cars, and even pedestrians.</li> </ul> <p>AI is still learning and growing, just like you! It's a tool that can help us do amazing things, but it's important to remember that it's not perfect. We still need people to help it learn and make sure it's used for good.</p> <p>Does that make sense? Do you have any other questions about AI?</p> In\u00a0[9]: Copied! <pre>chat = client.chats.create(model='gemini-2.0-flash', history=[])\nresponse = chat.send_message('Hello! My name is Zlork.')\nprint(response.text)\n</pre> chat = client.chats.create(model='gemini-2.0-flash', history=[]) response = chat.send_message('Hello! My name is Zlork.') print(response.text) <pre>Nice to meet you, Zlork! It's great to have you here. How can I help you today?\n\n</pre> In\u00a0[10]: Copied! <pre>response = chat.send_message('Can you tell me something interesting about dinosaurs?')\nprint(response.text)\n</pre> response = chat.send_message('Can you tell me something interesting about dinosaurs?') print(response.text) <pre>Okay, here's something interesting about dinosaurs:\n\n**There's evidence that some dinosaurs were brightly colored, possibly with iridescent feathers, and even different colors within the same species, similar to modern birds!**\n\nFor a long time, scientists believed dinosaurs were largely drab in color, like reptiles. However, through analyzing fossilized feathers and scales, and even the microscopic structures called melanosomes (which contain pigment), researchers have been able to infer the colors of some dinosaurs.\n\nThis has led to the discovery that some dinosaurs might have had brilliant hues, stripes, spots, and even iridescent sheens like a hummingbird. The *Microraptor*, for example, is thought to have had iridescent black feathers. And the *Sinosauropteryx* is believed to have had reddish-brown stripes.\n\nThis discovery changes our understanding of dinosaur behavior, suggesting that color may have played a role in camouflage, communication, and courtship, just like it does in modern birds.\n\nSo, the next time you picture a dinosaur, don't just think of a giant, green lizard. Imagine it possibly strutting around with vibrant, eye-catching colors!\n\n</pre> <p>While you have the <code>chat</code> object alive, the conversation state persists. Confirm that by asking if it knows the user's name.</p> In\u00a0[11]: Copied! <pre>response = chat.send_message('Do you remember what my name is?')\nprint(response.text)\n</pre> response = chat.send_message('Do you remember what my name is?') print(response.text) <pre>Yes, your name is Zlork.\n\n</pre> In\u00a0[12]: Copied! <pre>for model in client.models.list():\n  print(model.name)\n</pre> for model in client.models.list():   print(model.name) <pre>models/chat-bison-001\nmodels/text-bison-001\nmodels/embedding-gecko-001\nmodels/gemini-1.0-pro-vision-latest\nmodels/gemini-pro-vision\nmodels/gemini-1.5-pro-latest\nmodels/gemini-1.5-pro-001\nmodels/gemini-1.5-pro-002\nmodels/gemini-1.5-pro\nmodels/gemini-1.5-flash-latest\nmodels/gemini-1.5-flash-001\nmodels/gemini-1.5-flash-001-tuning\nmodels/gemini-1.5-flash\nmodels/gemini-1.5-flash-002\nmodels/gemini-1.5-flash-8b\nmodels/gemini-1.5-flash-8b-001\nmodels/gemini-1.5-flash-8b-latest\nmodels/gemini-1.5-flash-8b-exp-0827\nmodels/gemini-1.5-flash-8b-exp-0924\nmodels/gemini-2.5-pro-exp-03-25\nmodels/gemini-2.0-flash-exp\nmodels/gemini-2.0-flash\nmodels/gemini-2.0-flash-001\nmodels/gemini-2.0-flash-exp-image-generation\nmodels/gemini-2.0-flash-lite-001\nmodels/gemini-2.0-flash-lite\nmodels/gemini-2.0-flash-lite-preview-02-05\nmodels/gemini-2.0-flash-lite-preview\nmodels/gemini-2.0-pro-exp\nmodels/gemini-2.0-pro-exp-02-05\nmodels/gemini-exp-1206\nmodels/gemini-2.0-flash-thinking-exp-01-21\nmodels/gemini-2.0-flash-thinking-exp\nmodels/gemini-2.0-flash-thinking-exp-1219\nmodels/learnlm-1.5-pro-experimental\nmodels/gemma-3-27b-it\nmodels/embedding-001\nmodels/text-embedding-004\nmodels/gemini-embedding-exp-03-07\nmodels/gemini-embedding-exp\nmodels/aqa\nmodels/imagen-3.0-generate-002\n</pre> <p>The <code>models.list</code> response also returns additional information about the model's capabilities, like the token limits and supported parameters.</p> In\u00a0[13]: Copied! <pre>from pprint import pprint\n\nfor model in client.models.list():\n  if model.name == 'models/gemini-2.0-flash':\n    pprint(model.to_json_dict())\n    break\n</pre> from pprint import pprint  for model in client.models.list():   if model.name == 'models/gemini-2.0-flash':     pprint(model.to_json_dict())     break <pre>{'description': 'Gemini 2.0 Flash',\n 'display_name': 'Gemini 2.0 Flash',\n 'input_token_limit': 1048576,\n 'name': 'models/gemini-2.0-flash',\n 'output_token_limit': 8192,\n 'supported_actions': ['generateContent', 'countTokens'],\n 'tuned_model_info': {},\n 'version': '2.0'}\n</pre> In\u00a0[14]: Copied! <pre>from google.genai import types\n\nshort_config = types.GenerateContentConfig(max_output_tokens=200)\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=short_config,\n    contents='Write a 1000 word essay on the importance of olives in modern society.')\n\nprint(response.text)\n</pre> from google.genai import types  short_config = types.GenerateContentConfig(max_output_tokens=200)  response = client.models.generate_content(     model='gemini-2.0-flash',     config=short_config,     contents='Write a 1000 word essay on the importance of olives in modern society.')  print(response.text) <pre>## The Humble Olive: A Cornerstone of Modern Society\n\nThe olive, a small, often brine-soaked fruit, may seem insignificant amidst the complexities of modern society. Yet, beneath its unassuming exterior lies a profound importance, extending far beyond its culinary applications. From its historical roots to its contributions to health, economy, and culture, the olive tree and its fruit have woven themselves into the fabric of modern life, demonstrating an enduring relevance that belies its perceived simplicity.\n\nThe story of the olive is deeply intertwined with the history of civilization. Originating in the Mediterranean basin thousands of years ago, the olive tree, *Olea europaea*, quickly became a cornerstone of ancient economies and societies. Olive oil was not merely a foodstuff; it was a source of fuel for lamps, a component of religious rituals, a base for perfumes and medicines, and a symbol of peace and prosperity. Ancient Greece valued the olive tree so highly that it was considered sacred to the goddess Athena, and victors in\n</pre> In\u00a0[15]: Copied! <pre>response = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=short_config,\n    contents='Write a short poem on the importance of olives in modern society.')\n\nprint(response.text)\n</pre> response = client.models.generate_content(     model='gemini-2.0-flash',     config=short_config,     contents='Write a short poem on the importance of olives in modern society.')  print(response.text) <pre>From groves sun-drenched and ancient roots,\nThe olive's fruit, a bounty shoots.\nPressed into oil, a golden stream,\nNourishing bodies, fulfilling dream.\n\nOn pizzas scattered, in tapenade's art,\nA Mediterranean piece, a vital part.\nA symbol of peace, a culinary grace,\nThe humble olive finds its modern place.\n\n</pre> <p>Explore with your own prompts. Try a prompt with a restrictive output limit and then adjust the prompt to work within that limit.</p> In\u00a0[16]: Copied! <pre>high_temp_config = types.GenerateContentConfig(temperature=2.0)\n\n\nfor _ in range(5):\n  response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      config=high_temp_config,\n      contents='Pick a random colour... (respond in a single word)')\n\n  if response.text:\n    print(response.text, '-' * 25)\n</pre> high_temp_config = types.GenerateContentConfig(temperature=2.0)   for _ in range(5):   response = client.models.generate_content(       model='gemini-2.0-flash',       config=high_temp_config,       contents='Pick a random colour... (respond in a single word)')    if response.text:     print(response.text, '-' * 25) <pre>Purple\n -------------------------\nMagenta\n -------------------------\nCerulean\n -------------------------\nMagenta\n -------------------------\nOrange\n -------------------------\n</pre> <p>Now try the same prompt with temperature set to zero. Note that the output is not completely deterministic, as other parameters affect token selection, but the results will tend to be more stable.</p> In\u00a0[17]: Copied! <pre>low_temp_config = types.GenerateContentConfig(temperature=0.0)\n\nfor _ in range(5):\n  response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      config=low_temp_config,\n      contents='Pick a random colour... (respond in a single word)')\n\n  if response.text:\n    print(response.text, '-' * 25)\n</pre> low_temp_config = types.GenerateContentConfig(temperature=0.0)  for _ in range(5):   response = client.models.generate_content(       model='gemini-2.0-flash',       config=low_temp_config,       contents='Pick a random colour... (respond in a single word)')    if response.text:     print(response.text, '-' * 25) <pre>Azure\n -------------------------\nAzure\n -------------------------\nAzure\n -------------------------\nAzure\n -------------------------\nAzure\n -------------------------\n</pre> In\u00a0[18]: Copied! <pre>model_config = types.GenerateContentConfig(\n    # These are the default values for gemini-2.0-flash.\n    temperature=1.0,\n    top_p=0.95,\n)\n\nstory_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=model_config,\n    contents=story_prompt)\n\nprint(response.text)\n</pre> model_config = types.GenerateContentConfig(     # These are the default values for gemini-2.0-flash.     temperature=1.0,     top_p=0.95, )  story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\" response = client.models.generate_content(     model='gemini-2.0-flash',     config=model_config,     contents=story_prompt)  print(response.text) <pre>Whiskers twitched, emerald eyes narrowed. Jasper, a ginger tabby of discerning tastes (salmon pate for breakfast, sunbeams for naps), was bored. The predictable rhythm of lapping milk and batting at dust bunnies had lost its luster. He craved\u2026 adventure.\n\nHis opportunity arrived with the morning mail. Mrs. Higgins, his beloved human, was distracted by a particularly lurid headline about a runaway llama. In the chaos, the back door swung ajar. Jasper, without a second thought, slipped through.\n\nThe world outside was a cacophony of smells and sights. Grass, ticklish and unfamiliar, carpeted the earth. Birds, unlike the predictable chirping of the cage-bound canary inside, swooped and called with wild abandon.\n\nJasper padded cautiously, his tail held high. He was a magnificent explorer, a fearless pioneer charting unknown territories. At least, that's what he told himself. The truth was, he was a little scared.\n\nHe soon encountered Bartholomew, a grizzled, one-eared tomcat with a permanent scowl. Bartholomew held court on a discarded milk crate, surrounded by a gang of scruffy felines. This was clearly serious cat business.\n\n\"Well, well, well,\" Bartholomew rasped, his voice like sandpaper. \"Look what the cat dragged in. A fluffy housecat, come to admire our street cred?\"\n\nJasper, puffed up with bravado (and a little bit of adrenaline), straightened his fur. \"I am here for adventure,\" he declared. \"Not to admire anyone's\u2026 cred.\"\n\nBartholomew chuckled, a low rumble in his chest. \"Adventure? You think chasing butterflies is adventure? We're talking real adventure, kitten. The legend of the Whispering Warehouse.\"\n\nJasper's ears pricked. \"The Whispering Warehouse?\"\n\nBartholomew leaned closer, his breath smelling of fish scraps and secrets. \"They say it's filled with treasures, guarded by\u2026 well, that's the adventure, isn't it?\"\n\nAnd so, Jasper found himself embroiled in a quest. He joined Bartholomew and his ragtag crew, navigating alleyways, scaling fences, and dodging grumpy dogs. They faced challenges beyond his wildest housecat dreams \u2013 a ferocious chihuahua named Napoleon, a leaky sewer grate, and the agonizing wait for the garbage truck to pass.\n\nFinally, they reached the Whispering Warehouse, a towering structure with boarded-up windows and an air of eerie silence. The air hummed with an unknown energy. Bartholomew, despite his bravado, looked nervous.\n\nInside, the warehouse was a labyrinth of forgotten goods. Dust motes danced in the dim light, illuminating piles of discarded furniture and forgotten toys. But there, in the center of the room, was a pile of\u2026 yarn.\n\nMountains and mountains of yarn. In every color imaginable. Soft, fluffy, irresistible yarn.\n\nFor Jasper, it was more valuable than gold. He launched himself into the pile, burying himself in the comforting texture. He purred with delight, batting and kneading, lost in a world of woolly bliss.\n\nBartholomew and his crew stared in disbelief. \"This is it?\" Bartholomew croaked. \"All this for\u2026 yarn?\"\n\nJasper, momentarily emerging from his yarn coma, shrugged. \"Adventure is what you make it,\" he said, before disappearing back into the fluffy abyss.\n\nLater, as the sun began to set, Jasper emerged from the warehouse, a single strand of vibrant blue yarn trailing from his whiskers. He bid farewell to Bartholomew and his crew, his heart full of gratitude and a strange, fuzzy sense of accomplishment.\n\nHe slipped back through the ajar back door, unnoticed. Mrs. Higgins was still preoccupied with the llama. Jasper, exhausted but exhilarated, curled up in his favorite sunbeam.\n\nThe adventure had changed him. He wasn't just a fluffy housecat anymore. He was Jasper, the Explorer, the Yarn Adventurer, the\u2026 Sleepy Cat. He yawned, tucked his paws under his chin, and drifted off to sleep, dreaming of mountains of yarn and the thrill of the unknown. He had a feeling this was just the beginning.\n\n</pre> In\u00a0[19]: Copied! <pre>model_config = types.GenerateContentConfig(\n    temperature=0.1,\n    top_p=1,\n    max_output_tokens=5,\n)\n\nzero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: \"Her\" is a disturbing study revealing the direction\nhumanity is headed if AI is allowed to keep evolving,\nunchecked. I wish there were more movies like this masterpiece.\nSentiment: \"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=model_config,\n    contents=zero_shot_prompt)\n\nprint(response.text)\n</pre> model_config = types.GenerateContentConfig(     temperature=0.1,     top_p=1,     max_output_tokens=5, )  zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE. Review: \"Her\" is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. I wish there were more movies like this masterpiece. Sentiment: \"\"\"  response = client.models.generate_content(     model='gemini-2.0-flash',     config=model_config,     contents=zero_shot_prompt)  print(response.text) <pre>POSITIVE\n\n</pre> In\u00a0[20]: Copied! <pre>import enum\n\nclass Sentiment(enum.Enum):\n    POSITIVE = \"positive\"\n    NEUTRAL = \"neutral\"\n    NEGATIVE = \"negative\"\n\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=Sentiment\n    ),\n    contents=zero_shot_prompt)\n\nprint(response.text)\n</pre> import enum  class Sentiment(enum.Enum):     POSITIVE = \"positive\"     NEUTRAL = \"neutral\"     NEGATIVE = \"negative\"   response = client.models.generate_content(     model='gemini-2.0-flash',     config=types.GenerateContentConfig(         response_mime_type=\"text/x.enum\",         response_schema=Sentiment     ),     contents=zero_shot_prompt)  print(response.text) <pre>positive\n</pre> <p>When using constrained output like an enum, the Python SDK will attempt to convert the model's text response into a Python object automatically. It's stored in the <code>response.parsed</code> field.</p> In\u00a0[21]: Copied! <pre>enum_response = response.parsed\nprint(enum_response)\nprint(type(enum_response))\n</pre> enum_response = response.parsed print(enum_response) print(type(enum_response)) <pre>Sentiment.POSITIVE\n&lt;enum 'Sentiment'&gt;\n</pre> In\u00a0[22]: Copied! <pre>few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni.\nJSON Response:\n```\n{\n\"size\": \"small\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n}\n```\n\nEXAMPLE:\nCan I get a large pizza with tomato sauce, basil and mozzarella\nJSON Response:\n```\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n}\n```\n\nORDER:\n\"\"\"\n\ncustomer_order = \"Give me a large with cheese &amp; pineapple\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=250,\n    ),\n    contents=[few_shot_prompt, customer_order])\n\nprint(response.text)\n</pre> few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:  EXAMPLE: I want a small pizza with cheese, tomato sauce, and pepperoni. JSON Response: ``` { \"size\": \"small\", \"type\": \"normal\", \"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"] } ```  EXAMPLE: Can I get a large pizza with tomato sauce, basil and mozzarella JSON Response: ``` { \"size\": \"large\", \"type\": \"normal\", \"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"] } ```  ORDER: \"\"\"  customer_order = \"Give me a large with cheese &amp; pineapple\"  response = client.models.generate_content(     model='gemini-2.0-flash',     config=types.GenerateContentConfig(         temperature=0.1,         top_p=1,         max_output_tokens=250,     ),     contents=[few_shot_prompt, customer_order])  print(response.text) <pre>```json\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"pineapple\"]\n}\n```\n\n</pre> In\u00a0[23]: Copied! <pre>import typing_extensions as typing\n\nclass PizzaOrder(typing.TypedDict):\n    size: str\n    ingredients: list[str]\n    type: str\n\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=0.1,\n        response_mime_type=\"application/json\",\n        response_schema=PizzaOrder,\n    ),\n    contents=\"Can I have a large dessert pizza with apple and chocolate\")\n\nprint(response.text)\n</pre> import typing_extensions as typing  class PizzaOrder(typing.TypedDict):     size: str     ingredients: list[str]     type: str   response = client.models.generate_content(     model='gemini-2.0-flash',     config=types.GenerateContentConfig(         temperature=0.1,         response_mime_type=\"application/json\",         response_schema=PizzaOrder,     ),     contents=\"Can I have a large dessert pizza with apple and chocolate\")  print(response.text) <pre>{\n  \"size\": \"large\",\n  \"ingredients\": [\"apple\", \"chocolate\"],\n  \"type\": \"dessert pizza\"\n}\n</pre> In\u00a0[24]: Copied! <pre>prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\nam 20 years old. How old is my partner? Return the answer directly.\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\nprint(response.text)\n</pre> prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner? Return the answer directly.\"\"\"  response = client.models.generate_content(     model='gemini-2.0-flash',     contents=prompt)  print(response.text) <pre>52\n\n</pre> <p>Now try the same approach, but indicate to the model that it should \"think step by step\".</p> In\u00a0[25]: Copied! <pre>prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\nI am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\nMarkdown(response.text)\n</pre> prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner? Let's think step by step.\"\"\"  response = client.models.generate_content(     model='gemini-2.0-flash',     contents=prompt)  Markdown(response.text) Out[25]: <p>Here's how to solve this:</p> <ol> <li><p>Find the age difference: When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.</p> </li> <li><p>Calculate the age difference: The age difference between you and your partner is 12 - 4 = 8 years.</p> </li> <li><p>Determine partner's current age: Since the age difference remains constant, your partner is currently 20 + 8 = 28 years old.</p> </li> </ol> <p>Answer: Your partner is currently 28 years old.</p> In\u00a0[26]: Copied! <pre>model_instructions = \"\"\"\nSolve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\nObservation is understanding relevant information from an Action's output and Action can be one of three types:\n (1) &lt;search&gt;entity&lt;/search&gt;, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n     will return some similar entities to search and you can try to search the information from those topics.\n (2) &lt;lookup&gt;keyword&lt;/lookup&gt;, which returns the next sentence containing keyword in the current context. This only does exact matches,\n     so keep your searches short.\n (3) &lt;finish&gt;answer&lt;/finish&gt;, which returns the answer and finishes the task.\n\"\"\"\n\nexample1 = \"\"\"Question\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n\nThought 1\nThe question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n\nAction 1\n&lt;search&gt;Milhouse&lt;/search&gt;\n\nObservation 1\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n\nThought 2\nThe paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n\nAction 2\n&lt;lookup&gt;named after&lt;/lookup&gt;\n\nObservation 2\nMilhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n\nThought 3\nMilhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n\nAction 3\n&lt;finish&gt;Richard Nixon&lt;/finish&gt;\n\"\"\"\n\nexample2 = \"\"\"Question\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n\nThought 1\nI need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n\nAction 1\n&lt;search&gt;Colorado orogeny&lt;/search&gt;\n\nObservation 1\nThe Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n\nThought 2\nIt does not mention the eastern sector. So I need to look up eastern sector.\n\nAction 2\n&lt;lookup&gt;eastern sector&lt;/lookup&gt;\n\nObservation 2\nThe eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\nThought 3\nThe eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n\nAction 3\n&lt;search&gt;High Plains&lt;/search&gt;\n\nObservation 3\nHigh Plains refers to one of two distinct land regions\n\nThought 4\nI need to instead search High Plains (United States).\n\nAction 4\n&lt;search&gt;High Plains (United States)&lt;/search&gt;\n\nObservation 4\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n\nThought 5\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\nAction 5\n&lt;finish&gt;1,800 to 7,000 ft&lt;/finish&gt;\n\"\"\"\n\n# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/\n</pre> model_instructions = \"\"\" Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, Observation is understanding relevant information from an Action's output and Action can be one of three types:  (1) entity, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it      will return some similar entities to search and you can try to search the information from those topics.  (2) keyword, which returns the next sentence containing keyword in the current context. This only does exact matches,      so keep your searches short.  (3) answer, which returns the answer and finishes the task. \"\"\"  example1 = \"\"\"Question Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?  Thought 1 The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.  Action 1 Milhouse  Observation 1 Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.  Thought 2 The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".  Action 2 named after  Observation 2 Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.  Thought 3 Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.  Action 3 Richard Nixon \"\"\"  example2 = \"\"\"Question What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?  Thought 1 I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.  Action 1 Colorado orogeny  Observation 1 The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.  Thought 2 It does not mention the eastern sector. So I need to look up eastern sector.  Action 2 eastern sector  Observation 2 The eastern sector extends into the High Plains and is called the Central Plains orogeny.  Thought 3 The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.  Action 3 High Plains  Observation 3 High Plains refers to one of two distinct land regions  Thought 4 I need to instead search High Plains (United States).  Action 4 High Plains (United States)  Observation 4 The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).  Thought 5 High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.  Action 5 1,800 to 7,000 ft \"\"\"  # Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/ <p>To capture a single step at a time, while ignoring any hallucinated Observation steps, you will use <code>stop_sequences</code> to end the generation process. The steps are <code>Thought</code>, <code>Action</code>, <code>Observation</code>, in that order.</p> In\u00a0[27]: Copied! <pre>question = \"\"\"Question\nWho was the youngest author listed on the transformers NLP paper?\n\"\"\"\n\n# You will perform the Action; so generate up to, but not including, the Observation.\nreact_config = types.GenerateContentConfig(\n    stop_sequences=[\"\\nObservation\"],\n    system_instruction=model_instructions + example1 + example2,\n)\n\n# Create a chat that has the model instructions and examples pre-seeded.\nreact_chat = client.chats.create(\n    model='gemini-2.0-flash',\n    config=react_config,\n)\n\nresp = react_chat.send_message(question)\nprint(resp.text)\n</pre> question = \"\"\"Question Who was the youngest author listed on the transformers NLP paper? \"\"\"  # You will perform the Action; so generate up to, but not including, the Observation. react_config = types.GenerateContentConfig(     stop_sequences=[\"\\nObservation\"],     system_instruction=model_instructions + example1 + example2, )  # Create a chat that has the model instructions and examples pre-seeded. react_chat = client.chats.create(     model='gemini-2.0-flash',     config=react_config, )  resp = react_chat.send_message(question) print(resp.text) <pre>Thought 1\nI need to find the transformers NLP paper and look for the author information. Then find the youngest author.\n\nAction 1\n&lt;search&gt;transformers NLP paper&lt;/search&gt;\n\n</pre> <p>Now you can perform this research yourself and supply it back to the model.</p> In\u00a0[28]: Copied! <pre>observation = \"\"\"Observation 1\n[1706.03762] Attention Is All You Need\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\"\"\"\nresp = react_chat.send_message(observation)\nprint(resp.text)\n</pre> observation = \"\"\"Observation 1 [1706.03762] Attention Is All You Need Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. \"\"\" resp = react_chat.send_message(observation) print(resp.text) <pre>Thought 2\nI have found the paper, now I need to find the age of each author and find the youngest. This is difficult without searching each author individually. I will try to search for the ages of the authors.\n\nAction 2\n&lt;search&gt;Ashish Vaswani age&lt;/search&gt;\n\n</pre> <p>This process repeats until the <code>&lt;finish&gt;</code> action is reached. You can continue running this yourself if you like, or try the Wikipedia example to see a fully automated ReAct system at work.</p> In\u00a0[29]: Copied! <pre>import io\nfrom IPython.display import Markdown, clear_output\n\n\nresponse = client.models.generate_content_stream(\n    model='gemini-2.0-flash-thinking-exp',\n    contents='Who was the youngest author listed on the transformers NLP paper?',\n)\n\nbuf = io.StringIO()\nfor chunk in response:\n    buf.write(chunk.text)\n    # Display the response as it is streamed\n    print(chunk.text, end='')\n\n# And then render the finished response as formatted markdown.\nclear_output()\nMarkdown(buf.getvalue())\n</pre> import io from IPython.display import Markdown, clear_output   response = client.models.generate_content_stream(     model='gemini-2.0-flash-thinking-exp',     contents='Who was the youngest author listed on the transformers NLP paper?', )  buf = io.StringIO() for chunk in response:     buf.write(chunk.text)     # Display the response as it is streamed     print(chunk.text, end='')  # And then render the finished response as formatted markdown. clear_output() Markdown(buf.getvalue()) Out[29]: <p>Based on the information available online, and considering typical academic career paths, Aidan N. Gomez appears to be the youngest author listed on the \"Attention is All You Need\" paper, which introduced the Transformer architecture.</p> <p>Here's why:</p> <ul> <li><p>Aidan N. Gomez was an undergraduate student at the University of Oxford at the time the paper was written.  His LinkedIn profile indicates he graduated from Oxford in 2017, the same year the paper was published.  This typically places someone in their early 20s when publishing such a paper as an undergraduate.</p> </li> <li><p>The other authors were researchers at Google Brain, University of Toronto, and RWTH Aachen University, and many held PhDs or were in advanced stages of their research careers. This generally suggests they were older than an undergraduate student.</p> </li> </ul> <p>While precise birthdates aren't readily available for all authors to definitively confirm age,  the academic stage of Aidan N. Gomez at the time of publication strongly suggests he was the youngest author on the paper.</p> <p>Therefore, the most likely answer is Aidan N. Gomez.</p> In\u00a0[30]: Copied! <pre># The Gemini models love to talk, so it helps to specify they stick to the code if that\n# is all that you want.\ncode_prompt = \"\"\"\nWrite a Python function to calculate the factorial of a number. No explanation, provide only the code.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=1,\n        top_p=1,\n        max_output_tokens=1024,\n    ),\n    contents=code_prompt)\n\nMarkdown(response.text)\n</pre> # The Gemini models love to talk, so it helps to specify they stick to the code if that # is all that you want. code_prompt = \"\"\" Write a Python function to calculate the factorial of a number. No explanation, provide only the code. \"\"\"  response = client.models.generate_content(     model='gemini-2.0-flash',     config=types.GenerateContentConfig(         temperature=1,         top_p=1,         max_output_tokens=1024,     ),     contents=code_prompt)  Markdown(response.text) Out[30]: <pre>def factorial(n):\n  \"\"\"Calculate the factorial of a number.\"\"\"\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)\n</pre> In\u00a0[31]: Copied! <pre>from pprint import pprint\n\nconfig = types.GenerateContentConfig(\n    tools=[types.Tool(code_execution=types.ToolCodeExecution())],\n)\n\ncode_exec_prompt = \"\"\"\nGenerate the first 14 odd prime numbers, then calculate their sum.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=config,\n    contents=code_exec_prompt)\n\nfor part in response.candidates[0].content.parts:\n  pprint(part.to_json_dict())\n  print(\"-----\")\n</pre> from pprint import pprint  config = types.GenerateContentConfig(     tools=[types.Tool(code_execution=types.ToolCodeExecution())], )  code_exec_prompt = \"\"\" Generate the first 14 odd prime numbers, then calculate their sum. \"\"\"  response = client.models.generate_content(     model='gemini-2.0-flash',     config=config,     contents=code_exec_prompt)  for part in response.candidates[0].content.parts:   pprint(part.to_json_dict())   print(\"-----\") <pre>{'text': 'Okay, I can do that. First, I will generate the first 14 odd prime '\n         'numbers. Remember that a prime number is a number greater than 1 '\n         'that has only two divisors: 1 and itself. Also, note that 2 is the '\n         'only even prime number, so all other prime numbers are odd. After '\n         'generating these numbers, I will sum them.\\n'\n         '\\n'}\n-----\n{'executable_code': {'code': 'def is_prime(n):\\n'\n                             '    if n &lt;= 1:\\n'\n                             '        return False\\n'\n                             '    if n &lt;= 3:\\n'\n                             '        return True\\n'\n                             '    if n % 2 == 0 or n % 3 == 0:\\n'\n                             '        return False\\n'\n                             '    i = 5\\n'\n                             '    while i * i &lt;= n:\\n'\n                             '        if n % i == 0 or n % (i + 2) == 0:\\n'\n                             '            return False\\n'\n                             '        i += 6\\n'\n                             '    return True\\n'\n                             '\\n'\n                             'primes = []\\n'\n                             'num = 3\\n'\n                             'while len(primes) &lt; 14:\\n'\n                             '    if is_prime(num):\\n'\n                             '        primes.append(num)\\n'\n                             '    num += 2\\n'\n                             '\\n'\n                             \"print(f'{primes=}')\\n\"\n                             '\\n'\n                             'sum_of_primes = sum(primes)\\n'\n                             \"print(f'{sum_of_primes=}')\\n\",\n                     'language': 'PYTHON'}}\n-----\n{'code_execution_result': {'outcome': 'OUTCOME_OK',\n                           'output': 'primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, '\n                                     '31, 37, 41, 43, 47]\\n'\n                                     'sum_of_primes=326\\n'}}\n-----\n{'text': 'The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, '\n         '31, 37, 41, 43, and 47.\\n'\n         '\\n'\n         'The sum of these prime numbers is 326.\\n'}\n-----\n</pre> <p>This response contains multiple parts, including an opening and closing text part that represent regular responses, an <code>executable_code</code> part that represents generated code and a <code>code_execution_result</code> part that represents the results from running the generated code.</p> <p>You can explore them individually.</p> In\u00a0[32]: Copied! <pre>for part in response.candidates[0].content.parts:\n    if part.text:\n        display(Markdown(part.text))\n    elif part.executable_code:\n        display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))\n    elif part.code_execution_result:\n        if part.code_execution_result.outcome != 'OUTCOME_OK':\n            display(Markdown(f'## Status {part.code_execution_result.outcome}'))\n\n        display(Markdown(f'```\\n{part.code_execution_result.output}\\n```'))\n</pre> for part in response.candidates[0].content.parts:     if part.text:         display(Markdown(part.text))     elif part.executable_code:         display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))     elif part.code_execution_result:         if part.code_execution_result.outcome != 'OUTCOME_OK':             display(Markdown(f'## Status {part.code_execution_result.outcome}'))          display(Markdown(f'```\\n{part.code_execution_result.output}\\n```')) <p>Okay, I can do that. First, I will generate the first 14 odd prime numbers. Remember that a prime number is a number greater than 1 that has only two divisors: 1 and itself. Also, note that 2 is the only even prime number, so all other prime numbers are odd. After generating these numbers, I will sum them.</p> <pre>def is_prime(n):\n    if n &lt;= 1:\n        return False\n    if n &lt;= 3:\n        return True\n    if n % 2 == 0 or n % 3 == 0:\n        return False\n    i = 5\n    while i * i &lt;= n:\n        if n % i == 0 or n % (i + 2) == 0:\n            return False\n        i += 6\n    return True\n\nprimes = []\nnum = 3\nwhile len(primes) &lt; 14:\n    if is_prime(num):\n        primes.append(num)\n    num += 2\n\nprint(f'{primes=}')\n\nsum_of_primes = sum(primes)\nprint(f'{sum_of_primes=}')\n</pre> <pre><code>primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\nsum_of_primes=326\n\n</code></pre> <p>The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, and 47.</p> <p>The sum of these prime numbers is 326.</p> In\u00a0[33]: Copied! <pre>file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n\nexplain_prompt = f\"\"\"\nPlease explain what this file does at a very high level. What is it, and why would I use it?\n\n```\n{file_contents}\n```\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=explain_prompt)\n\nMarkdown(response.text)\n</pre> file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh  explain_prompt = f\"\"\" Please explain what this file does at a very high level. What is it, and why would I use it?  ``` {file_contents} ``` \"\"\"  response = client.models.generate_content(     model='gemini-2.0-flash',     contents=explain_prompt)  Markdown(response.text) Out[33]: <p>This file, <code>git-prompt.sh</code>, is a shell script designed to enhance your command-line prompt when working with Git repositories.  It provides information about the current Git status directly in your prompt, such as the branch name, the state of tracked/untracked files, and the divergence from the remote repository.</p> <p>In essence, it's a cosmetic and productivity tool for developers using Git.</p> <p>Here's a breakdown of why you might use it:</p> <ul> <li>Git Status at a Glance:  Instead of constantly running <code>git status</code>, the script displays key information (branch, modified files, etc.) directly in your prompt.</li> <li>Improved Workflow:  Knowing the Git status immediately helps you avoid mistakes and stay organized.</li> <li>Customization:  The script offers many options for customizing the prompt's appearance, including colors, symbols, and the information displayed.</li> <li>Virtual Environment awareness: The script also shows any virtual environments that you are working on, such as virtualenv, node virtualenv, and conda environments.</li> </ul> <p>In short, this file helps you create a more informative and visually appealing command-line prompt that integrates seamlessly with Git.</p>"},{"location":"day_01/day-1-prompting/#copyright-2025-google-llc","title":"Copyright 2025 Google LLC.\u00b6","text":""},{"location":"day_01/day-1-prompting/#day-1-prompting","title":"Day 1 - Prompting\u00b6","text":"<p>Welcome to the Kaggle 5-day Generative AI course!</p> <p>This notebook will show you how to get started with the Gemini API and walk you through some of the example prompts and techniques that you can also read about in the Prompting whitepaper. You don't need to read the whitepaper to use this notebook, but the papers will give you some theoretical context and background to complement this interactive notebook.</p>"},{"location":"day_01/day-1-prompting/#before-you-begin","title":"Before you begin\u00b6","text":"<p>In this notebook, you'll start exploring prompting using the Python SDK and AI Studio. For some inspiration, you might enjoy exploring some apps that have been built using the Gemini family of models. Here are a few that we like, and we think you will too.</p> <ul> <li>TextFX is a suite of AI-powered tools for rappers, made in collaboration with Lupe Fiasco,</li> <li>SQL Talk shows how you can talk directly to a database using the Gemini API,</li> <li>NotebookLM uses Gemini models to build your own personal AI research assistant.</li> </ul>"},{"location":"day_01/day-1-prompting/#for-help","title":"For help\u00b6","text":"<p>Common issues are covered in the FAQ and troubleshooting guide.</p>"},{"location":"day_01/day-1-prompting/#new-for-gemini-20","title":"New for Gemini 2.0!\u00b6","text":"<p>This course material was first launched in November 2024. The AI and LLM space is moving incredibly fast, so we have made some updates to use the latest models and capabilities.</p> <ul> <li>These codelabs have been updated to use the Gemini 2.0 family of models.</li> <li>The Python SDK has been updated from <code>google-generativeai</code> to the new, unified <code>google-genai</code> SDK.<ul> <li>This new SDK works with both the developer Gemini API as well as Google Cloud Vertex AI, and switching is as simple as changing some fields.</li> </ul> </li> <li>New model capabilities have been added to the relevant codelabs, such as \"thinking mode\" in this lab.</li> <li>Day 1 includes a new Evaluation codelab.</li> </ul>"},{"location":"day_01/day-1-prompting/#get-started-with-kaggle-notebooks","title":"Get started with Kaggle notebooks\u00b6","text":"<p>If this is your first time using a Kaggle notebook, welcome! You can read about how to use Kaggle notebooks in the docs.</p> <p>First, you will need to phone verify your account at kaggle.com/settings.</p> <p></p>"},{"location":"day_01/day-1-prompting/#problems","title":"Problems?\u00b6","text":"<p>If you have any problems, head over to the Kaggle Discord, find the <code>#5dgai-q-and-a</code> channel and ask for help.</p>"},{"location":"day_01/day-1-prompting/#get-started-with-the-gemini-api","title":"Get started with the Gemini API\u00b6","text":"<p>All of the exercises in this notebook will use the Gemini API by way of the Python SDK. Each of these prompts can be accessed directly in Google AI Studio too, so if you would rather use a web interface and skip the code for this activity, look for the  AI Studio link on each prompt.</p>"},{"location":"day_01/day-1-prompting/#install-the-sdk","title":"Install the SDK\u00b6","text":""},{"location":"day_01/day-1-prompting/#set-up-your-api-key","title":"Set up your API key\u00b6","text":"<p>To run the following cell, your API key must be stored it in a Kaggle secret named <code>GOOGLE_API_KEY</code>.</p> <p>If you don't already have an API key, you can grab one from AI Studio. You can find detailed instructions in the docs.</p> <p>To make the key available through Kaggle secrets, choose <code>Secrets</code> from the <code>Add-ons</code> menu and follow the instructions to add your key or enable it for this notebook.</p>"},{"location":"day_01/day-1-prompting/#run-your-first-prompt","title":"Run your first prompt\u00b6","text":"<p>In this step, you will test that your API key is set up correctly by making a request.</p> <p>The Python SDK uses a <code>Client</code> object to make requests to the API. The client lets you control which back-end to use (between the Gemini API and Vertex AI) and handles authentication (the API key).</p> <p>The <code>gemini-2.0-flash</code> model has been selected here.</p> <p>Note: If you see a <code>TransportError</code> on this step, you may need to \ud83d\udd01 Factory reset the notebook one time.</p>"},{"location":"day_01/day-1-prompting/#start-a-chat","title":"Start a chat\u00b6","text":"<p>The previous example uses a single-turn, text-in/text-out structure, but you can also set up a multi-turn chat structure too.</p>"},{"location":"day_01/day-1-prompting/#choose-a-model","title":"Choose a model\u00b6","text":"<p>The Gemini API provides access to a number of models from the Gemini model family. Read about the available models and their capabilities on the model overview page.</p> <p>In this step you'll use the API to list all of the available models.</p>"},{"location":"day_01/day-1-prompting/#explore-generation-parameters","title":"Explore generation parameters\u00b6","text":""},{"location":"day_01/day-1-prompting/#output-length","title":"Output length\u00b6","text":"<p>When generating text with an LLM, the output length affects cost and performance. Generating more tokens increases computation, leading to higher energy consumption, latency, and cost.</p> <p>To stop the model from generating tokens past a limit, you can specify the <code>max_output_tokens</code> parameter when using the Gemini API. Specifying this parameter does not influence the generation of the output tokens, so the output will not become more stylistically or textually succinct, but it will stop generating tokens once the specified length is reached. Prompt engineering may be required to generate a more complete output for your given limit.</p>"},{"location":"day_01/day-1-prompting/#temperature","title":"Temperature\u00b6","text":"<p>Temperature controls the degree of randomness in token selection. Higher temperatures result in a higher number of candidate tokens from which the next output token is selected, and can produce more diverse results, while lower temperatures have the opposite effect, such that a temperature of 0 results in greedy decoding, selecting the most probable token at each step.</p> <p>Temperature doesn't provide any guarantees of randomness, but it can be used to \"nudge\" the output somewhat.</p>"},{"location":"day_01/day-1-prompting/#top-p","title":"Top-P\u00b6","text":"<p>Like temperature, the top-P parameter is also used to control the diversity of the model's output.</p> <p>Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.</p> <p>You may also see top-K referenced in LLM literature. Top-K is not configurable in the Gemini 2.0 series of models, but can be changed in older models. Top-K is a positive integer that defines the number of most probable tokens from which to select the output token. A top-K of 1 selects a single token, performing greedy decoding.</p> <p>Run this example a number of times, change the settings and observe the change in output.</p>"},{"location":"day_01/day-1-prompting/#prompting","title":"Prompting\u00b6","text":"<p>This section contains some prompts from the chapter for you to try out directly in the API. Try changing the text here to see how each prompt performs with different instructions, more examples, or any other changes you can think of.</p>"},{"location":"day_01/day-1-prompting/#zero-shot","title":"Zero-shot\u00b6","text":"<p>Zero-shot prompts are prompts that describe the request for the model directly.</p>  Open in AI Studio"},{"location":"day_01/day-1-prompting/#enum-mode","title":"Enum mode\u00b6","text":"<p>The models are trained to generate text, and while the Gemini 2.0 models are great at following instructions, other models can sometimes produce more text than you may wish for. In the preceding example, the model will output the label, but sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards. See this prompt in AI Studio for an example.</p> <p>The Gemini API has an Enum mode feature that allows you to constrain the output to a fixed set of values.</p>"},{"location":"day_01/day-1-prompting/#one-shot-and-few-shot","title":"One-shot and few-shot\u00b6","text":"<p>Providing an example of the expected response is known as a \"one-shot\" prompt. When you provide multiple examples, it is a \"few-shot\" prompt.</p>  Open in AI Studio"},{"location":"day_01/day-1-prompting/#json-mode","title":"JSON mode\u00b6","text":"<p>To provide control over the schema, and to ensure that you only receive JSON (with no other text or markdown), you can use the Gemini API's JSON mode. This forces the model to constrain decoding, such that token selection is guided by the supplied schema.</p>"},{"location":"day_01/day-1-prompting/#chain-of-thought-cot","title":"Chain of Thought (CoT)\u00b6","text":"<p>Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.</p> <p>Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.</p> <p>Models like the Gemini family are trained to be \"chatty\" or \"thoughtful\" and will provide reasoning steps without prompting, so for this simple example you can ask the model to be more direct in the prompt to force a non-reasoning response. Try re-running this step if the model gets lucky and gets the answer correct on the first try.</p>"},{"location":"day_01/day-1-prompting/#react-reason-and-act","title":"ReAct: Reason and act\u00b6","text":"<p>In this example you will run a ReAct prompt directly in the Gemini API and perform the searching steps yourself. As this prompt follows a well-defined structure, there are frameworks available that wrap the prompt into easier-to-use APIs that make tool calls automatically, such as the LangChain example from the \"Prompting\" whitepaper.</p> <p>To try this out with the Wikipedia search engine, check out the Searching Wikipedia with ReAct cookbook example.</p> <p>Note: The prompt and in-context examples used here are from https://github.com/ysymyth/ReAct which is published under an MIT license, Copyright (c) 2023 Shunyu Yao.</p>  Open in AI Studio"},{"location":"day_01/day-1-prompting/#thinking-mode","title":"Thinking mode\u00b6","text":"<p>The experiemental Gemini Flash 2.0 \"Thinking\" model has been trained to generate the \"thinking process\" the model goes through as part of its response. As a result, the Flash Thinking model is capable of stronger reasoning capabilities in its responses.</p> <p>Using a \"thinking mode\" model can provide you with high-quality responses without needing specialised prompting like the previous approaches. One reason this technique is effective is that you induce the model to generate relevant information (\"brainstorming\", or \"thoughts\") that is then used as part of the context in which the final response is generated.</p> <p>Note that when you use the API, you get the final response from the model, but the thoughts are not captured. To see the intermediate thoughts, try out the thinking mode model in AI Studio.</p>  Open in AI Studio"},{"location":"day_01/day-1-prompting/#code-prompting","title":"Code prompting\u00b6","text":""},{"location":"day_01/day-1-prompting/#generating-code","title":"Generating code\u00b6","text":"<p>The Gemini family of models can be used to generate code, configuration and scripts. Generating code can be helpful when learning to code, learning a new language or for rapidly generating a first draft.</p> <p>It's important to be aware that since LLMs can make mistakes, and can repeat training data, it's essential to read and test your code first, and comply with any relevant licenses.</p>  Open in AI Studio"},{"location":"day_01/day-1-prompting/#code-execution","title":"Code execution\u00b6","text":"<p>The Gemini API can automatically run generated code too, and will return the output.</p>  Open in AI Studio"},{"location":"day_01/day-1-prompting/#explaining-code","title":"Explaining code\u00b6","text":"<p>The Gemini family of models can explain code to you too. In this example, you pass a bash script and ask some questions.</p>  Open in AI Studio"},{"location":"day_01/day-1-prompting/#learn-more","title":"Learn more\u00b6","text":"<p>To learn more about prompting in depth:</p> <ul> <li>Check out the whitepaper issued with today's content,</li> <li>Try out the apps listed at the top of this notebook (TextFX, SQL Talk and NotebookLM),</li> <li>Read the Introduction to Prompting from the Gemini API docs,</li> <li>Explore the Gemini API's prompt gallery and try them out in AI Studio,</li> <li>Check out the Gemini API cookbook for inspirational examples and educational quickstarts.</li> </ul> <p>Be sure to check out the codelabs on day 3 too, where you will explore some more advanced prompting with code execution.</p> <p>And please share anything exciting you have tried in the Discord!</p> <p>- Mark McD</p>"},{"location":"day_01/extras/","title":"Day 1 Livestream with Paige Bailey \u2013 5-Day Gen AI Intensive Course | Kaggle","text":""},{"location":"day_01/podcast/","title":"Whitepaper Companion Podcast - Foundational LLMs &amp; Text Generation","text":""},{"location":"day_01/podcast2/","title":"Whitepaper Companion Podcast - Prompt Engineering","text":""},{"location":"day_01/whitepaper/","title":"Foundational Large Language Models &amp; Text Generation","text":"<p>Authors: Mohammadamin Barektain, Anant Nawalgaria, Daniel J. Mankowitz, Majd Al Merey, Yaniv Leviathan, Massimo Mascaro, Matan Kalman, Elena Buchatskaya, Aliaksei Severyn, Irina Sigler, and Antonio Gulli </p>"},{"location":"day_01/whitepaper/#introduction","title":"Introduction","text":"<p>The advent of Large Language Models (LLMs) represents a seismic shift in the world of artificial intelligence. Their ability to process, generate, and understand user intent is fundamentally changing the way we interact with information and technology.</p> <p>The advent of Large Language Models (LLMs) represents a seismic shift in the world of artificial intelligence. Their ability to process, generate, and understand user intent is fundamentally changing the way we interact with information and technology. An LLM is an advanced artificial intelligence system that specializes in processing, understanding, and generating human-like text. These systems are typically implemented as a deep neural network and are trained on massive amounts of text data. This allows them to learn the intricate patterns of language, giving them the ability to perform a variety of tasks, like machine translation, creative text generation, question answering, text summarization, and many more reasoning and language oriented tasks. This whitepaper dives into the timeline of the various architectures and approaches building up to the large language models and the architectures being used at the time of publication. It also discusses fine- We believe that this new crop of tuning techniques to customize an LLM to a certain domain or task, methods to make the training more efficient, as well as methods to accelerate inference. These are then followed by various applications and code examples.</p>"},{"location":"day_01/whitepaper/#read-the-whitepaper-below","title":"Read the whitepaper below","text":""},{"location":"day_01/whitepaper2/","title":"Prompt Engineering","text":"<p>Author: Lee Boonstra</p>"},{"location":"day_01/whitepaper2/#introduction","title":"Introduction","text":"<p>When thinking about a large language model input and output, a text prompt (sometimes accompanied by other modalities such as image prompts) is the input the model uses to predict a specific output. You don\u2019t need to be a data scientist or a machine learning engineer \u2013 everyone can write a prompt. However, crafting the most effective prompt can be complicated. Many aspects of your prompt affect its efficacy: the model you use, the model\u2019s training data, the model configurations, your word-choice, style and tone, structure, and context all matters. Therefore, prompt engineering is an iterative process. Inadequate prompts can lead to ambiguous, inaccurate responses, and can hinder the model\u2019s ability to provide meaningful output.</p> <p>When you chat with the Gemini chatbot, you basically write prompts, however this whitepaper focuses on writing prompts for the Gemini model within Vertex AI or by using the API, because by prompting the model directly you will have access to the configuration such as temperature etc. This whitepaper discusses prompt engineering in detail. We will look into the various prompting techniques to help you getting started and share tips and best practices to become a prompting expert. We will also discuss some of the challenges you can face while crafting prompts.</p>"},{"location":"day_01/whitepaper2/#read-the-whitepaper-below","title":"Read the whitepaper below","text":""},{"location":"day_02/","title":"Day 2 \u2013 Embeddings and Vector Stores/Databases","text":"<p>Learn about embeddings, vector search algorithms, and real-world applications.</p> <p>Assignments:</p> <ol> <li> <p>Unit 2: \u201cEmbeddings and Vector Stores/Databases\u201d </p> <ul> <li>\ud83c\udfa7 Podcast (Optional) </li> <li>\ud83d\udcc4 Whitepaper </li> <li>\ud83d\udcbb Code Labs:<ul> <li>RAG Question Answering </li> <li>Text Similarity </li> <li>Classification with Keras</li> </ul> </li> </ul> </li> <li> <p>\ud83c\udfa5 YouTube Livestream Recording</p> </li> </ol>"},{"location":"day_02/day-2-classifying-embeddings-with-keras/","title":"Day 2 - Classifying embeddings with Keras and the Gemini API","text":"In\u00a0[1]: Copied! <pre># @title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # @title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. In\u00a0[2]: Copied! <pre>!pip uninstall -qqy jupyterlab kfp 2&gt;/dev/null  # Remove unused conflicting packages\n!pip install -U -q \"google-genai==1.7.0\"\n</pre> !pip uninstall -qqy jupyterlab kfp 2&gt;/dev/null  # Remove unused conflicting packages !pip install -U -q \"google-genai==1.7.0\" In\u00a0[3]: Copied! <pre>from google import genai\nfrom google.genai import types\n\ngenai.__version__\n</pre> from google import genai from google.genai import types  genai.__version__ Out[3]: <pre>'1.7.0'</pre> In\u00a0[4]: Copied! <pre>from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)\n</pre> from kaggle_secrets import UserSecretsClient  GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")  client = genai.Client(api_key=GOOGLE_API_KEY) <p>If you received an error response along the lines of <code>No user secrets exist for kernel id ...</code>, then you need to add your API key via <code>Add-ons</code>, <code>Secrets</code> and enable it.</p> <p></p> In\u00a0[5]: Copied! <pre>from sklearn.datasets import fetch_20newsgroups\n\nnewsgroups_train = fetch_20newsgroups(subset=\"train\")\nnewsgroups_test = fetch_20newsgroups(subset=\"test\")\n\n# View list of class names for dataset\nnewsgroups_train.target_names\n</pre> from sklearn.datasets import fetch_20newsgroups  newsgroups_train = fetch_20newsgroups(subset=\"train\") newsgroups_test = fetch_20newsgroups(subset=\"test\")  # View list of class names for dataset newsgroups_train.target_names Out[5]: <pre>['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']</pre> <p>Here is an example of what a record from the training set looks like.</p> In\u00a0[6]: Copied! <pre>print(newsgroups_train.data[0])\n</pre> print(newsgroups_train.data[0]) <pre>From: lerxst@wam.umd.edu (where's my thing)\nSubject: WHAT car is this!?\nNntp-Posting-Host: rac3.wam.umd.edu\nOrganization: University of Maryland, College Park\nLines: 15\n\n I was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n\nThanks,\n- IL\n   ---- brought to you by your neighborhood Lerxst ----\n\n\n\n\n\n</pre> <p>Start by preprocessing the data for this tutorial in a Pandas dataframe. To remove any sensitive information like names and email addresses, you will take only the subject and body of each message. This is an optional step that transforms the input data into more generic text, rather than email posts, so that it will work in other contexts.</p> In\u00a0[7]: Copied! <pre>import email\nimport re\n\nimport pandas as pd\n\n\ndef preprocess_newsgroup_row(data):\n    # Extract only the subject and body\n    msg = email.message_from_string(data)\n    text = f\"{msg['Subject']}\\n\\n{msg.get_payload()}\"\n    # Strip any remaining email addresses\n    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)\n    # Truncate each entry to 5,000 characters\n    text = text[:5000]\n\n    return text\n\n\ndef preprocess_newsgroup_data(newsgroup_dataset):\n    # Put data points into dataframe\n    df = pd.DataFrame(\n        {\"Text\": newsgroup_dataset.data, \"Label\": newsgroup_dataset.target}\n    )\n    # Clean up the text\n    df[\"Text\"] = df[\"Text\"].apply(preprocess_newsgroup_row)\n    # Match label to target name index\n    df[\"Class Name\"] = df[\"Label\"].map(lambda l: newsgroup_dataset.target_names[l])\n\n    return df\n</pre> import email import re  import pandas as pd   def preprocess_newsgroup_row(data):     # Extract only the subject and body     msg = email.message_from_string(data)     text = f\"{msg['Subject']}\\n\\n{msg.get_payload()}\"     # Strip any remaining email addresses     text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)     # Truncate each entry to 5,000 characters     text = text[:5000]      return text   def preprocess_newsgroup_data(newsgroup_dataset):     # Put data points into dataframe     df = pd.DataFrame(         {\"Text\": newsgroup_dataset.data, \"Label\": newsgroup_dataset.target}     )     # Clean up the text     df[\"Text\"] = df[\"Text\"].apply(preprocess_newsgroup_row)     # Match label to target name index     df[\"Class Name\"] = df[\"Label\"].map(lambda l: newsgroup_dataset.target_names[l])      return df In\u00a0[8]: Copied! <pre># Apply preprocessing function to training and test datasets\ndf_train = preprocess_newsgroup_data(newsgroups_train)\ndf_test = preprocess_newsgroup_data(newsgroups_test)\n\ndf_train.head()\n</pre> # Apply preprocessing function to training and test datasets df_train = preprocess_newsgroup_data(newsgroups_train) df_test = preprocess_newsgroup_data(newsgroups_test)  df_train.head() Out[8]: Text Label Class Name 0 WHAT car is this!?\\n\\n I was wondering if anyo... 7 rec.autos 1 SI Clock Poll - Final Call\\n\\nA fair number of... 4 comp.sys.mac.hardware 2 PB questions...\\n\\nwell folks, my mac plus fin... 4 comp.sys.mac.hardware 3 Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr... 1 comp.graphics 4 Re: Shuttle Launch Question\\n\\nFrom article &lt;&gt;... 14 sci.space <p>Next, you will sample some of the data by taking 100 data points in the training dataset, and dropping a few of the categories to run through this tutorial. Choose the science categories to compare.</p> In\u00a0[9]: Copied! <pre>def sample_data(df, num_samples, classes_to_keep):\n    # Sample rows, selecting num_samples of each Label.\n    df = (\n        df.groupby(\"Label\")[df.columns]\n        .apply(lambda x: x.sample(num_samples))\n        .reset_index(drop=True)\n    )\n\n    df = df[df[\"Class Name\"].str.contains(classes_to_keep)]\n\n    # We have fewer categories now, so re-calibrate the label encoding.\n    df[\"Class Name\"] = df[\"Class Name\"].astype(\"category\")\n    df[\"Encoded Label\"] = df[\"Class Name\"].cat.codes\n\n    return df\n</pre> def sample_data(df, num_samples, classes_to_keep):     # Sample rows, selecting num_samples of each Label.     df = (         df.groupby(\"Label\")[df.columns]         .apply(lambda x: x.sample(num_samples))         .reset_index(drop=True)     )      df = df[df[\"Class Name\"].str.contains(classes_to_keep)]      # We have fewer categories now, so re-calibrate the label encoding.     df[\"Class Name\"] = df[\"Class Name\"].astype(\"category\")     df[\"Encoded Label\"] = df[\"Class Name\"].cat.codes      return df In\u00a0[10]: Copied! <pre>TRAIN_NUM_SAMPLES = 100\nTEST_NUM_SAMPLES = 25\n# Class name should contain 'sci' to keep science categories.\n# Try different labels from the data - see newsgroups_train.target_names\nCLASSES_TO_KEEP = \"sci\"\n\ndf_train = sample_data(df_train, TRAIN_NUM_SAMPLES, CLASSES_TO_KEEP)\ndf_test = sample_data(df_test, TEST_NUM_SAMPLES, CLASSES_TO_KEEP)\n</pre> TRAIN_NUM_SAMPLES = 100 TEST_NUM_SAMPLES = 25 # Class name should contain 'sci' to keep science categories. # Try different labels from the data - see newsgroups_train.target_names CLASSES_TO_KEEP = \"sci\"  df_train = sample_data(df_train, TRAIN_NUM_SAMPLES, CLASSES_TO_KEEP) df_test = sample_data(df_test, TEST_NUM_SAMPLES, CLASSES_TO_KEEP) In\u00a0[11]: Copied! <pre>df_train.value_counts(\"Class Name\")\n</pre> df_train.value_counts(\"Class Name\") Out[11]: <pre>Class Name\nsci.crypt          100\nsci.electronics    100\nsci.med            100\nsci.space          100\nName: count, dtype: int64</pre> In\u00a0[12]: Copied! <pre>df_test.value_counts(\"Class Name\")\n</pre> df_test.value_counts(\"Class Name\") Out[12]: <pre>Class Name\nsci.crypt          25\nsci.electronics    25\nsci.med            25\nsci.space          25\nName: count, dtype: int64</pre> In\u00a0[13]: Copied! <pre>from google.api_core import retry\nimport tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Add tqdm to Pandas...\ntqdmr.pandas()\n\n# ...But suppress the experimental warning.\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n# Define a helper to retry when per-minute quota is reached.\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n@retry.Retry(predicate=is_retriable, timeout=300.0)\ndef embed_fn(text: str) -&gt; list[float]:\n    # You will be performing classification, so set task_type accordingly.\n    response = client.models.embed_content(\n        model=\"models/text-embedding-004\",\n        contents=text,\n        config=types.EmbedContentConfig(\n            task_type=\"classification\",\n        ),\n    )\n\n    return response.embeddings[0].values\n\n\ndef create_embeddings(df):\n    df[\"Embeddings\"] = df[\"Text\"].progress_apply(embed_fn)\n    return df\n</pre> from google.api_core import retry import tqdm from tqdm.rich import tqdm as tqdmr import warnings  # Add tqdm to Pandas... tqdmr.pandas()  # ...But suppress the experimental warning. warnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)  # Define a helper to retry when per-minute quota is reached. is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})  @retry.Retry(predicate=is_retriable, timeout=300.0) def embed_fn(text: str) -&gt; list[float]:     # You will be performing classification, so set task_type accordingly.     response = client.models.embed_content(         model=\"models/text-embedding-004\",         contents=text,         config=types.EmbedContentConfig(             task_type=\"classification\",         ),     )      return response.embeddings[0].values   def create_embeddings(df):     df[\"Embeddings\"] = df[\"Text\"].progress_apply(embed_fn)     return df <p>This code is optimised for clarity, and is not particularly fast. It is left as an exercise for the reader to implement batch or parallel/asynchronous embedding generation. Running this step will take some time.</p> In\u00a0[14]: Copied! <pre>df_train = create_embeddings(df_train)\ndf_test = create_embeddings(df_test)\n</pre> df_train = create_embeddings(df_train) df_test = create_embeddings(df_test) <pre>Output()</pre> <pre></pre> <pre>\n</pre> <pre>Output()</pre> <pre></pre> <pre>\n</pre> In\u00a0[15]: Copied! <pre>df_train.head()\n</pre> df_train.head() Out[15]: Text Label Class Name Encoded Label Embeddings 1100 Re: Once tapped, your code is no good any more... 11 sci.crypt 0 [0.016574237, 0.0027566992, -0.045826472, 0.01... 1101 Re: PGP ideas for IBM systems\\n\\n:    I've bee... 11 sci.crypt 0 [-0.0021864863, 0.009111009, -0.03448642, 0.02... 1102 Clipper will corrupt cops (was WH proposal fro... 11 sci.crypt 0 [0.01497736, 0.022450915, -0.07499878, 0.01783... 1103 Re: (new) reason for Clipper alg'm secrecy\\n\\n... 11 sci.crypt 0 [0.0071349684, 0.036663644, -0.054815542, 0.01... 1104 Re: Secret algorithm [Re: Clipper Chip and cry... 11 sci.crypt 0 [-0.022487164, 0.026177764, -0.013673776, 0.03... In\u00a0[16]: Copied! <pre>import keras\nfrom keras import layers\n\n\ndef build_classification_model(input_size: int, num_classes: int) -&gt; keras.Model:\n    return keras.Sequential(\n        [\n            layers.Input([input_size], name=\"embedding_inputs\"),\n            layers.Dense(input_size, activation=\"relu\", name=\"hidden\"),\n            layers.Dense(num_classes, activation=\"softmax\", name=\"output_probs\"),\n        ]\n    )\n</pre> import keras from keras import layers   def build_classification_model(input_size: int, num_classes: int) -&gt; keras.Model:     return keras.Sequential(         [             layers.Input([input_size], name=\"embedding_inputs\"),             layers.Dense(input_size, activation=\"relu\", name=\"hidden\"),             layers.Dense(num_classes, activation=\"softmax\", name=\"output_probs\"),         ]     ) In\u00a0[17]: Copied! <pre># Derive the embedding size from observing the data. The embedding size can also be specified\n# with the `output_dimensionality` parameter to `embed_content` if you need to reduce it.\nembedding_size = len(df_train[\"Embeddings\"].iloc[0])\n\nclassifier = build_classification_model(\n    embedding_size, len(df_train[\"Class Name\"].unique())\n)\nclassifier.summary()\n\nclassifier.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(),\n    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n    metrics=[\"accuracy\"],\n)\n</pre> # Derive the embedding size from observing the data. The embedding size can also be specified # with the `output_dimensionality` parameter to `embed_content` if you need to reduce it. embedding_size = len(df_train[\"Embeddings\"].iloc[0])  classifier = build_classification_model(     embedding_size, len(df_train[\"Class Name\"].unique()) ) classifier.summary()  classifier.compile(     loss=keras.losses.SparseCategoricalCrossentropy(),     optimizer=keras.optimizers.Adam(learning_rate=0.001),     metrics=[\"accuracy\"], ) <pre>Model: \"sequential\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 hidden (Dense)                  \u2502 (None, 768)            \u2502       590,592 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 output_probs (Dense)            \u2502 (None, 4)              \u2502         3,076 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 593,668 (2.26 MB)\n</pre> <pre> Trainable params: 593,668 (2.26 MB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[18]: Copied! <pre>import numpy as np\n\n\nNUM_EPOCHS = 20\nBATCH_SIZE = 32\n\n# Split the x and y components of the train and validation subsets.\ny_train = df_train[\"Encoded Label\"]\nx_train = np.stack(df_train[\"Embeddings\"])\ny_val = df_test[\"Encoded Label\"]\nx_val = np.stack(df_test[\"Embeddings\"])\n\n# Specify that it's OK to stop early if accuracy stabilises.\nearly_stop = keras.callbacks.EarlyStopping(monitor=\"accuracy\", patience=3)\n\n# Train the model for the desired number of epochs.\nhistory = classifier.fit(\n    x=x_train,\n    y=y_train,\n    validation_data=(x_val, y_val),\n    callbacks=[early_stop],\n    batch_size=BATCH_SIZE,\n    epochs=NUM_EPOCHS,\n)\n</pre> import numpy as np   NUM_EPOCHS = 20 BATCH_SIZE = 32  # Split the x and y components of the train and validation subsets. y_train = df_train[\"Encoded Label\"] x_train = np.stack(df_train[\"Embeddings\"]) y_val = df_test[\"Encoded Label\"] x_val = np.stack(df_test[\"Embeddings\"])  # Specify that it's OK to stop early if accuracy stabilises. early_stop = keras.callbacks.EarlyStopping(monitor=\"accuracy\", patience=3)  # Train the model for the desired number of epochs. history = classifier.fit(     x=x_train,     y=y_train,     validation_data=(x_val, y_val),     callbacks=[early_stop],     batch_size=BATCH_SIZE,     epochs=NUM_EPOCHS, ) <pre>Epoch 1/20\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 24ms/step - accuracy: 0.3351 - loss: 1.3626 - val_accuracy: 0.6500 - val_loss: 1.2670\nEpoch 2/20\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 9ms/step - accuracy: 0.6880 - loss: 1.1977 - val_accuracy: 0.7100 - val_loss: 1.1232\nEpoch 3/20\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 9ms/step - accuracy: 0.8327 - loss: 1.0034 - val_accuracy: 0.8200 - val_loss: 0.9574\nEpoch 4/20\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - accuracy: 0.9354 - loss: 0.7962 - val_accuracy: 0.8500 - val_loss: 0.8072\nEpoch 5/20\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - accuracy: 0.9502 - loss: 0.5904 - val_accuracy: 0.8300 - val_loss: 0.6900\nEpoch 6/20\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 9ms/step - accuracy: 0.9329 - loss: 0.4859 - val_accuracy: 0.9000 - val_loss: 0.5657\nEpoch 7/20\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - accuracy: 0.9654 - loss: 0.3539 - val_accuracy: 0.9100 - val_loss: 0.4896\nEpoch 8/20\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - accuracy: 0.9798 - loss: 0.2737 - val_accuracy: 0.8700 - val_loss: 0.4484\nEpoch 9/20\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - accuracy: 0.9672 - loss: 0.2440 - val_accuracy: 0.8700 - val_loss: 0.3981\nEpoch 10/20\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 9ms/step - accuracy: 0.9863 - loss: 0.1752 - val_accuracy: 0.9000 - val_loss: 0.3698\nEpoch 11/20\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 9ms/step - accuracy: 0.9661 - loss: 0.1895 - val_accuracy: 0.8900 - val_loss: 0.3585\n</pre> In\u00a0[19]: Copied! <pre>classifier.evaluate(x=x_val, y=y_val, return_dict=True)\n</pre> classifier.evaluate(x=x_val, y=y_val, return_dict=True) <pre>4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.8956 - loss: 0.3520 \n</pre> Out[19]: <pre>{'accuracy': 0.8899999856948853, 'loss': 0.3584713637828827}</pre> <p>To learn more about training models with Keras, including how to visualise the model training metrics, read Training &amp; evaluation with built-in methods.</p> In\u00a0[20]: Copied! <pre>def make_prediction(text: str) -&gt; list[float]:\n    \"\"\"Infer categories from the provided text.\"\"\"\n    # Remember that the model takes embeddings as input, so calculate them first.\n    embedded = embed_fn(new_text)\n\n    # And recall that the input must be batched, so here they are wrapped as a\n    # list to provide a batch of 1.\n    inp = np.array([embedded])\n\n    # And un-batched here.\n    [result] = classifier.predict(inp)\n    return result\n</pre> def make_prediction(text: str) -&gt; list[float]:     \"\"\"Infer categories from the provided text.\"\"\"     # Remember that the model takes embeddings as input, so calculate them first.     embedded = embed_fn(new_text)      # And recall that the input must be batched, so here they are wrapped as a     # list to provide a batch of 1.     inp = np.array([embedded])      # And un-batched here.     [result] = classifier.predict(inp)     return result In\u00a0[21]: Copied! <pre># This example avoids any space-specific terminology to see if the model avoids\n# biases towards specific jargon.\nnew_text = \"\"\"\nFirst-timer looking to get out of here.\n\nHi, I'm writing about my interest in travelling to the outer limits!\n\nWhat kind of craft can I buy? What is easiest to access from this 3rd rock?\n\nLet me know how to do that please.\n\"\"\"\n\nresult = make_prediction(new_text)\n\nfor idx, category in enumerate(df_test[\"Class Name\"].cat.categories):\n    print(f\"{category}: {result[idx] * 100:0.2f}%\")\n</pre> # This example avoids any space-specific terminology to see if the model avoids # biases towards specific jargon. new_text = \"\"\" First-timer looking to get out of here.  Hi, I'm writing about my interest in travelling to the outer limits!  What kind of craft can I buy? What is easiest to access from this 3rd rock?  Let me know how to do that please. \"\"\"  result = make_prediction(new_text)  for idx, category in enumerate(df_test[\"Class Name\"].cat.categories):     print(f\"{category}: {result[idx] * 100:0.2f}%\") <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 53ms/step\nsci.crypt: 0.26%\nsci.electronics: 1.72%\nsci.med: 0.36%\nsci.space: 97.66%\n</pre>"},{"location":"day_02/day-2-classifying-embeddings-with-keras/#copyright-2025-google-llc","title":"Copyright 2025 Google LLC.\u00b6","text":""},{"location":"day_02/day-2-classifying-embeddings-with-keras/#day-2-classifying-embeddings-with-keras-and-the-gemini-api","title":"Day 2 - Classifying embeddings with Keras and the Gemini API\u00b6","text":""},{"location":"day_02/day-2-classifying-embeddings-with-keras/#overview","title":"Overview\u00b6","text":"<p>Welcome back to the Kaggle 5-day Generative AI course. In this notebook, you'll learn to use the embeddings produced by the Gemini API to train a model that can classify newsgroup posts into the categories (the newsgroup itself) from the post contents.</p> <p>This technique uses the Gemini API's embeddings as input, avoiding the need to train on text input directly, and as a result it is able to perform quite well using relatively few examples compared to training a text model from scratch.</p>"},{"location":"day_02/day-2-classifying-embeddings-with-keras/#for-help","title":"For help\u00b6","text":"<p>Common issues are covered in the FAQ and troubleshooting guide.</p>"},{"location":"day_02/day-2-classifying-embeddings-with-keras/#set-up-your-api-key","title":"Set up your API key\u00b6","text":"<p>To run the following cell, your API key must be stored it in a Kaggle secret named <code>GOOGLE_API_KEY</code>.</p> <p>If you don't already have an API key, you can grab one from AI Studio. You can find detailed instructions in the docs.</p> <p>To make the key available through Kaggle secrets, choose <code>Secrets</code> from the <code>Add-ons</code> menu and follow the instructions to add your key or enable it for this notebook.</p>"},{"location":"day_02/day-2-classifying-embeddings-with-keras/#dataset","title":"Dataset\u00b6","text":"<p>The 20 Newsgroups Text Dataset contains 18,000 newsgroups posts on 20 topics divided into training and test sets. The split between the training and test datasets are based on messages posted before and after a specific date. For this tutorial, you will use sampled subsets of the training and test sets, and perform some processing using Pandas.</p>"},{"location":"day_02/day-2-classifying-embeddings-with-keras/#create-the-embeddings","title":"Create the embeddings\u00b6","text":"<p>In this section, you will generate embeddings for each piece of text using the Gemini API embeddings endpoint. To learn more about embeddings, visit the embeddings guide.</p> <p>NOTE: Embeddings are computed one at a time, so large sample sizes can take a long time!</p>"},{"location":"day_02/day-2-classifying-embeddings-with-keras/#task-types","title":"Task types\u00b6","text":"<p>The <code>text-embedding-004</code> model supports a task type parameter that generates embeddings tailored for the specific task.</p> Task Type Description RETRIEVAL_QUERY Specifies the given text is a query in a search/retrieval setting. RETRIEVAL_DOCUMENT Specifies the given text is a document in a search/retrieval setting. SEMANTIC_SIMILARITY Specifies the given text will be used for Semantic Textual Similarity (STS). CLASSIFICATION Specifies that the embeddings will be used for classification. CLUSTERING Specifies that the embeddings will be used for clustering. FACT_VERIFICATION Specifies that the given text will be used for fact verification. <p>For this example you will be performing classification.</p>"},{"location":"day_02/day-2-classifying-embeddings-with-keras/#build-a-classification-model","title":"Build a classification model\u00b6","text":"<p>Here you will define a simple model that accepts the raw embedding data as input, has one hidden layer, and an output layer specifying the class probabilities. The prediction will correspond to the probability of a piece of text being a particular class of news.</p> <p>When you run the model, Keras will take care of details like shuffling the data points, calculating metrics and other ML boilerplate.</p>"},{"location":"day_02/day-2-classifying-embeddings-with-keras/#train-the-model","title":"Train the model\u00b6","text":"<p>Finally, you can train your model. This code uses early stopping to exit the training loop once the loss value stabilises, so the number of epoch loops executed may differ from the specified value.</p>"},{"location":"day_02/day-2-classifying-embeddings-with-keras/#evaluate-model-performance","title":"Evaluate model performance\u00b6","text":"<p>Use Keras <code>Model.evaluate</code> to calculate the loss and accuracy on the test dataset.</p>"},{"location":"day_02/day-2-classifying-embeddings-with-keras/#try-a-custom-prediction","title":"Try a custom prediction\u00b6","text":"<p>Now that you have a trained model with good evaluation metrics, you can try to make a prediction with new, hand-written data. Use the provided example or try your own data to see how the model performs.</p>"},{"location":"day_02/day-2-classifying-embeddings-with-keras/#further-reading","title":"Further reading\u00b6","text":"<p>To explore training custom models with Keras further, check out the Keras guides.</p> <p>- Mark McD</p>"},{"location":"day_02/day-2-document-q-a-with-rag/","title":"Day 2 - Document Q&amp;A with RAG using Chroma","text":"In\u00a0[13]: Copied! <pre># @title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # @title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. In\u00a0[14]: Copied! <pre>!pip uninstall -qqy jupyterlab kfp  # Remove unused conflicting packages\n!pip install -qU \"google-genai==1.7.0\" \"chromadb==0.6.3\"\n</pre> !pip uninstall -qqy jupyterlab kfp  # Remove unused conflicting packages !pip install -qU \"google-genai==1.7.0\" \"chromadb==0.6.3\" In\u00a0[15]: Copied! <pre>from google import genai\nfrom google.genai import types\n\nfrom IPython.display import Markdown\n\ngenai.__version__\n</pre> from google import genai from google.genai import types  from IPython.display import Markdown  genai.__version__ Out[15]: <pre>'1.7.0'</pre> In\u00a0[16]: Copied! <pre>from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n</pre> from kaggle_secrets import UserSecretsClient  GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\") <p>If you received an error response along the lines of <code>No user secrets exist for kernel id ...</code>, then you need to add your API key via <code>Add-ons</code>, <code>Secrets</code> and enable it.</p> <p></p> In\u00a0[17]: Copied! <pre>client = genai.Client(api_key=GOOGLE_API_KEY)\n\nfor m in client.models.list():\n    if \"embedContent\" in m.supported_actions:\n        print(m.name)\n</pre> client = genai.Client(api_key=GOOGLE_API_KEY)  for m in client.models.list():     if \"embedContent\" in m.supported_actions:         print(m.name) <pre>models/embedding-001\nmodels/text-embedding-004\nmodels/gemini-embedding-exp-03-07\nmodels/gemini-embedding-exp\n</pre> In\u00a0[18]: Copied! <pre>DOCUMENT1 = \"Operating the Climate Control System  Your Googlecar has a climate control system that allows you to adjust the temperature and airflow in the car. To operate the climate control system, use the buttons and knobs located on the center console.  Temperature: The temperature knob controls the temperature inside the car. Turn the knob clockwise to increase the temperature or counterclockwise to decrease the temperature. Airflow: The airflow knob controls the amount of airflow inside the car. Turn the knob clockwise to increase the airflow or counterclockwise to decrease the airflow. Fan speed: The fan speed knob controls the speed of the fan. Turn the knob clockwise to increase the fan speed or counterclockwise to decrease the fan speed. Mode: The mode button allows you to select the desired mode. The available modes are: Auto: The car will automatically adjust the temperature and airflow to maintain a comfortable level. Cool: The car will blow cool air into the car. Heat: The car will blow warm air into the car. Defrost: The car will blow warm air onto the windshield to defrost it.\"\nDOCUMENT2 = 'Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.'\nDOCUMENT3 = \"Shifting Gears Your Googlecar has an automatic transmission. To shift gears, simply move the shift lever to the desired position.  Park: This position is used when you are parked. The wheels are locked and the car cannot move. Reverse: This position is used to back up. Neutral: This position is used when you are stopped at a light or in traffic. The car is not in gear and will not move unless you press the gas pedal. Drive: This position is used to drive forward. Low: This position is used for driving in snow or other slippery conditions.\"\n\ndocuments = [DOCUMENT1, DOCUMENT2, DOCUMENT3]\n</pre> DOCUMENT1 = \"Operating the Climate Control System  Your Googlecar has a climate control system that allows you to adjust the temperature and airflow in the car. To operate the climate control system, use the buttons and knobs located on the center console.  Temperature: The temperature knob controls the temperature inside the car. Turn the knob clockwise to increase the temperature or counterclockwise to decrease the temperature. Airflow: The airflow knob controls the amount of airflow inside the car. Turn the knob clockwise to increase the airflow or counterclockwise to decrease the airflow. Fan speed: The fan speed knob controls the speed of the fan. Turn the knob clockwise to increase the fan speed or counterclockwise to decrease the fan speed. Mode: The mode button allows you to select the desired mode. The available modes are: Auto: The car will automatically adjust the temperature and airflow to maintain a comfortable level. Cool: The car will blow cool air into the car. Heat: The car will blow warm air into the car. Defrost: The car will blow warm air onto the windshield to defrost it.\" DOCUMENT2 = 'Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.' DOCUMENT3 = \"Shifting Gears Your Googlecar has an automatic transmission. To shift gears, simply move the shift lever to the desired position.  Park: This position is used when you are parked. The wheels are locked and the car cannot move. Reverse: This position is used to back up. Neutral: This position is used when you are stopped at a light or in traffic. The car is not in gear and will not move unless you press the gas pedal. Drive: This position is used to drive forward. Low: This position is used for driving in snow or other slippery conditions.\"  documents = [DOCUMENT1, DOCUMENT2, DOCUMENT3] In\u00a0[19]: Copied! <pre>from chromadb import Documents, EmbeddingFunction, Embeddings\nfrom google.api_core import retry\n\nfrom google.genai import types\n\n\n# Define a helper to retry when per-minute quota is reached.\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n\nclass GeminiEmbeddingFunction(EmbeddingFunction):\n    # Specify whether to generate embeddings for documents, or queries\n    document_mode = True\n\n    @retry.Retry(predicate=is_retriable)\n    def __call__(self, input: Documents) -&gt; Embeddings:\n        if self.document_mode:\n            embedding_task = \"retrieval_document\"\n        else:\n            embedding_task = \"retrieval_query\"\n\n        response = client.models.embed_content(\n            model=\"models/text-embedding-004\",\n            contents=input,\n            config=types.EmbedContentConfig(\n                task_type=embedding_task,\n            ),\n        )\n        return [e.values for e in response.embeddings]\n</pre> from chromadb import Documents, EmbeddingFunction, Embeddings from google.api_core import retry  from google.genai import types   # Define a helper to retry when per-minute quota is reached. is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})   class GeminiEmbeddingFunction(EmbeddingFunction):     # Specify whether to generate embeddings for documents, or queries     document_mode = True      @retry.Retry(predicate=is_retriable)     def __call__(self, input: Documents) -&gt; Embeddings:         if self.document_mode:             embedding_task = \"retrieval_document\"         else:             embedding_task = \"retrieval_query\"          response = client.models.embed_content(             model=\"models/text-embedding-004\",             contents=input,             config=types.EmbedContentConfig(                 task_type=embedding_task,             ),         )         return [e.values for e in response.embeddings] <p>Now create a Chroma database client that uses the <code>GeminiEmbeddingFunction</code> and populate the database with the documents you defined above.</p> In\u00a0[20]: Copied! <pre>import chromadb\n\nDB_NAME = \"googlecardb\"\n\nembed_fn = GeminiEmbeddingFunction()\nembed_fn.document_mode = True\n\nchroma_client = chromadb.Client()\ndb = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n\ndb.add(documents=documents, ids=[str(i) for i in range(len(documents))])\n</pre> import chromadb  DB_NAME = \"googlecardb\"  embed_fn = GeminiEmbeddingFunction() embed_fn.document_mode = True  chroma_client = chromadb.Client() db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)  db.add(documents=documents, ids=[str(i) for i in range(len(documents))]) <p>Confirm that the data was inserted by looking at the database.</p> In\u00a0[21]: Copied! <pre>db.count()\n# You can peek at the data too.\n# db.peek(1)\n</pre> db.count() # You can peek at the data too. # db.peek(1) Out[21]: <pre>3</pre> In\u00a0[22]: Copied! <pre># Switch to query mode when generating embeddings.\nembed_fn.document_mode = False\n\n# Search the Chroma DB using the specified query.\nquery = \"How do you use the touchscreen to play music?\"\n\nresult = db.query(query_texts=[query], n_results=1)\n[all_passages] = result[\"documents\"]\n\nMarkdown(all_passages[0])\n</pre> # Switch to query mode when generating embeddings. embed_fn.document_mode = False  # Search the Chroma DB using the specified query. query = \"How do you use the touchscreen to play music?\"  result = db.query(query_texts=[query], n_results=1) [all_passages] = result[\"documents\"]  Markdown(all_passages[0]) Out[22]: <p>Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.</p> In\u00a0[23]: Copied! <pre>query_oneline = query.replace(\"\\n\", \" \")\n\n# This prompt is where you can specify any guidance on tone, or what topics the model should stick to, or avoid.\nprompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below. \nBe sure to respond in a complete sentence, being comprehensive, including all relevant background information. \nHowever, you are talking to a non-technical audience, so be sure to break down complicated concepts and \nstrike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n\nQUESTION: {query_oneline}\n\"\"\"\n\n# Add the retrieved documents to the prompt.\nfor passage in all_passages:\n    passage_oneline = passage.replace(\"\\n\", \" \")\n    prompt += f\"PASSAGE: {passage_oneline}\\n\"\n\nprint(prompt)\n</pre> query_oneline = query.replace(\"\\n\", \" \")  # This prompt is where you can specify any guidance on tone, or what topics the model should stick to, or avoid. prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.  Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.  However, you are talking to a non-technical audience, so be sure to break down complicated concepts and  strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.  QUESTION: {query_oneline} \"\"\"  # Add the retrieved documents to the prompt. for passage in all_passages:     passage_oneline = passage.replace(\"\\n\", \" \")     prompt += f\"PASSAGE: {passage_oneline}\\n\"  print(prompt) <pre>You are a helpful and informative bot that answers questions using text from the reference passage included below. \nBe sure to respond in a complete sentence, being comprehensive, including all relevant background information. \nHowever, you are talking to a non-technical audience, so be sure to break down complicated concepts and \nstrike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n\nQUESTION: How do you use the touchscreen to play music?\nPASSAGE: Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.\n\n</pre> <p>Now use the <code>generate_content</code> method to to generate an answer to the question.</p> In\u00a0[24]: Copied! <pre>answer = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=prompt)\n\nMarkdown(answer.text)\n</pre> answer = client.models.generate_content(     model=\"gemini-2.0-flash\",     contents=prompt)  Markdown(answer.text) Out[24]: <p>To play your favorite songs on the touchscreen display in your Googlecar, simply touch the \"Music\" icon, which will then allow you to access your favorite songs.</p>"},{"location":"day_02/day-2-document-q-a-with-rag/#copyright-2025-google-llc","title":"Copyright 2025 Google LLC.\u00b6","text":""},{"location":"day_02/day-2-document-q-a-with-rag/#day-2-document-qa-with-rag-using-chroma","title":"Day 2 - Document Q&amp;A with RAG using Chroma\u00b6","text":"<p>Welcome back to the Kaggle 5-day Generative AI course!</p> <p>NOTE: The Day 1 notebook contains lots of information for getting set up with Kaggle Notebooks. If you are having any issues, please check out the troubleshooting steps there.</p> <p>Two big limitations of LLMs are 1) that they only \"know\" the information that they were trained on, and 2) that they have limited input context windows. A way to address both of these limitations is to use a technique called Retrieval Augmented Generation, or RAG. A RAG system has three stages:</p> <ol> <li>Indexing</li> <li>Retrieval</li> <li>Generation</li> </ol> <p>Indexing happens ahead of time, and allows you to quickly look up relevant information at query-time. When a query comes in, you retrieve relevant documents, combine them with your instructions and the user's query, and have the LLM generate a tailored answer in natural language using the supplied information. This allows you to provide information that the model hasn't seen before, such as product-specific knowledge or live weather updates.</p> <p>In this notebook you will use the Gemini API to create a vector database, retrieve answers to questions from the database and generate a final answer. You will use Chroma, an open-source vector database. With Chroma, you can store embeddings alongside metadata, embed documents and queries, and search your documents.</p>"},{"location":"day_02/day-2-document-q-a-with-rag/#for-help","title":"For help\u00b6","text":"<p>Common issues are covered in the FAQ and troubleshooting guide.</p>"},{"location":"day_02/day-2-document-q-a-with-rag/#setup","title":"Setup\u00b6","text":"<p>First, install ChromaDB and the Gemini API Python SDK.</p>"},{"location":"day_02/day-2-document-q-a-with-rag/#set-up-your-api-key","title":"Set up your API key\u00b6","text":"<p>To run the following cell, your API key must be stored it in a Kaggle secret named <code>GOOGLE_API_KEY</code>.</p> <p>If you don't already have an API key, you can grab one from AI Studio. You can find detailed instructions in the docs.</p> <p>To make the key available through Kaggle secrets, choose <code>Secrets</code> from the <code>Add-ons</code> menu and follow the instructions to add your key or enable it for this notebook.</p>"},{"location":"day_02/day-2-document-q-a-with-rag/#explore-available-models","title":"Explore available models\u00b6","text":"<p>You will be using the <code>embedContent</code> API method to calculate embeddings in this guide. Find a model that supports it through the <code>models.list</code> endpoint. You can also find more information about the embedding models on the models page.</p> <p><code>text-embedding-004</code> is the most recent generally-available embedding model, so you will use it for this exercise, but try out the experimental <code>gemini-embedding-exp-03-07</code> model too.</p>"},{"location":"day_02/day-2-document-q-a-with-rag/#data","title":"Data\u00b6","text":"<p>Here is a small set of documents you will use to create an embedding database.</p>"},{"location":"day_02/day-2-document-q-a-with-rag/#creating-the-embedding-database-with-chromadb","title":"Creating the embedding database with ChromaDB\u00b6","text":"<p>Create a custom function to generate embeddings with the Gemini API. In this task, you are implementing a retrieval system, so the <code>task_type</code> for generating the document embeddings is <code>retrieval_document</code>. Later, you will use <code>retrieval_query</code> for the query embeddings. Check out the API reference for the full list of supported tasks.</p> <p>Key words: Documents are the items that are in the database. They are inserted first, and later retrieved. Queries are the textual search terms and can be simple keywords or textual descriptions of the desired documents.</p>"},{"location":"day_02/day-2-document-q-a-with-rag/#retrieval-find-relevant-documents","title":"Retrieval: Find relevant documents\u00b6","text":"<p>To search the Chroma database, call the <code>query</code> method. Note that you also switch to the <code>retrieval_query</code> mode of embedding generation.</p>"},{"location":"day_02/day-2-document-q-a-with-rag/#augmented-generation-answer-the-question","title":"Augmented generation: Answer the question\u00b6","text":"<p>Now that you have found a relevant passage from the set of documents (the retrieval step), you can now assemble a generation prompt to have the Gemini API generate a final answer. Note that in this example only a single passage was retrieved. In practice, especially when the size of your underlying data is large, you will want to retrieve more than one result and let the Gemini model determine what passages are relevant in answering the question. For this reason it's OK if some retrieved passages are not directly related to the question - this generation step should ignore them.</p>"},{"location":"day_02/day-2-document-q-a-with-rag/#next-steps","title":"Next steps\u00b6","text":"<p>Congrats on building a Retrieval-Augmented Generation app!</p> <p>To learn more about using embeddings in the Gemini API, check out the Intro to embeddings or to learn more fundamentals, study the embeddings chapter of the Machine Learning Crash Course.</p> <p>For a hosted RAG system, check out the Semantic Retrieval service in the Gemini API. You can implement question-answering on your own documents in a single request, or host a database for even faster responses.</p> <p>- Mark McD</p>"},{"location":"day_02/day-2-embeddings-and-similarity-scores/","title":"Day 2 - Embeddings and similarity scores","text":"In\u00a0[1]: Copied! <pre># @title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # @title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. In\u00a0[2]: Copied! <pre>!pip uninstall -qqy jupyterlab kfp  # Remove unused conflicting packages\n!pip install -U -q \"google-genai==1.7.0\"\n</pre> !pip uninstall -qqy jupyterlab kfp  # Remove unused conflicting packages !pip install -U -q \"google-genai==1.7.0\" In\u00a0[3]: Copied! <pre>from google import genai\nfrom google.genai import types\n\ngenai.__version__\n</pre> from google import genai from google.genai import types  genai.__version__ Out[3]: <pre>'1.7.0'</pre> In\u00a0[4]: Copied! <pre>from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n</pre> from kaggle_secrets import UserSecretsClient  GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\") <p>If you received an error response along the lines of <code>No user secrets exist for kernel id ...</code>, then you need to add your API key via <code>Add-ons</code>, <code>Secrets</code> and enable it.</p> <p></p> In\u00a0[5]: Copied! <pre>client = genai.Client(api_key=GOOGLE_API_KEY)\n\nfor model in client.models.list():\n  if 'embedContent' in model.supported_actions:\n    print(model.name)\n</pre> client = genai.Client(api_key=GOOGLE_API_KEY)  for model in client.models.list():   if 'embedContent' in model.supported_actions:     print(model.name) <pre>models/embedding-001\nmodels/text-embedding-004\nmodels/gemini-embedding-exp-03-07\nmodels/gemini-embedding-exp\n</pre> In\u00a0[6]: Copied! <pre>texts = [\n    'The quick brown fox jumps over the lazy dog.',\n    'The quick rbown fox jumps over the lazy dog.',\n    'teh fast fox jumps over the slow woofer.',\n    'a quick brown fox jmps over lazy dog.',\n    'brown fox jumping over dog',\n    'fox &gt; dog',\n    # Alternative pangram for comparison:\n    'The five boxing wizards jump quickly.',\n    # Unrelated text, also for comparison:\n    'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus et hendrerit massa. Sed pulvinar, nisi a lobortis sagittis, neque risus gravida dolor, in porta dui odio vel purus.',\n]\n\n\nresponse = client.models.embed_content(\n    model='models/text-embedding-004',\n    contents=texts,\n    config=types.EmbedContentConfig(task_type='semantic_similarity'))\n</pre> texts = [     'The quick brown fox jumps over the lazy dog.',     'The quick rbown fox jumps over the lazy dog.',     'teh fast fox jumps over the slow woofer.',     'a quick brown fox jmps over lazy dog.',     'brown fox jumping over dog',     'fox &gt; dog',     # Alternative pangram for comparison:     'The five boxing wizards jump quickly.',     # Unrelated text, also for comparison:     'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus et hendrerit massa. Sed pulvinar, nisi a lobortis sagittis, neque risus gravida dolor, in porta dui odio vel purus.', ]   response = client.models.embed_content(     model='models/text-embedding-004',     contents=texts,     config=types.EmbedContentConfig(task_type='semantic_similarity')) <p>Define a short helper function that will make it easier to display longer embedding texts in our visualisation.</p> In\u00a0[7]: Copied! <pre>def truncate(t: str, limit: int = 50) -&gt; str:\n  \"\"\"Truncate labels to fit on the chart.\"\"\"\n  if len(t) &gt; limit:\n    return t[:limit-3] + '...'\n  else:\n    return t\n\ntruncated_texts = [truncate(t) for t in texts]\n</pre> def truncate(t: str, limit: int = 50) -&gt; str:   \"\"\"Truncate labels to fit on the chart.\"\"\"   if len(t) &gt; limit:     return t[:limit-3] + '...'   else:     return t  truncated_texts = [truncate(t) for t in texts] <p>A similarity score of two embedding vectors can be obtained by calculating their inner product. If $\\mathbf{u}$ is the first embedding vector, and $\\mathbf{v}$ the second, this is $\\mathbf{u}^T \\mathbf{v}$. As the API provides embedding vectors that are normalised to unit length, this is also the cosine similarity.</p> <p>This score can be computed across all embeddings through the matrix self-multiplication: <code>df @ df.T</code>.</p> <p>Note that the range from 0.0 (completely dissimilar) to 1.0 (completely similar) is depicted in the heatmap from light (0.0) to dark (1.0).</p> In\u00a0[8]: Copied! <pre>import pandas as pd\nimport seaborn as sns\n\n\n# Set up the embeddings in a dataframe.\ndf = pd.DataFrame([e.values for e in response.embeddings], index=truncated_texts)\n# Perform the similarity calculation\nsim = df @ df.T\n# Draw!\nsns.heatmap(sim, vmin=0, vmax=1, cmap=\"Greens\");\n</pre> import pandas as pd import seaborn as sns   # Set up the embeddings in a dataframe. df = pd.DataFrame([e.values for e in response.embeddings], index=truncated_texts) # Perform the similarity calculation sim = df @ df.T # Draw! sns.heatmap(sim, vmin=0, vmax=1, cmap=\"Greens\"); <p>You can see the scores for a particular term directly by looking it up in the dataframe.</p> In\u00a0[9]: Copied! <pre>sim['The quick brown fox jumps over the lazy dog.'].sort_values(ascending=False)\n</pre> sim['The quick brown fox jumps over the lazy dog.'].sort_values(ascending=False) Out[9]: <pre>The quick brown fox jumps over the lazy dog.          0.999999\nThe quick rbown fox jumps over the lazy dog.          0.975623\na quick brown fox jmps over lazy dog.                 0.939730\nbrown fox jumping over dog                            0.894507\nteh fast fox jumps over the slow woofer.              0.842152\nfox &gt; dog                                             0.776455\nThe five boxing wizards jump quickly.                 0.635346\nLorem ipsum dolor sit amet, consectetur adipisc...    0.472174\nName: The quick brown fox jumps over the lazy dog., dtype: float64</pre> <p>Try exploring the embeddings of your own datasets, or explore those available in Kaggle datasets.</p>"},{"location":"day_02/day-2-embeddings-and-similarity-scores/#copyright-2025-google-llc","title":"Copyright 2025 Google LLC.\u00b6","text":""},{"location":"day_02/day-2-embeddings-and-similarity-scores/#day-2-embeddings-and-similarity-scores","title":"Day 2 - Embeddings and similarity scores\u00b6","text":"<p>Welcome back to the Kaggle 5-day Generative AI course!</p> <p>In this notebook you will use the Gemini API's embedding endpoint to explore similarity scores.</p> <p>NOTE: The Day 1 notebook contains lots of information for getting set up with Kaggle Notebooks. If you are having any issues, please check out the troubleshooting steps there.</p>"},{"location":"day_02/day-2-embeddings-and-similarity-scores/#for-help","title":"For help\u00b6","text":"<p>Common issues are covered in the FAQ and troubleshooting guide.</p>"},{"location":"day_02/day-2-embeddings-and-similarity-scores/#set-up-the-sdk","title":"Set up the SDK\u00b6","text":""},{"location":"day_02/day-2-embeddings-and-similarity-scores/#set-up-your-api-key","title":"Set up your API key\u00b6","text":"<p>To run the following cell, your API key must be stored it in a Kaggle secret named <code>GOOGLE_API_KEY</code>.</p> <p>If you don't already have an API key, you can grab one from AI Studio. You can find detailed instructions in the docs.</p> <p>To make the key available through Kaggle secrets, choose <code>Secrets</code> from the <code>Add-ons</code> menu and follow the instructions to add your key or enable it for this notebook.</p>"},{"location":"day_02/day-2-embeddings-and-similarity-scores/#explore-available-models","title":"Explore available models\u00b6","text":"<p>You will be using the <code>embedContent</code> API method to calculate batch embeddings in this guide. Find a model that supports it through the <code>models.list</code> endpoint. You can also find more information about the embedding models on the models page.</p>"},{"location":"day_02/day-2-embeddings-and-similarity-scores/#calculate-similarity-scores","title":"Calculate similarity scores\u00b6","text":"<p>This example embeds some variations on the pangram, <code>The quick brown fox jumps over the lazy dog</code>, including spelling mistakes and shortenings of the phrase. Another pangram and a somewhat unrelated phrase have been included for comparison.</p> <p>In this task, you are going to use the embeddings to calculate similarity scores, so the <code>task_type</code> for these embeddings is <code>semantic_similarity</code>. Check out the API reference for the full list of tasks.</p>"},{"location":"day_02/day-2-embeddings-and-similarity-scores/#further-reading","title":"Further reading\u00b6","text":"<ul> <li>Explore search re-ranking using embeddings with the Wikipedia API</li> <li>Perform anomaly detection using embeddings</li> </ul> <p>- Mark McD</p>"},{"location":"day_02/extras/","title":"Day 2 Livestream with Paige Bailey \u2013 5-Day Gen AI Intensive Course | Kaggle","text":""},{"location":"day_02/podcast/","title":"Whitepaper Companion Podcast - Embeddings &amp; Vector Stores","text":""},{"location":"day_02/whitepaper/","title":"Embeddings &amp; Vector Stores","text":"<p>Authors: Anant Nawalgaria, Xiaoqi Ren and Charles Sugnet</p>"},{"location":"day_02/whitepaper/#introduction","title":"Introduction","text":"<p>Modern machine learning thrives on diverse data\u2014images, text, audio, and more. This whitepaper explores the power of embeddings, which transform this heterogeneous data into a unified vector representation for seamless use in various applications. We'll guide you through:</p> <ul> <li>Understanding Embeddings: Why they are essential for handling multimodal data and their diverse applications.</li> <li>Embedding Techniques: Methods and models for mapping different data types into a common vector space.</li> <li>Evaluating Embeddings: Methods for evaluating the quality of embeddings in downstream applications.</li> <li>Vector Databases: Specialized systems for managing and querying embeddings, including practical considerations for production deployment.</li> <li>Real-World Applications: Concrete examples of how embeddings and vector databases are combined with large language models (LLMs) to solve real-world problems.</li> </ul> <p>Throughout the whitepaper, code snippets provide hands-on illustrations of key concepts.</p>"},{"location":"day_02/whitepaper/#read-the-whitepaper-below","title":"Read the whitepaper below","text":""},{"location":"day_03/","title":"Day 3 \u2013 Generative Agents","text":"<p>Build sophisticated AI agents and connect them to real-world tools.</p> <p>Assignments:</p> <ol> <li> <p>Unit 3: \u201cGenerative Agents\u201d </p> <ul> <li>\ud83c\udfa7 Podcast (Optional) </li> <li>\ud83d\udcc4 Whitepaper </li> <li>\ud83e\udde0 Case Study (Optional) </li> <li>\ud83d\udcbb Code Labs:<ul> <li>Function Calling </li> <li>LangGraph Agent</li> </ul> </li> </ul> </li> <li> <p>\ud83c\udfa5 YouTube Livestream Recording</p> </li> </ol>"},{"location":"day_03/day-3-building-an-agent-with-langgraph/","title":"Day 3 - Building an agent with LangGraph and the Gemini API","text":"In\u00a0[2]: Copied! <pre># @title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # @title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. In\u00a0[2]: Copied! <pre># Remove conflicting packages from the Kaggle base environment.\n!pip uninstall -qqy kfp jupyterlab libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai\n# Install langgraph and the packages used in this lab.\n!pip install -qU 'langgraph==0.3.21' 'langchain-google-genai==2.1.2' 'langgraph-prebuilt==0.1.7'\n</pre> # Remove conflicting packages from the Kaggle base environment. !pip uninstall -qqy kfp jupyterlab libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai # Install langgraph and the packages used in this lab. !pip install -qU 'langgraph==0.3.21' 'langchain-google-genai==2.1.2' 'langgraph-prebuilt==0.1.7' In\u00a0[1]: Copied! <pre>import os\nfrom kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nos.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n</pre> import os from kaggle_secrets import UserSecretsClient  GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\") os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY <p>If you received an error response along the lines of <code>No user secrets exist for kernel id ...</code>, then you need to add your API key via <code>Add-ons</code>, <code>Secrets</code> and enable it.</p> <p></p> In\u00a0[3]: Copied! <pre>from typing import Annotated\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph.message import add_messages\n\n\nclass OrderState(TypedDict):\n    \"\"\"State representing the customer's order conversation.\"\"\"\n\n    # The chat conversation. This preserves the conversation history\n    # between nodes. The `add_messages` annotation indicates to LangGraph\n    # that state is updated by appending returned messages, not replacing\n    # them.\n    messages: Annotated[list, add_messages]\n\n    # The customer's in-progress order.\n    order: list[str]\n\n    # Flag indicating that the order is placed and completed.\n    finished: bool\n\n\n# The system instruction defines how the chatbot is expected to behave and includes\n# rules for when to call different functions, as well as rules for the conversation, such\n# as tone and what is permitted for discussion.\nBARISTABOT_SYSINT = (\n    \"system\",  # 'system' indicates the message is a system instruction.\n    \"You are a BaristaBot, an interactive cafe ordering system. A human will talk to you about the \"\n    \"available products you have and you will answer any questions about menu items (and only about \"\n    \"menu items - no off-topic discussion, but you can chat about the products and their history). \"\n    \"The customer will place an order for 1 or more items from the menu, which you will structure \"\n    \"and send to the ordering system after confirming the order with the human. \"\n    \"\\n\\n\"\n    \"Add items to the customer's order with add_to_order, and reset the order with clear_order. \"\n    \"To see the contents of the order so far, call get_order (this is shown to you, not the user) \"\n    \"Always confirm_order with the user (double-check) before calling place_order. Calling confirm_order will \"\n    \"display the order items to the user and returns their response to seeing the list. Their response may contain modifications. \"\n    \"Always verify and respond with drink and modifier names from the MENU before adding them to the order. \"\n    \"If you are unsure a drink or modifier matches those on the MENU, ask a question to clarify or redirect. \"\n    \"You only have the modifiers listed on the menu. \"\n    \"Once the customer has finished ordering items, Call confirm_order to ensure it is correct then make \"\n    \"any necessary updates and then call place_order. Once place_order has returned, thank the user and \"\n    \"say goodbye!\"\n    \"\\n\\n\"\n    \"If any of the tools are unavailable, you can break the fourth wall and tell the user that \"\n    \"they have not implemented them yet and should keep reading to do so.\",\n)\n\n# This is the message with which the system opens the conversation.\nWELCOME_MSG = \"Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?\"\n</pre> from typing import Annotated from typing_extensions import TypedDict  from langgraph.graph.message import add_messages   class OrderState(TypedDict):     \"\"\"State representing the customer's order conversation.\"\"\"      # The chat conversation. This preserves the conversation history     # between nodes. The `add_messages` annotation indicates to LangGraph     # that state is updated by appending returned messages, not replacing     # them.     messages: Annotated[list, add_messages]      # The customer's in-progress order.     order: list[str]      # Flag indicating that the order is placed and completed.     finished: bool   # The system instruction defines how the chatbot is expected to behave and includes # rules for when to call different functions, as well as rules for the conversation, such # as tone and what is permitted for discussion. BARISTABOT_SYSINT = (     \"system\",  # 'system' indicates the message is a system instruction.     \"You are a BaristaBot, an interactive cafe ordering system. A human will talk to you about the \"     \"available products you have and you will answer any questions about menu items (and only about \"     \"menu items - no off-topic discussion, but you can chat about the products and their history). \"     \"The customer will place an order for 1 or more items from the menu, which you will structure \"     \"and send to the ordering system after confirming the order with the human. \"     \"\\n\\n\"     \"Add items to the customer's order with add_to_order, and reset the order with clear_order. \"     \"To see the contents of the order so far, call get_order (this is shown to you, not the user) \"     \"Always confirm_order with the user (double-check) before calling place_order. Calling confirm_order will \"     \"display the order items to the user and returns their response to seeing the list. Their response may contain modifications. \"     \"Always verify and respond with drink and modifier names from the MENU before adding them to the order. \"     \"If you are unsure a drink or modifier matches those on the MENU, ask a question to clarify or redirect. \"     \"You only have the modifiers listed on the menu. \"     \"Once the customer has finished ordering items, Call confirm_order to ensure it is correct then make \"     \"any necessary updates and then call place_order. Once place_order has returned, thank the user and \"     \"say goodbye!\"     \"\\n\\n\"     \"If any of the tools are unavailable, you can break the fourth wall and tell the user that \"     \"they have not implemented them yet and should keep reading to do so.\", )  # This is the message with which the system opens the conversation. WELCOME_MSG = \"Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?\" In\u00a0[4]: Copied! <pre>from langgraph.graph import StateGraph, START, END\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n\n# Try using different models. The Gemini 2.0 flash model is highly\n# capable, great with tools, and has a generous free tier. If you\n# try the older 1.5 models, note that the `pro` models are better at\n# complex multi-tool cases like this, but the `flash` models are\n# faster and have more free quota.\n# Check out the features and quota differences here:\n#  - https://ai.google.dev/gemini-api/docs/models/gemini\nllm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n\n\ndef chatbot(state: OrderState) -&gt; OrderState:\n    \"\"\"The chatbot itself. A simple wrapper around the model's own chat interface.\"\"\"\n    message_history = [BARISTABOT_SYSINT] + state[\"messages\"]\n    return {\"messages\": [llm.invoke(message_history)]}\n\n\n# Set up the initial graph based on our state definition.\ngraph_builder = StateGraph(OrderState)\n\n# Add the chatbot function to the app graph as a node called \"chatbot\".\ngraph_builder.add_node(\"chatbot\", chatbot)\n\n# Define the chatbot node as the app entrypoint.\ngraph_builder.add_edge(START, \"chatbot\")\n\nchat_graph = graph_builder.compile()\n</pre> from langgraph.graph import StateGraph, START, END from langchain_google_genai import ChatGoogleGenerativeAI  # Try using different models. The Gemini 2.0 flash model is highly # capable, great with tools, and has a generous free tier. If you # try the older 1.5 models, note that the `pro` models are better at # complex multi-tool cases like this, but the `flash` models are # faster and have more free quota. # Check out the features and quota differences here: #  - https://ai.google.dev/gemini-api/docs/models/gemini llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")   def chatbot(state: OrderState) -&gt; OrderState:     \"\"\"The chatbot itself. A simple wrapper around the model's own chat interface.\"\"\"     message_history = [BARISTABOT_SYSINT] + state[\"messages\"]     return {\"messages\": [llm.invoke(message_history)]}   # Set up the initial graph based on our state definition. graph_builder = StateGraph(OrderState)  # Add the chatbot function to the app graph as a node called \"chatbot\". graph_builder.add_node(\"chatbot\", chatbot)  # Define the chatbot node as the app entrypoint. graph_builder.add_edge(START, \"chatbot\")  chat_graph = graph_builder.compile() <p>It can be helpful to visualise the graph you just defined. The following code renders the graph.</p> In\u00a0[5]: Copied! <pre>from IPython.display import Image, display\n\nImage(chat_graph.get_graph().draw_mermaid_png())\n</pre> from IPython.display import Image, display  Image(chat_graph.get_graph().draw_mermaid_png()) Out[5]: <p>Now that the graph is defined, you can run it. It only has one node, and one transition into that node, so it will transition from <code>__start__</code> to <code>chatbot</code>, execute the <code>chatbot</code> node, and terminate.</p> <p>To run the graph, you call <code>invoke</code> and pass an initial state object. In this case it begins with the user's initial message.</p> In\u00a0[6]: Copied! <pre>from pprint import pprint\n\nuser_msg = \"Hello, what can you do?\"\nstate = chat_graph.invoke({\"messages\": [user_msg]})\n\n# The state object contains lots of information. Uncomment the pprint lines to see it all.\n# pprint(state)\n\n# Note that the final state now has 2 messages. Our HumanMessage, and an additional AIMessage.\nfor msg in state[\"messages\"]:\n    print(f\"{type(msg).__name__}: {msg.content}\")\n</pre> from pprint import pprint  user_msg = \"Hello, what can you do?\" state = chat_graph.invoke({\"messages\": [user_msg]})  # The state object contains lots of information. Uncomment the pprint lines to see it all. # pprint(state)  # Note that the final state now has 2 messages. Our HumanMessage, and an additional AIMessage. for msg in state[\"messages\"]:     print(f\"{type(msg).__name__}: {msg.content}\") <pre>HumanMessage: Hello, what can you do?\nAIMessage: Hi there! I'm BaristaBot, your friendly cafe ordering system. I can tell you about the items on our menu, answer questions about them, and take your order. I can add items to your order, remove items, and confirm the order with you before sending it to the kitchen. What are you in the mood for today?\n</pre> <p>You could execute this in a Python loop, but for simplicity, manually invoke one more conversational turn. This second invocation takes the state from the first call and appends another user message to elicit another response from the chatbot.</p> In\u00a0[7]: Copied! <pre>user_msg = \"Oh great, what kinds of latte can you make?\"\n\nstate[\"messages\"].append(user_msg)\nstate = chat_graph.invoke(state)\n\n# pprint(state)\nfor msg in state[\"messages\"]:\n    print(f\"{type(msg).__name__}: {msg.content}\")\n</pre> user_msg = \"Oh great, what kinds of latte can you make?\"  state[\"messages\"].append(user_msg) state = chat_graph.invoke(state)  # pprint(state) for msg in state[\"messages\"]:     print(f\"{type(msg).__name__}: {msg.content}\") <pre>HumanMessage: Hello, what can you do?\nAIMessage: Hi there! I'm BaristaBot, your friendly cafe ordering system. I can tell you about the items on our menu, answer questions about them, and take your order. I can add items to your order, remove items, and confirm the order with you before sending it to the kitchen. What are you in the mood for today?\nHumanMessage: Oh great, what kinds of latte can you make?\nAIMessage: We have a few delicious latte options:\n\n*   **Latte:** Our classic latte, made with espresso and steamed milk, topped with a thin layer of foam.\n*   **Mocha Latte:** A chocolatey twist on the classic latte, made with espresso, chocolate syrup, steamed milk, and topped with whipped cream.\n*   **Caramel Latte:** A sweet and decadent latte, made with espresso, caramel syrup, steamed milk, and topped with caramel drizzle.\n*   **Vanilla Latte:** A simple and satisfying latte, made with espresso, vanilla syrup, and steamed milk.\n\nWould you like to know more about any of these?\n</pre> In\u00a0[8]: Copied! <pre>from langchain_core.messages.ai import AIMessage\n\n\ndef human_node(state: OrderState) -&gt; OrderState:\n    \"\"\"Display the last model message to the user, and receive the user's input.\"\"\"\n    last_msg = state[\"messages\"][-1]\n    print(\"Model:\", last_msg.content)\n\n    user_input = input(\"User: \")\n\n    # If it looks like the user is trying to quit, flag the conversation\n    # as over.\n    if user_input in {\"q\", \"quit\", \"exit\", \"goodbye\"}:\n        state[\"finished\"] = True\n\n    return state | {\"messages\": [(\"user\", user_input)]}\n\n\ndef chatbot_with_welcome_msg(state: OrderState) -&gt; OrderState:\n    \"\"\"The chatbot itself. A wrapper around the model's own chat interface.\"\"\"\n\n    if state[\"messages\"]:\n        # If there are messages, continue the conversation with the Gemini model.\n        new_output = llm.invoke([BARISTABOT_SYSINT] + state[\"messages\"])\n    else:\n        # If there are no messages, start with the welcome message.\n        new_output = AIMessage(content=WELCOME_MSG)\n\n    return state | {\"messages\": [new_output]}\n\n\n# Start building a new graph.\ngraph_builder = StateGraph(OrderState)\n\n# Add the chatbot and human nodes to the app graph.\ngraph_builder.add_node(\"chatbot\", chatbot_with_welcome_msg)\ngraph_builder.add_node(\"human\", human_node)\n\n# Start with the chatbot again.\ngraph_builder.add_edge(START, \"chatbot\")\n\n# The chatbot will always go to the human next.\ngraph_builder.add_edge(\"chatbot\", \"human\");\n</pre> from langchain_core.messages.ai import AIMessage   def human_node(state: OrderState) -&gt; OrderState:     \"\"\"Display the last model message to the user, and receive the user's input.\"\"\"     last_msg = state[\"messages\"][-1]     print(\"Model:\", last_msg.content)      user_input = input(\"User: \")      # If it looks like the user is trying to quit, flag the conversation     # as over.     if user_input in {\"q\", \"quit\", \"exit\", \"goodbye\"}:         state[\"finished\"] = True      return state | {\"messages\": [(\"user\", user_input)]}   def chatbot_with_welcome_msg(state: OrderState) -&gt; OrderState:     \"\"\"The chatbot itself. A wrapper around the model's own chat interface.\"\"\"      if state[\"messages\"]:         # If there are messages, continue the conversation with the Gemini model.         new_output = llm.invoke([BARISTABOT_SYSINT] + state[\"messages\"])     else:         # If there are no messages, start with the welcome message.         new_output = AIMessage(content=WELCOME_MSG)      return state | {\"messages\": [new_output]}   # Start building a new graph. graph_builder = StateGraph(OrderState)  # Add the chatbot and human nodes to the app graph. graph_builder.add_node(\"chatbot\", chatbot_with_welcome_msg) graph_builder.add_node(\"human\", human_node)  # Start with the chatbot again. graph_builder.add_edge(START, \"chatbot\")  # The chatbot will always go to the human next. graph_builder.add_edge(\"chatbot\", \"human\"); <p>Before you can run this, note that if you added an edge from <code>human</code> back to <code>chatbot</code>, the graph will cycle forever as there is no exit condition. One way to break the cycle is to add a check for a human input like <code>q</code> or <code>quit</code> and use that to break the loop.</p> <p>In LangGraph, this is achieved with a conditional edge. This is similar to a regular graph transition, except a custom function is called to determine which edge to traverse.</p> <p>Conditional edge functions take the state as input, and return a string representing the name of the node to which it will transition.</p> In\u00a0[9]: Copied! <pre>from typing import Literal\n\n\ndef maybe_exit_human_node(state: OrderState) -&gt; Literal[\"chatbot\", \"__end__\"]:\n    \"\"\"Route to the chatbot, unless it looks like the user is exiting.\"\"\"\n    if state.get(\"finished\", False):\n        return END\n    else:\n        return \"chatbot\"\n\n\ngraph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n\nchat_with_human_graph = graph_builder.compile()\n\nImage(chat_with_human_graph.get_graph().draw_mermaid_png())\n</pre> from typing import Literal   def maybe_exit_human_node(state: OrderState) -&gt; Literal[\"chatbot\", \"__end__\"]:     \"\"\"Route to the chatbot, unless it looks like the user is exiting.\"\"\"     if state.get(\"finished\", False):         return END     else:         return \"chatbot\"   graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)  chat_with_human_graph = graph_builder.compile()  Image(chat_with_human_graph.get_graph().draw_mermaid_png()) Out[9]: <p>Run this new graph to see how the interaction loop is now captured within the graph. Input <code>quit</code> to exit the program.</p> <p>You must uncomment the <code>.invoke(...)</code> line to run this step.</p> In\u00a0[10]: Copied! <pre># The default recursion limit for traversing nodes is 25 - setting it higher means\n# you can try a more complex order with multiple steps and round-trips (and you\n# can chat for longer!)\nconfig = {\"recursion_limit\": 100}\n\n# Remember that this will loop forever, unless you input `q`, `quit` or one of the\n# other exit terms defined in `human_node`.\n# Uncomment this line to execute the graph:\n# state = chat_with_human_graph.invoke({\"messages\": []}, config)\n\n# Things to try:\n#  - Just chat! There's no ordering or menu yet.\n#  - 'q' to exit.\n\n# pprint(state)\n</pre> # The default recursion limit for traversing nodes is 25 - setting it higher means # you can try a more complex order with multiple steps and round-trips (and you # can chat for longer!) config = {\"recursion_limit\": 100}  # Remember that this will loop forever, unless you input `q`, `quit` or one of the # other exit terms defined in `human_node`. # Uncomment this line to execute the graph: # state = chat_with_human_graph.invoke({\"messages\": []}, config)  # Things to try: #  - Just chat! There's no ordering or menu yet. #  - 'q' to exit.  # pprint(state) <pre>Model: Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?\n</pre> <pre>User:  hello!\n</pre> <pre>Model: Hi there! What can I get for you today? We have a fine selection of coffees, teas, and pastries.\n</pre> <pre>User:  q\n</pre> In\u00a0[11]: Copied! <pre>from langchain_core.tools import tool\n\n\n@tool\ndef get_menu() -&gt; str:\n    \"\"\"Provide the latest up-to-date menu.\"\"\"\n    # Note that this is just hard-coded text, but you could connect this to a live stock\n    # database, or you could use Gemini's multi-modal capabilities and take live photos of\n    # your cafe's chalk menu or the products on the counter and assmble them into an input.\n\n    return \"\"\"\n    MENU:\n    Coffee Drinks:\n    Espresso\n    Americano\n    Cold Brew\n\n    Coffee Drinks with Milk:\n    Latte\n    Cappuccino\n    Cortado\n    Macchiato\n    Mocha\n    Flat White\n\n    Tea Drinks:\n    English Breakfast Tea\n    Green Tea\n    Earl Grey\n\n    Tea Drinks with Milk:\n    Chai Latte\n    Matcha Latte\n    London Fog\n\n    Other Drinks:\n    Steamer\n    Hot Chocolate\n\n    Modifiers:\n    Milk options: Whole, 2%, Oat, Almond, 2% Lactose Free; Default option: whole\n    Espresso shots: Single, Double, Triple, Quadruple; default: Double\n    Caffeine: Decaf, Regular; default: Regular\n    Hot-Iced: Hot, Iced; Default: Hot\n    Sweeteners (option to add one or more): vanilla sweetener, hazelnut sweetener, caramel sauce, chocolate sauce, sugar free vanilla sweetener\n    Special requests: any reasonable modification that does not involve items not on the menu, for example: 'extra hot', 'one pump', 'half caff', 'extra foam', etc.\n\n    \"dirty\" means add a shot of espresso to a drink that doesn't usually have it, like \"Dirty Chai Latte\".\n    \"Regular milk\" is the same as 'whole milk'.\n    \"Sweetened\" means add some regular sugar, not a sweetener.\n\n    Soy milk has run out of stock today, so soy is not available.\n  \"\"\"\n</pre> from langchain_core.tools import tool   @tool def get_menu() -&gt; str:     \"\"\"Provide the latest up-to-date menu.\"\"\"     # Note that this is just hard-coded text, but you could connect this to a live stock     # database, or you could use Gemini's multi-modal capabilities and take live photos of     # your cafe's chalk menu or the products on the counter and assmble them into an input.      return \"\"\"     MENU:     Coffee Drinks:     Espresso     Americano     Cold Brew      Coffee Drinks with Milk:     Latte     Cappuccino     Cortado     Macchiato     Mocha     Flat White      Tea Drinks:     English Breakfast Tea     Green Tea     Earl Grey      Tea Drinks with Milk:     Chai Latte     Matcha Latte     London Fog      Other Drinks:     Steamer     Hot Chocolate      Modifiers:     Milk options: Whole, 2%, Oat, Almond, 2% Lactose Free; Default option: whole     Espresso shots: Single, Double, Triple, Quadruple; default: Double     Caffeine: Decaf, Regular; default: Regular     Hot-Iced: Hot, Iced; Default: Hot     Sweeteners (option to add one or more): vanilla sweetener, hazelnut sweetener, caramel sauce, chocolate sauce, sugar free vanilla sweetener     Special requests: any reasonable modification that does not involve items not on the menu, for example: 'extra hot', 'one pump', 'half caff', 'extra foam', etc.      \"dirty\" means add a shot of espresso to a drink that doesn't usually have it, like \"Dirty Chai Latte\".     \"Regular milk\" is the same as 'whole milk'.     \"Sweetened\" means add some regular sugar, not a sweetener.      Soy milk has run out of stock today, so soy is not available.   \"\"\" <p>Now add the new tool to the graph. The <code>get_menu</code> tool is wrapped in a <code>ToolNode</code> that handles calling the tool and passing the response as a message through the graph. The tools are also bound to the <code>llm</code> object so that the underlying model knows they exist. As you now have a different <code>llm</code> object to invoke, you need to update the <code>chatbot</code> node so that it is aware of the tools.</p> In\u00a0[12]: Copied! <pre>from langgraph.prebuilt import ToolNode\n\n\n# Define the tools and create a \"tools\" node.\ntools = [get_menu]\ntool_node = ToolNode(tools)\n\n# Attach the tools to the model so that it knows what it can call.\nllm_with_tools = llm.bind_tools(tools)\n\n\ndef maybe_route_to_tools(state: OrderState) -&gt; Literal[\"tools\", \"human\"]:\n    \"\"\"Route between human or tool nodes, depending if a tool call is made.\"\"\"\n    if not (msgs := state.get(\"messages\", [])):\n        raise ValueError(f\"No messages found when parsing state: {state}\")\n\n    # Only route based on the last message.\n    msg = msgs[-1]\n\n    # When the chatbot returns tool_calls, route to the \"tools\" node.\n    if hasattr(msg, \"tool_calls\") and len(msg.tool_calls) &gt; 0:\n        return \"tools\"\n    else:\n        return \"human\"\n\n\ndef chatbot_with_tools(state: OrderState) -&gt; OrderState:\n    \"\"\"The chatbot with tools. A simple wrapper around the model's own chat interface.\"\"\"\n    defaults = {\"order\": [], \"finished\": False}\n\n    if state[\"messages\"]:\n        new_output = llm_with_tools.invoke([BARISTABOT_SYSINT] + state[\"messages\"])\n    else:\n        new_output = AIMessage(content=WELCOME_MSG)\n\n    # Set up some defaults if not already set, then pass through the provided state,\n    # overriding only the \"messages\" field.\n    return defaults | state | {\"messages\": [new_output]}\n\n\ngraph_builder = StateGraph(OrderState)\n\n# Add the nodes, including the new tool_node.\ngraph_builder.add_node(\"chatbot\", chatbot_with_tools)\ngraph_builder.add_node(\"human\", human_node)\ngraph_builder.add_node(\"tools\", tool_node)\n\n# Chatbot may go to tools, or human.\ngraph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools)\n# Human may go back to chatbot, or exit.\ngraph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n\n# Tools always route back to chat afterwards.\ngraph_builder.add_edge(\"tools\", \"chatbot\")\n\ngraph_builder.add_edge(START, \"chatbot\")\ngraph_with_menu = graph_builder.compile()\n\nImage(graph_with_menu.get_graph().draw_mermaid_png())\n</pre> from langgraph.prebuilt import ToolNode   # Define the tools and create a \"tools\" node. tools = [get_menu] tool_node = ToolNode(tools)  # Attach the tools to the model so that it knows what it can call. llm_with_tools = llm.bind_tools(tools)   def maybe_route_to_tools(state: OrderState) -&gt; Literal[\"tools\", \"human\"]:     \"\"\"Route between human or tool nodes, depending if a tool call is made.\"\"\"     if not (msgs := state.get(\"messages\", [])):         raise ValueError(f\"No messages found when parsing state: {state}\")      # Only route based on the last message.     msg = msgs[-1]      # When the chatbot returns tool_calls, route to the \"tools\" node.     if hasattr(msg, \"tool_calls\") and len(msg.tool_calls) &gt; 0:         return \"tools\"     else:         return \"human\"   def chatbot_with_tools(state: OrderState) -&gt; OrderState:     \"\"\"The chatbot with tools. A simple wrapper around the model's own chat interface.\"\"\"     defaults = {\"order\": [], \"finished\": False}      if state[\"messages\"]:         new_output = llm_with_tools.invoke([BARISTABOT_SYSINT] + state[\"messages\"])     else:         new_output = AIMessage(content=WELCOME_MSG)      # Set up some defaults if not already set, then pass through the provided state,     # overriding only the \"messages\" field.     return defaults | state | {\"messages\": [new_output]}   graph_builder = StateGraph(OrderState)  # Add the nodes, including the new tool_node. graph_builder.add_node(\"chatbot\", chatbot_with_tools) graph_builder.add_node(\"human\", human_node) graph_builder.add_node(\"tools\", tool_node)  # Chatbot may go to tools, or human. graph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools) # Human may go back to chatbot, or exit. graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)  # Tools always route back to chat afterwards. graph_builder.add_edge(\"tools\", \"chatbot\")  graph_builder.add_edge(START, \"chatbot\") graph_with_menu = graph_builder.compile()  Image(graph_with_menu.get_graph().draw_mermaid_png()) Out[12]: <p>Now run the new graph to see how the model uses the menu.</p> <p>You must uncomment the <code>.invoke(...)</code> line to run this step.</p> In\u00a0[13]: Copied! <pre># Remember that you have not implemented ordering yet, so this will loop forever,\n# unless you input `q`, `quit` or one of the other exit terms defined in the\n# `human_node`.\n# Uncomment this line to execute the graph:\n# state = graph_with_menu.invoke({\"messages\": []}, config)\n\n# Things to try:\n# - I'd love an espresso drink, what have you got?\n# - What teas do you have?\n# - Can you do a long black? (this is on the menu as an \"Americano\" - see if it can\n#   figure it out)\n# - 'q' to exit.\n\n\n# pprint(state)\n</pre> # Remember that you have not implemented ordering yet, so this will loop forever, # unless you input `q`, `quit` or one of the other exit terms defined in the # `human_node`. # Uncomment this line to execute the graph: # state = graph_with_menu.invoke({\"messages\": []}, config)  # Things to try: # - I'd love an espresso drink, what have you got? # - What teas do you have? # - Can you do a long black? (this is on the menu as an \"Americano\" - see if it can #   figure it out) # - 'q' to exit.   # pprint(state) <pre>Model: Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?\n</pre> <pre>User:  what espresso drinks you got?\n</pre> <pre>Model: We have Espresso, Americano, Latte, Cappuccino, Cortado, Macchiato, Mocha, and Flat White.\n</pre> <pre>User:  can i get a long black?\n</pre> <pre>Model: I do not see Long Black on the menu. Did you mean to order an Americano? It is espresso and hot water.\n</pre> <pre>User:  yes please!\n</pre> <pre>Model: Sorry, that function is not available. You will need to implement it for me to use it.\n</pre> <pre>User:  q\n</pre> In\u00a0[14]: Copied! <pre>from collections.abc import Iterable\nfrom random import randint\n\nfrom langchain_core.messages.tool import ToolMessage\n\n# These functions have no body; LangGraph does not allow @tools to update\n# the conversation state, so you will implement a separate node to handle\n# state updates. Using @tools is still very convenient for defining the tool\n# schema, so empty functions have been defined that will be bound to the LLM\n# but their implementation is deferred to the order_node.\n\n\n@tool\ndef add_to_order(drink: str, modifiers: Iterable[str]) -&gt; str:\n    \"\"\"Adds the specified drink to the customer's order, including any modifiers.\n\n    Returns:\n      The updated order in progress.\n    \"\"\"\n\n\n@tool\ndef confirm_order() -&gt; str:\n    \"\"\"Asks the customer if the order is correct.\n\n    Returns:\n      The user's free-text response.\n    \"\"\"\n\n\n@tool\ndef get_order() -&gt; str:\n    \"\"\"Returns the users order so far. One item per line.\"\"\"\n\n\n@tool\ndef clear_order():\n    \"\"\"Removes all items from the user's order.\"\"\"\n\n\n@tool\ndef place_order() -&gt; int:\n    \"\"\"Sends the order to the barista for fulfillment.\n\n    Returns:\n      The estimated number of minutes until the order is ready.\n    \"\"\"\n\n\ndef order_node(state: OrderState) -&gt; OrderState:\n    \"\"\"The ordering node. This is where the order state is manipulated.\"\"\"\n    tool_msg = state.get(\"messages\", [])[-1]\n    order = state.get(\"order\", [])\n    outbound_msgs = []\n    order_placed = False\n\n    for tool_call in tool_msg.tool_calls:\n\n        if tool_call[\"name\"] == \"add_to_order\":\n\n            # Each order item is just a string. This is where it assembled as \"drink (modifiers, ...)\".\n            modifiers = tool_call[\"args\"][\"modifiers\"]\n            modifier_str = \", \".join(modifiers) if modifiers else \"no modifiers\"\n\n            order.append(f'{tool_call[\"args\"][\"drink\"]} ({modifier_str})')\n            response = \"\\n\".join(order)\n\n        elif tool_call[\"name\"] == \"confirm_order\":\n\n            # We could entrust the LLM to do order confirmation, but it is a good practice to\n            # show the user the exact data that comprises their order so that what they confirm\n            # precisely matches the order that goes to the kitchen - avoiding hallucination\n            # or reality skew.\n\n            # In a real scenario, this is where you would connect your POS screen to show the\n            # order to the user.\n\n            print(\"Your order:\")\n            if not order:\n                print(\"  (no items)\")\n\n            for drink in order:\n                print(f\"  {drink}\")\n\n            response = input(\"Is this correct? \")\n\n        elif tool_call[\"name\"] == \"get_order\":\n\n            response = \"\\n\".join(order) if order else \"(no order)\"\n\n        elif tool_call[\"name\"] == \"clear_order\":\n\n            order.clear()\n            response = None\n\n        elif tool_call[\"name\"] == \"place_order\":\n\n            order_text = \"\\n\".join(order)\n            print(\"Sending order to kitchen!\")\n            print(order_text)\n\n            # TODO(you!): Implement cafe.\n            order_placed = True\n            response = randint(1, 5)  # ETA in minutes\n\n        else:\n            raise NotImplementedError(f'Unknown tool call: {tool_call[\"name\"]}')\n\n        # Record the tool results as tool messages.\n        outbound_msgs.append(\n            ToolMessage(\n                content=response,\n                name=tool_call[\"name\"],\n                tool_call_id=tool_call[\"id\"],\n            )\n        )\n\n    return {\"messages\": outbound_msgs, \"order\": order, \"finished\": order_placed}\n\n\ndef maybe_route_to_tools(state: OrderState) -&gt; str:\n    \"\"\"Route between chat and tool nodes if a tool call is made.\"\"\"\n    if not (msgs := state.get(\"messages\", [])):\n        raise ValueError(f\"No messages found when parsing state: {state}\")\n\n    msg = msgs[-1]\n\n    if state.get(\"finished\", False):\n        # When an order is placed, exit the app. The system instruction indicates\n        # that the chatbot should say thanks and goodbye at this point, so we can exit\n        # cleanly.\n        return END\n\n    elif hasattr(msg, \"tool_calls\") and len(msg.tool_calls) &gt; 0:\n        # Route to `tools` node for any automated tool calls first.\n        if any(\n            tool[\"name\"] in tool_node.tools_by_name.keys() for tool in msg.tool_calls\n        ):\n            return \"tools\"\n        else:\n            return \"ordering\"\n\n    else:\n        return \"human\"\n</pre> from collections.abc import Iterable from random import randint  from langchain_core.messages.tool import ToolMessage  # These functions have no body; LangGraph does not allow @tools to update # the conversation state, so you will implement a separate node to handle # state updates. Using @tools is still very convenient for defining the tool # schema, so empty functions have been defined that will be bound to the LLM # but their implementation is deferred to the order_node.   @tool def add_to_order(drink: str, modifiers: Iterable[str]) -&gt; str:     \"\"\"Adds the specified drink to the customer's order, including any modifiers.      Returns:       The updated order in progress.     \"\"\"   @tool def confirm_order() -&gt; str:     \"\"\"Asks the customer if the order is correct.      Returns:       The user's free-text response.     \"\"\"   @tool def get_order() -&gt; str:     \"\"\"Returns the users order so far. One item per line.\"\"\"   @tool def clear_order():     \"\"\"Removes all items from the user's order.\"\"\"   @tool def place_order() -&gt; int:     \"\"\"Sends the order to the barista for fulfillment.      Returns:       The estimated number of minutes until the order is ready.     \"\"\"   def order_node(state: OrderState) -&gt; OrderState:     \"\"\"The ordering node. This is where the order state is manipulated.\"\"\"     tool_msg = state.get(\"messages\", [])[-1]     order = state.get(\"order\", [])     outbound_msgs = []     order_placed = False      for tool_call in tool_msg.tool_calls:          if tool_call[\"name\"] == \"add_to_order\":              # Each order item is just a string. This is where it assembled as \"drink (modifiers, ...)\".             modifiers = tool_call[\"args\"][\"modifiers\"]             modifier_str = \", \".join(modifiers) if modifiers else \"no modifiers\"              order.append(f'{tool_call[\"args\"][\"drink\"]} ({modifier_str})')             response = \"\\n\".join(order)          elif tool_call[\"name\"] == \"confirm_order\":              # We could entrust the LLM to do order confirmation, but it is a good practice to             # show the user the exact data that comprises their order so that what they confirm             # precisely matches the order that goes to the kitchen - avoiding hallucination             # or reality skew.              # In a real scenario, this is where you would connect your POS screen to show the             # order to the user.              print(\"Your order:\")             if not order:                 print(\"  (no items)\")              for drink in order:                 print(f\"  {drink}\")              response = input(\"Is this correct? \")          elif tool_call[\"name\"] == \"get_order\":              response = \"\\n\".join(order) if order else \"(no order)\"          elif tool_call[\"name\"] == \"clear_order\":              order.clear()             response = None          elif tool_call[\"name\"] == \"place_order\":              order_text = \"\\n\".join(order)             print(\"Sending order to kitchen!\")             print(order_text)              # TODO(you!): Implement cafe.             order_placed = True             response = randint(1, 5)  # ETA in minutes          else:             raise NotImplementedError(f'Unknown tool call: {tool_call[\"name\"]}')          # Record the tool results as tool messages.         outbound_msgs.append(             ToolMessage(                 content=response,                 name=tool_call[\"name\"],                 tool_call_id=tool_call[\"id\"],             )         )      return {\"messages\": outbound_msgs, \"order\": order, \"finished\": order_placed}   def maybe_route_to_tools(state: OrderState) -&gt; str:     \"\"\"Route between chat and tool nodes if a tool call is made.\"\"\"     if not (msgs := state.get(\"messages\", [])):         raise ValueError(f\"No messages found when parsing state: {state}\")      msg = msgs[-1]      if state.get(\"finished\", False):         # When an order is placed, exit the app. The system instruction indicates         # that the chatbot should say thanks and goodbye at this point, so we can exit         # cleanly.         return END      elif hasattr(msg, \"tool_calls\") and len(msg.tool_calls) &gt; 0:         # Route to `tools` node for any automated tool calls first.         if any(             tool[\"name\"] in tool_node.tools_by_name.keys() for tool in msg.tool_calls         ):             return \"tools\"         else:             return \"ordering\"      else:         return \"human\" <p>Now define the graph. The LLM needs to know about the tools too, so that it can invoke them. Here you set up 2 sets of tools corresponding to the nodes under which they operate: automated and ordering.</p> In\u00a0[15]: Copied! <pre># Auto-tools will be invoked automatically by the ToolNode\nauto_tools = [get_menu]\ntool_node = ToolNode(auto_tools)\n\n# Order-tools will be handled by the order node.\norder_tools = [add_to_order, confirm_order, get_order, clear_order, place_order]\n\n# The LLM needs to know about all of the tools, so specify everything here.\nllm_with_tools = llm.bind_tools(auto_tools + order_tools)\n\n\ngraph_builder = StateGraph(OrderState)\n\n# Nodes\ngraph_builder.add_node(\"chatbot\", chatbot_with_tools)\ngraph_builder.add_node(\"human\", human_node)\ngraph_builder.add_node(\"tools\", tool_node)\ngraph_builder.add_node(\"ordering\", order_node)\n\n# Chatbot -&gt; {ordering, tools, human, END}\ngraph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools)\n# Human -&gt; {chatbot, END}\ngraph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n\n# Tools (both kinds) always route back to chat afterwards.\ngraph_builder.add_edge(\"tools\", \"chatbot\")\ngraph_builder.add_edge(\"ordering\", \"chatbot\")\n\ngraph_builder.add_edge(START, \"chatbot\")\ngraph_with_order_tools = graph_builder.compile()\n\nImage(graph_with_order_tools.get_graph().draw_mermaid_png())\n</pre> # Auto-tools will be invoked automatically by the ToolNode auto_tools = [get_menu] tool_node = ToolNode(auto_tools)  # Order-tools will be handled by the order node. order_tools = [add_to_order, confirm_order, get_order, clear_order, place_order]  # The LLM needs to know about all of the tools, so specify everything here. llm_with_tools = llm.bind_tools(auto_tools + order_tools)   graph_builder = StateGraph(OrderState)  # Nodes graph_builder.add_node(\"chatbot\", chatbot_with_tools) graph_builder.add_node(\"human\", human_node) graph_builder.add_node(\"tools\", tool_node) graph_builder.add_node(\"ordering\", order_node)  # Chatbot -&gt; {ordering, tools, human, END} graph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools) # Human -&gt; {chatbot, END} graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)  # Tools (both kinds) always route back to chat afterwards. graph_builder.add_edge(\"tools\", \"chatbot\") graph_builder.add_edge(\"ordering\", \"chatbot\")  graph_builder.add_edge(START, \"chatbot\") graph_with_order_tools = graph_builder.compile()  Image(graph_with_order_tools.get_graph().draw_mermaid_png()) Out[15]: <p>Now run the complete ordering system graph.</p> <p>You must uncomment the <code>.invoke(...)</code> line to run this step.</p> In\u00a0[16]: Copied! <pre># Uncomment this line to execute the graph:\n# state = graph_with_order_tools.invoke({\"messages\": []}, config)\n\n# Things to try:\n# - Order a drink!\n# - Make a change to your order.\n# - \"Which teas are from England?\"\n# - Note that the graph should naturally exit after placing an order.\n\n# pprint(state)\n</pre> # Uncomment this line to execute the graph: # state = graph_with_order_tools.invoke({\"messages\": []}, config)  # Things to try: # - Order a drink! # - Make a change to your order. # - \"Which teas are from England?\" # - Note that the graph should naturally exit after placing an order.  # pprint(state) <pre>Model: Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?\n</pre> <pre>User:  What tea do you have?\n</pre> <pre>Model: We have English Breakfast Tea, Green Tea, and Earl Grey.\n</pre> <pre>User:  Earl grey, hot!\n</pre> <pre>Model: OK, I have added an Earl Grey Tea, served hot, to your order. Anything else?\n</pre> <pre>User:  1 caramel latte pls\n</pre> <pre>Model: OK, I've added a Latte with caramel sauce. Is there anything else?\n</pre> <pre>User:  no that's everything\n</pre> <pre>Your order:\n  Earl Grey (Hot)\n  Latte (caramel sauce)\n</pre> <pre>Is this correct?  y\n</pre> <pre>Model: You have ordered an Earl Grey Tea (Hot) and a Latte (caramel sauce). Is that correct?\n</pre> <pre>User:  y\n</pre> <pre>Sending order to kitchen!\nEarl Grey (Hot)\nLatte (caramel sauce)\n</pre> <p>The order state has been captured both in the <code>place_order</code> function and in the final conversational state returned from executing the graph. This iillustrates how you can integrate your own systems to a graph app, as well as collect the final results of executing such an app.</p> In\u00a0[17]: Copied! <pre># Uncomment this once you have run the graph from the previous cell.\n# pprint(state[\"order\"])\n</pre> # Uncomment this once you have run the graph from the previous cell. # pprint(state[\"order\"]) <pre>['Earl Grey (Hot)', 'Latte (caramel sauce)']\n</pre>"},{"location":"day_03/day-3-building-an-agent-with-langgraph/#copyright-2025-google-llc","title":"Copyright 2025 Google LLC.\u00b6","text":""},{"location":"day_03/day-3-building-an-agent-with-langgraph/#day-3-building-an-agent-with-langgraph-and-the-gemini-api","title":"Day 3 - Building an agent with LangGraph and the Gemini API\u00b6","text":"<p>Welcome back to the Kaggle 5-day Generative AI course!</p> <p>In this notebook, you will use LangGraph to define a stateful graph-based application built on top of the Gemini API.</p> <p>You will build a simulated cafe ordering system, called BaristaBot. It will provide a looping chat interface to customers where they can order cafe beverages using natural language, and you will build nodes to represent the cafe's live menu and the \"back room\" ordering system.</p> <p>BaristaBot is used in other Gemini API demos, so if you are looking to explore something with a more minimal implementation, check out the BaristaBot function calling example that implements a similar system using only the Gemini API Python SDK and function calling.</p>"},{"location":"day_03/day-3-building-an-agent-with-langgraph/#important","title":"IMPORTANT!\u00b6","text":"<p>The app built in this notebook takes user input using a text box (Python's <code>input</code>). These are commented-out to ensure that you can use the <code>Run all</code> feature without interruption. Keep an eye out for the steps where you need to uncomment the <code>.invoke(...)</code> calls in order to interact with the app.</p> <p>If you wish to save a version of this notebook with <code>Save and Run all</code>, you will need to re-comment the lines you commented-out to ensure that the notebook can run without human input.</p>"},{"location":"day_03/day-3-building-an-agent-with-langgraph/#for-help","title":"For help\u00b6","text":"<p>Common issues are covered in the FAQ and troubleshooting guide.</p>"},{"location":"day_03/day-3-building-an-agent-with-langgraph/#get-set-up","title":"Get set up\u00b6","text":"<p>Start by installing and importing the LangGraph SDK and LangChain support for the Gemini API.</p>"},{"location":"day_03/day-3-building-an-agent-with-langgraph/#set-up-your-api-key","title":"Set up your API key\u00b6","text":"<p>The <code>GOOGLE_API_KEY</code> environment variable can be set to automatically configure the underlying API. This works for both the official Gemini Python SDK and for LangChain/LangGraph.</p> <p>To run the following cell, your API key must be stored it in a Kaggle secret named <code>GOOGLE_API_KEY</code>.</p> <p>If you don't already have an API key, you can grab one from AI Studio. You can find detailed instructions in the docs.</p> <p>To make the key available through Kaggle secrets, choose <code>Secrets</code> from the <code>Add-ons</code> menu and follow the instructions to add your key or enable it for this notebook.</p>"},{"location":"day_03/day-3-building-an-agent-with-langgraph/#key-concepts","title":"Key concepts\u00b6","text":"<p>LangGraph applications are built around a graph structure. As the developer, you define an application graph that models the state transitions for your application. Your app will define a state schema, and an instance of that schema is propagated through the graph.</p> <p>Each node in the graph represents an action or step that can be taken. Nodes will make changes to the state in some way through code that you define. These changes can be the result of invoking an LLM, by calling an API, or executing any logic that the node defines.</p> <p>Each edge in the graph represents a transition between states, defining the flow of the program. Edge transitions can be fixed, for example if you define a text-only chatbot where output is always displayed to a user, you may always transition from <code>chatbot -&gt; user</code>. The transitions can also be conditional, allowing you to add branching (like an <code>if-else</code> statement) or looping (like <code>for</code> or <code>while</code> loops).</p> <p>LangGraph is highly extensible and provides a number of features that are not part of this tutorial, such as memory, persistance and streaming. To better understand the key concepts and philophies behind LangGraph, check out their Conceptual guides and High-level overview.</p>"},{"location":"day_03/day-3-building-an-agent-with-langgraph/#define-core-instructions","title":"Define core instructions\u00b6","text":"<p>State is a fundamental concept for a LangGraph app. A state object is passed between every node and transition in the app. Here you define a state object, <code>OrderState</code>, that holds the conversation history, a structured order, and a flag indicating if the customer has finished placing their order. For simplicity, the \"structure\" in this order is just a list of strings, but this can be expanded to any Python data structure.</p> <p>In Python, the LangGraph state object is a Python dictionary. You can provide a schema for this dictionary by defining it as a <code>TypedDict</code>.</p> <p>Here you also define the system instruction that the Gemini model will use. You can capture tone and style here, as well as the playbook under which the chatbot should operate.</p>"},{"location":"day_03/day-3-building-an-agent-with-langgraph/#define-a-single-turn-chatboot","title":"Define a single turn chatboot\u00b6","text":"<p>To illustrate how LangGraph works, the following program defines a chatbot node that will execute a single turn in a chat conversation using the instructions supplied.</p> <p>Each node in the graph operates on the state object. The state (a Python dictionary) is passed as a parameter into the node (a function) and the new state is returned. This can be restated as pseudo-code, where <code>state = node(state)</code>.</p> <p>Note: For the <code>chatbot</code> node, the state is updated by adding the new conversation message. The <code>add_messages</code> annotation on <code>OrderState.messages</code> indicates that messages are appended when returned from a node. Typically state is updated by replacement, but this annotation causes <code>messages</code> to behave differently.</p>"},{"location":"day_03/day-3-building-an-agent-with-langgraph/#add-a-human-node","title":"Add a human node\u00b6","text":"<p>Instead of repeatedly running the \"graph\" in a Python loop, you can use LangGraph to loop between nodes.</p> <p>The <code>human</code> node will display the last message from the LLM to the user, and then prompt them for their next input. Here this is done using standard Python <code>print</code> and <code>input</code> functions, but for a real cafe situation, you could render the chat to a display or audio, and accept input from a mic or on-screen keyboard.</p> <p>The <code>chatbot</code> node function has also been updated to include the welcome message to start the conversation.</p>"},{"location":"day_03/day-3-building-an-agent-with-langgraph/#add-a-live-menu","title":"Add a \"live\" menu\u00b6","text":"<p>BaristaBot currently has no awareness of the available items at the cafe, so it will hallucinate a menu. One option would be to hard-code a menu into the system prompt. This would work well, but to simulate a system where the menu is more dynamic and could respond to fluctuating stock levels, you will put the menu into a custom tool.</p> <p>There are two types of tools that this system will use. Stateless tools that can be run automatically, and stateful tools that modify the order. The \"get current menu\" tool is stateless, in that it does not make any changes to the live order, so it can be called automatically.</p> <p>In a LangGraph app, you can annotate Python functions as tools by applying the <code>@tools</code> annotation.</p>"},{"location":"day_03/day-3-building-an-agent-with-langgraph/#handle-orders","title":"Handle orders\u00b6","text":"<p>To build up an order during the chat conversation, you will need to update the state to track the order, and provide simple tools that update this state. These need to be explicit as the model should not directly have access to the apps internal state, or it risks being manipulated arbitrarily.</p> <p>The ordering tools will be added as stubs in a separate node so that you can edit the state directly. Using the <code>@tool</code> annotation is still a handy way to define their schema, so the ordering tools below are implemented as empty Python functions.</p>"},{"location":"day_03/day-3-building-an-agent-with-langgraph/#further-exercises","title":"Further exercises\u00b6","text":"<p>Congratulations on building an agentic, human-in-the-loop, natural-language powered cafe ordering system using LangGraph and the Gemini API!</p> <p>This example app could be taken in many different directions. You should try and build out your own ideas, but for some inspiration, consider:</p> <ul> <li>Adding more structure the order (<code>OrderState.order</code>) - e.g. separate fields for item, modifiers and even quantity.</li> <li>Currently the model can only clear and re-add items, so add a function to <code>remove_item</code>s from the order.</li> <li>Try building a UI that displays the in-progress order and hosts the chat. Frameworks like Gradio or Mesop are great for this.</li> </ul> <p>This system works well for a single person ordering, but agentic systems can interact with many sources. For a big stretch exercise, try and extend this app to run at a specific schedule, and contact your friends or colleagues over Chat to collect their daily coffee orders.</p> <p>- Mark McD</p>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/","title":"Day 3 - Function calling with the Gemini API","text":"In\u00a0[55]: Copied! <pre># @title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # @title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. In\u00a0[56]: Copied! <pre>!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n!pip install -U -q \"google-genai==1.7.0\"\n</pre> !pip uninstall -qqy jupyterlab  # Remove unused conflicting packages !pip install -U -q \"google-genai==1.7.0\" In\u00a0[57]: Copied! <pre>from google import genai\nfrom google.genai import types\n\ngenai.__version__\n</pre> from google import genai from google.genai import types  genai.__version__ Out[57]: <pre>'1.7.0'</pre> In\u00a0[58]: Copied! <pre>from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n</pre> from kaggle_secrets import UserSecretsClient  GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\") <p>If you received an error response along the lines of <code>No user secrets exist for kernel id ...</code>, then you need to add your API key via <code>Add-ons</code>, <code>Secrets</code> and enable it.</p> <p></p> In\u00a0[59]: Copied! <pre># Define a retry policy. The model might make multiple consecutive calls automatically\n# for a complex query, this ensures the client retries if it hits quota limits.\nfrom google.api_core import retry\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\nif not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n  genai.models.Models.generate_content = retry.Retry(\n      predicate=is_retriable)(genai.models.Models.generate_content)\n</pre> # Define a retry policy. The model might make multiple consecutive calls automatically # for a complex query, this ensures the client retries if it hits quota limits. from google.api_core import retry  is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})  if not hasattr(genai.models.Models.generate_content, '__wrapped__'):   genai.models.Models.generate_content = retry.Retry(       predicate=is_retriable)(genai.models.Models.generate_content) In\u00a0[60]: Copied! <pre>%load_ext sql\n%sql sqlite:///sample.db\n</pre> %load_ext sql %sql sqlite:///sample.db <pre>The sql extension is already loaded. To reload it, use:\n  %reload_ext sql\n</pre> <p>Create the tables and insert some synthetic data. Feel free to tweak this structure and data.</p> In\u00a0[61]: Copied! <pre>%%sql\n-- Create the 'products' table\nCREATE TABLE IF NOT EXISTS products (\n  \tproduct_id INTEGER PRIMARY KEY AUTOINCREMENT,\n  \tproduct_name VARCHAR(255) NOT NULL,\n  \tprice DECIMAL(10, 2) NOT NULL\n  );\n\n-- Create the 'staff' table\nCREATE TABLE IF NOT EXISTS staff (\n  \tstaff_id INTEGER PRIMARY KEY AUTOINCREMENT,\n  \tfirst_name VARCHAR(255) NOT NULL,\n  \tlast_name VARCHAR(255) NOT NULL\n  );\n\n-- Create the 'orders' table\nCREATE TABLE IF NOT EXISTS orders (\n  \torder_id INTEGER PRIMARY KEY AUTOINCREMENT,\n  \tcustomer_name VARCHAR(255) NOT NULL,\n  \tstaff_id INTEGER NOT NULL,\n  \tproduct_id INTEGER NOT NULL,\n  \tFOREIGN KEY (staff_id) REFERENCES staff (staff_id),\n  \tFOREIGN KEY (product_id) REFERENCES products (product_id)\n  );\n\n-- Insert data into the 'products' table\nINSERT INTO products (product_name, price) VALUES\n  \t('Laptop', 799.99),\n  \t('Keyboard', 129.99),\n  \t('Mouse', 29.99);\n\n-- Insert data into the 'staff' table\nINSERT INTO staff (first_name, last_name) VALUES\n  \t('Alice', 'Smith'),\n  \t('Bob', 'Johnson'),\n  \t('Charlie', 'Williams');\n\n-- Insert data into the 'orders' table\nINSERT INTO orders (customer_name, staff_id, product_id) VALUES\n  \t('David Lee', 1, 1),\n  \t('Emily Chen', 2, 2),\n  \t('Frank Brown', 1, 3);\n</pre> %%sql -- Create the 'products' table CREATE TABLE IF NOT EXISTS products (   \tproduct_id INTEGER PRIMARY KEY AUTOINCREMENT,   \tproduct_name VARCHAR(255) NOT NULL,   \tprice DECIMAL(10, 2) NOT NULL   );  -- Create the 'staff' table CREATE TABLE IF NOT EXISTS staff (   \tstaff_id INTEGER PRIMARY KEY AUTOINCREMENT,   \tfirst_name VARCHAR(255) NOT NULL,   \tlast_name VARCHAR(255) NOT NULL   );  -- Create the 'orders' table CREATE TABLE IF NOT EXISTS orders (   \torder_id INTEGER PRIMARY KEY AUTOINCREMENT,   \tcustomer_name VARCHAR(255) NOT NULL,   \tstaff_id INTEGER NOT NULL,   \tproduct_id INTEGER NOT NULL,   \tFOREIGN KEY (staff_id) REFERENCES staff (staff_id),   \tFOREIGN KEY (product_id) REFERENCES products (product_id)   );  -- Insert data into the 'products' table INSERT INTO products (product_name, price) VALUES   \t('Laptop', 799.99),   \t('Keyboard', 129.99),   \t('Mouse', 29.99);  -- Insert data into the 'staff' table INSERT INTO staff (first_name, last_name) VALUES   \t('Alice', 'Smith'),   \t('Bob', 'Johnson'),   \t('Charlie', 'Williams');  -- Insert data into the 'orders' table INSERT INTO orders (customer_name, staff_id, product_id) VALUES   \t('David Lee', 1, 1),   \t('Emily Chen', 2, 2),   \t('Frank Brown', 1, 3); <pre> * sqlite:///sample.db\nDone.\nDone.\nDone.\n(sqlite3.OperationalError) database is locked\n[SQL: -- Insert data into the 'products' table\nINSERT INTO products (product_name, price) VALUES\n  \t('Laptop', 799.99),\n  \t('Keyboard', 129.99),\n  \t('Mouse', 29.99);]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)\n</pre> In\u00a0[62]: Copied! <pre>import sqlite3\n\ndb_file = \"sample.db\"\ndb_conn = sqlite3.connect(db_file)\n</pre> import sqlite3  db_file = \"sample.db\" db_conn = sqlite3.connect(db_file) <p>The first function will list all tables available in the database. Define it, and test it out to ensure it works.</p> In\u00a0[63]: Copied! <pre>def list_tables() -&gt; list[str]:\n    \"\"\"Retrieve the names of all tables in the database.\"\"\"\n    # Include print logging statements so you can see when functions are being called.\n    print(' - DB CALL: list_tables()')\n\n    cursor = db_conn.cursor()\n\n    # Fetch the table names.\n    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n\n    tables = cursor.fetchall()\n    return [t[0] for t in tables]\n\n\nlist_tables()\n</pre> def list_tables() -&gt; list[str]:     \"\"\"Retrieve the names of all tables in the database.\"\"\"     # Include print logging statements so you can see when functions are being called.     print(' - DB CALL: list_tables()')      cursor = db_conn.cursor()      # Fetch the table names.     cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")      tables = cursor.fetchall()     return [t[0] for t in tables]   list_tables() <pre> - DB CALL: list_tables()\n</pre> Out[63]: <pre>['products', 'sqlite_sequence', 'staff', 'orders']</pre> <p>Once the available tables is known, the next step a database user will need is to understand what columns are available in a given table. Define that too, and test that it works as expected.</p> In\u00a0[64]: Copied! <pre>def describe_table(table_name: str) -&gt; list[tuple[str, str]]:\n    \"\"\"Look up the table schema.\n\n    Returns:\n      List of columns, where each entry is a tuple of (column, type).\n    \"\"\"\n    print(f' - DB CALL: describe_table({table_name})')\n\n    cursor = db_conn.cursor()\n\n    cursor.execute(f\"PRAGMA table_info({table_name});\")\n\n    schema = cursor.fetchall()\n    # [column index, column name, column type, ...]\n    return [(col[1], col[2]) for col in schema]\n\n\ndescribe_table(\"products\")\n</pre> def describe_table(table_name: str) -&gt; list[tuple[str, str]]:     \"\"\"Look up the table schema.      Returns:       List of columns, where each entry is a tuple of (column, type).     \"\"\"     print(f' - DB CALL: describe_table({table_name})')      cursor = db_conn.cursor()      cursor.execute(f\"PRAGMA table_info({table_name});\")      schema = cursor.fetchall()     # [column index, column name, column type, ...]     return [(col[1], col[2]) for col in schema]   describe_table(\"products\") <pre> - DB CALL: describe_table(products)\n</pre> Out[64]: <pre>[('product_id', 'INTEGER'),\n ('product_name', 'VARCHAR(255)'),\n ('price', 'DECIMAL(10, 2)')]</pre> <p>Now that the system knows what tables and columns are present, it has enough information to be able to generate and run a <code>SELECT</code> query. Now provide that functionality, and test that it works.</p> In\u00a0[65]: Copied! <pre>def execute_query(sql: str) -&gt; list[list[str]]:\n    \"\"\"Execute an SQL statement, returning the results.\"\"\"\n    print(f' - DB CALL: execute_query({sql})')\n\n    cursor = db_conn.cursor()\n\n    cursor.execute(sql)\n    return cursor.fetchall()\n\n\nexecute_query(\"select * from products\")\n</pre> def execute_query(sql: str) -&gt; list[list[str]]:     \"\"\"Execute an SQL statement, returning the results.\"\"\"     print(f' - DB CALL: execute_query({sql})')      cursor = db_conn.cursor()      cursor.execute(sql)     return cursor.fetchall()   execute_query(\"select * from products\") <pre> - DB CALL: execute_query(select * from products)\n</pre> Out[65]: <pre>[(1, 'Laptop', 799.99),\n (2, 'Keyboard', 129.99),\n (3, 'Mouse', 29.99),\n (4, 'Laptop', 799.99),\n (5, 'Keyboard', 129.99),\n (6, 'Mouse', 29.99)]</pre> In\u00a0[66]: Copied! <pre># These are the Python functions defined above.\ndb_tools = [list_tables, describe_table, execute_query]\n\ninstruction = \"\"\"You are a helpful chatbot that can interact with an SQL database\nfor a computer store. You will take the users questions and turn them into SQL\nqueries using the tools available. Once you have the information you need, you will\nanswer the user's question using the data returned.\n\nUse list_tables to see what tables are present, describe_table to understand the\nschema, and execute_query to issue an SQL SELECT query.\"\"\"\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)\n\n# Start a chat with automatic function calling enabled.\nchat = client.chats.create(\n    model=\"gemini-2.0-flash\",\n    config=types.GenerateContentConfig(\n        system_instruction=instruction,\n        tools=db_tools,\n    ),\n)\n</pre> # These are the Python functions defined above. db_tools = [list_tables, describe_table, execute_query]  instruction = \"\"\"You are a helpful chatbot that can interact with an SQL database for a computer store. You will take the users questions and turn them into SQL queries using the tools available. Once you have the information you need, you will answer the user's question using the data returned.  Use list_tables to see what tables are present, describe_table to understand the schema, and execute_query to issue an SQL SELECT query.\"\"\"  client = genai.Client(api_key=GOOGLE_API_KEY)  # Start a chat with automatic function calling enabled. chat = client.chats.create(     model=\"gemini-2.0-flash\",     config=types.GenerateContentConfig(         system_instruction=instruction,         tools=db_tools,     ), ) <p>Now you can engage in a chat conversation where you can ask about the contents of the database.</p> In\u00a0[67]: Copied! <pre>resp = chat.send_message(\"What is the cheapest product?\")\nprint(f\"\\n{resp.text}\")\n</pre> resp = chat.send_message(\"What is the cheapest product?\") print(f\"\\n{resp.text}\") <pre> - DB CALL: list_tables()\n - DB CALL: describe_table(products)\n - DB CALL: execute_query(SELECT product_name, price FROM products ORDER BY price ASC LIMIT 1)\n\nThe cheapest product is the Mouse, which costs $29.99.\n\n</pre> <p>Explore the chat session and ask your own questions. The 2.0 models are quite capable and can usually answer questions requiring multiple steps.</p> In\u00a0[68]: Copied! <pre>chat = client.chats.create(\n    model=\"gemini-2.0-flash\",\n    config=types.GenerateContentConfig(\n        system_instruction=instruction,\n        tools=db_tools,\n    ),\n)\n\nresponse = chat.send_message('What products should salesperson Alice focus on to round out her portfolio? Explain why.')\nprint(f\"\\n{response.text}\")\n</pre> chat = client.chats.create(     model=\"gemini-2.0-flash\",     config=types.GenerateContentConfig(         system_instruction=instruction,         tools=db_tools,     ), )  response = chat.send_message('What products should salesperson Alice focus on to round out her portfolio? Explain why.') print(f\"\\n{response.text}\") <pre> - DB CALL: list_tables()\n - DB CALL: describe_table(products)\n - DB CALL: describe_table(staff)\n - DB CALL: describe_table(orders)\n - DB CALL: execute_query(SELECT staff_id FROM staff WHERE first_name = 'Alice')\n - DB CALL: execute_query(SELECT DISTINCT product_id FROM orders WHERE staff_id = 1)\n - DB CALL: execute_query(SELECT product_id, product_name FROM products)\n\nAlice has sold Laptop (product_id 1) and Mouse (product id 3). To round out her portfolio, Alice should focus on selling Keyboards (product_id 2 and 5) and Laptops (product_id 4). She has sold one type of laptop already, but there is another type of laptop that she could try to sell to customers.\n\n</pre> In\u00a0[69]: Copied! <pre>import textwrap\n\n\ndef print_chat_turns(chat):\n    \"\"\"Prints out each turn in the chat history, including function calls and responses.\"\"\"\n    for event in chat.get_history():\n        print(f\"{event.role.capitalize()}:\")\n\n        for part in event.parts:\n            if txt := part.text:\n                print(f'  \"{txt}\"')\n            elif fn := part.function_call:\n                args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n                print(f\"  Function call: {fn.name}({args})\")\n            elif resp := part.function_response:\n                print(\"  Function response:\")\n                print(textwrap.indent(str(resp.response['result']), \"    \"))\n\n        print()\n\n\nprint_chat_turns(chat)\n</pre> import textwrap   def print_chat_turns(chat):     \"\"\"Prints out each turn in the chat history, including function calls and responses.\"\"\"     for event in chat.get_history():         print(f\"{event.role.capitalize()}:\")          for part in event.parts:             if txt := part.text:                 print(f'  \"{txt}\"')             elif fn := part.function_call:                 args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())                 print(f\"  Function call: {fn.name}({args})\")             elif resp := part.function_response:                 print(\"  Function response:\")                 print(textwrap.indent(str(resp.response['result']), \"    \"))          print()   print_chat_turns(chat) <pre>User:\n  \"What products should salesperson Alice focus on to round out her portfolio? Explain why.\"\n\nModel:\n  Function call: list_tables()\n\nUser:\n  Function response:\n    ['products', 'sqlite_sequence', 'staff', 'orders']\n\nModel:\n  \"Okay, I will start by looking at the products table.\n\"\n  Function call: describe_table(table_name=products)\n\nUser:\n  Function response:\n    [('product_id', 'INTEGER'), ('product_name', 'VARCHAR(255)'), ('price', 'DECIMAL(10, 2)')]\n\nModel:\n  Function call: describe_table(table_name=staff)\n\nUser:\n  Function response:\n    [('staff_id', 'INTEGER'), ('first_name', 'VARCHAR(255)'), ('last_name', 'VARCHAR(255)')]\n\nModel:\n  Function call: describe_table(table_name=orders)\n\nUser:\n  Function response:\n    [('order_id', 'INTEGER'), ('customer_name', 'VARCHAR(255)'), ('staff_id', 'INTEGER'), ('product_id', 'INTEGER')]\n\nModel:\n  \"Okay, now I have the table schemas. I need to figure out what Alice has already sold, and then suggest products that are not in that set.\n\nFirst, I need to find Alice's staff ID.\n\"\n  Function call: execute_query(sql=SELECT staff_id FROM staff WHERE first_name = 'Alice')\n\nUser:\n  Function response:\n    [(1,), (4,)]\n\nModel:\n  \"It looks like there are two people named Alice. I will assume the user means the Alice with staff_id = 1. I will proceed, but I will need to ask the user to clarify if this is not correct.\n\nNow I need to find the products that Alice has sold.\n\"\n  Function call: execute_query(sql=SELECT DISTINCT product_id FROM orders WHERE staff_id = 1)\n\nUser:\n  Function response:\n    [(1,), (3,)]\n\nModel:\n  \"Alice has sold products with product_id 1 and 3. I will now find the names of all the products.\n\"\n  Function call: execute_query(sql=SELECT product_id, product_name FROM products)\n\nUser:\n  Function response:\n    [(1, 'Laptop'), (2, 'Keyboard'), (3, 'Mouse'), (4, 'Laptop'), (5, 'Keyboard'), (6, 'Mouse')]\n\nModel:\n  \"Alice has sold Laptop (product_id 1) and Mouse (product id 3). To round out her portfolio, Alice should focus on selling Keyboards (product_id 2 and 5) and Laptops (product_id 4). She has sold one type of laptop already, but there is another type of laptop that she could try to sell to customers.\n\"\n\n</pre> <p>In this output you can see each of the conversational turns that were made. Note that the model doesn't remember anything outside of the chat history, so you can make changes to the database structure or data and the model will respond without needing any code changes - try this out!</p> In\u00a0[70]: Copied! <pre>from pprint import pformat\nfrom IPython.display import display, Image, Markdown\n\n\nasync def handle_response(stream, tool_impl=None):\n  \"\"\"Stream output and handle any tool calls during the session.\"\"\"\n  all_responses = []\n\n  async for msg in stream.receive():\n    all_responses.append(msg)\n\n    if text := msg.text:\n      # Output any text chunks that are streamed back.\n      if len(all_responses) &lt; 2 or not all_responses[-2].text:\n        # Display a header if this is the first text chunk.\n        display(Markdown('### Text'))\n\n      print(text, end='')\n\n    elif tool_call := msg.tool_call:\n      # Handle tool-call requests.\n      for fc in tool_call.function_calls:\n        display(Markdown('### Tool call'))\n\n        # Execute the tool and collect the result to return to the model.\n        if callable(tool_impl):\n          try:\n            result = tool_impl(**fc.args)\n          except Exception as e:\n            result = str(e)\n        else:\n          result = 'ok'\n\n        tool_response = types.LiveClientToolResponse(\n            function_responses=[types.FunctionResponse(\n                name=fc.name,\n                id=fc.id,\n                response={'result': result},\n            )]\n        )\n        await stream.send(input=tool_response)\n\n    elif msg.server_content and msg.server_content.model_turn:\n      # Print any messages showing code the model generated and ran.\n\n      for part in msg.server_content.model_turn.parts:\n          if code := part.executable_code:\n            display(Markdown(\n                f'### Code\\n```\\n{code.code}\\n```'))\n\n          elif result := part.code_execution_result:\n            display(Markdown(f'### Result: {result.outcome}\\n'\n                             f'```\\n{pformat(result.output)}\\n```'))\n\n          elif img := part.inline_data:\n            display(Image(img.data))\n\n  print()\n  return all_responses\n</pre> from pprint import pformat from IPython.display import display, Image, Markdown   async def handle_response(stream, tool_impl=None):   \"\"\"Stream output and handle any tool calls during the session.\"\"\"   all_responses = []    async for msg in stream.receive():     all_responses.append(msg)      if text := msg.text:       # Output any text chunks that are streamed back.       if len(all_responses) &lt; 2 or not all_responses[-2].text:         # Display a header if this is the first text chunk.         display(Markdown('### Text'))        print(text, end='')      elif tool_call := msg.tool_call:       # Handle tool-call requests.       for fc in tool_call.function_calls:         display(Markdown('### Tool call'))          # Execute the tool and collect the result to return to the model.         if callable(tool_impl):           try:             result = tool_impl(**fc.args)           except Exception as e:             result = str(e)         else:           result = 'ok'          tool_response = types.LiveClientToolResponse(             function_responses=[types.FunctionResponse(                 name=fc.name,                 id=fc.id,                 response={'result': result},             )]         )         await stream.send(input=tool_response)      elif msg.server_content and msg.server_content.model_turn:       # Print any messages showing code the model generated and ran.        for part in msg.server_content.model_turn.parts:           if code := part.executable_code:             display(Markdown(                 f'### Code\\n```\\n{code.code}\\n```'))            elif result := part.code_execution_result:             display(Markdown(f'### Result: {result.outcome}\\n'                              f'```\\n{pformat(result.output)}\\n```'))            elif img := part.inline_data:             display(Image(img.data))    print()   return all_responses In\u00a0[71]: Copied! <pre>model = 'gemini-2.0-flash-exp'\nlive_client = genai.Client(api_key=GOOGLE_API_KEY,\n                           http_options=types.HttpOptions(api_version='v1alpha'))\n\n# Wrap the existing execute_query tool you used in the earlier example.\nexecute_query_tool_def = types.FunctionDeclaration.from_callable(\n    client=live_client, callable=execute_query)\n\n# Provide the model with enough information to use the tool, such as describing\n# the database so it understands which SQL syntax to use.\nsys_int = \"\"\"You are a database interface. Use the `execute_query` function\nto answer the users questions by looking up information in the database,\nrunning any necessary queries and responding to the user.\n\nYou need to look up table schema using sqlite3 syntax SQL, then once an\nanswer is found be sure to tell the user. If the user is requesting an\naction, you must also execute the actions.\n\"\"\"\n\nconfig = {\n    \"response_modalities\": [\"TEXT\"],\n    \"system_instruction\": {\"parts\": [{\"text\": sys_int}]},\n    \"tools\": [\n        {\"code_execution\": {}},\n        {\"function_declarations\": [execute_query_tool_def.to_json_dict()]},\n    ],\n}\n\nasync with live_client.aio.live.connect(model=model, config=config) as session:\n\n  message = \"Please generate and insert 5 new rows in the orders table.\"\n  print(f\"&gt; {message}\\n\")\n\n  await session.send(input=message, end_of_turn=True)\n  await handle_response(session, tool_impl=execute_query)\n</pre> model = 'gemini-2.0-flash-exp' live_client = genai.Client(api_key=GOOGLE_API_KEY,                            http_options=types.HttpOptions(api_version='v1alpha'))  # Wrap the existing execute_query tool you used in the earlier example. execute_query_tool_def = types.FunctionDeclaration.from_callable(     client=live_client, callable=execute_query)  # Provide the model with enough information to use the tool, such as describing # the database so it understands which SQL syntax to use. sys_int = \"\"\"You are a database interface. Use the `execute_query` function to answer the users questions by looking up information in the database, running any necessary queries and responding to the user.  You need to look up table schema using sqlite3 syntax SQL, then once an answer is found be sure to tell the user. If the user is requesting an action, you must also execute the actions. \"\"\"  config = {     \"response_modalities\": [\"TEXT\"],     \"system_instruction\": {\"parts\": [{\"text\": sys_int}]},     \"tools\": [         {\"code_execution\": {}},         {\"function_declarations\": [execute_query_tool_def.to_json_dict()]},     ], }  async with live_client.aio.live.connect(model=model, config=config) as session:    message = \"Please generate and insert 5 new rows in the orders table.\"   print(f\"&gt; {message}\\n\")    await session.send(input=message, end_of_turn=True)   await handle_response(session, tool_impl=execute_query) <pre>&gt; Please generate and insert 5 new rows in the orders table.\n\n</pre> <p>In the output from the previous step, you should see a <code>Code</code> section that shows code that the model generated in order to complete the task. It will look something like this:</p> <pre>sql_statements = [ ... ]\n\nfor sql in sql_statements:\n  print(default_api.execute_query(sql))\n</pre> <p>The model then runs this code (remotely), calling out to the provided tool when it reaches that part of the code. The <code>default_api</code> module contains the tools that you provided.</p> <p>This example simply executes in a loop, but the models are capable of more complex interactions with multiple tools, giving you a powerful agent framework that's effectively built in to the Gemini API.</p> In\u00a0[72]: Copied! <pre>async with live_client.aio.live.connect(model=model, config=config) as session:\n\n  message = \"Can you figure out the number of orders that were made by each of the staff?\"\n\n  print(f\"&gt; {message}\\n\")\n  await session.send(input=message, end_of_turn=True)\n  await handle_response(session, tool_impl=execute_query)\n\n  message = \"Generate and run some code to plot this as a python seaborn chart\"\n\n  print(f\"&gt; {message}\\n\")\n  await session.send(input=message, end_of_turn=True)\n  await handle_response(session, tool_impl=execute_query)\n</pre> async with live_client.aio.live.connect(model=model, config=config) as session:    message = \"Can you figure out the number of orders that were made by each of the staff?\"    print(f\"&gt; {message}\\n\")   await session.send(input=message, end_of_turn=True)   await handle_response(session, tool_impl=execute_query)    message = \"Generate and run some code to plot this as a python seaborn chart\"    print(f\"&gt; {message}\\n\")   await session.send(input=message, end_of_turn=True)   await handle_response(session, tool_impl=execute_query) <pre>&gt; Can you figure out the number of orders that were made by each of the staff?\n\n</pre> <pre> - DB CALL: execute_query(SELECT name FROM sqlite_master WHERE type='table';)\n</pre> <pre>Okay, I see the tables available are `products`, `staff`, and `orders`. To figure out the number of orders made by each staff, I'll need to join the `staff` and `orders` tables, probably using a staff ID. Let's inspect the schemas of both tables.\n</pre> <pre> - DB CALL: execute_query(PRAGMA table_info(staff);)\n</pre> <pre> - DB CALL: execute_query(PRAGMA table_info(orders);)\n</pre> <pre>Okay, it seems like `staff` table has `staff_id`, `first_name`, and `last_name` columns, and the `orders` table has `order_id`, `customer_name`, `staff_id`, and `product_id`.  I can join these tables on `staff.staff_id = orders.staff_id`. Then I will group by `staff_id` and count the number of orders to fulfill the user request. I will also select `first_name` and `last_name` to show to the user.\n</pre> <pre> - DB CALL: execute_query(\nSELECT\n    s.first_name,\n    s.last_name,\n    COUNT(o.order_id) AS number_of_orders\nFROM\n    staff s\nJOIN\n    orders o ON s.staff_id = o.staff_id\nGROUP BY\n    s.staff_id;\n)\n</pre> <pre>Okay, it seems like Alice Smith made 6 orders, Bob Johnson made 4 orders, and Charlie Williams made 1 order.\n\n&gt; Generate and run some code to plot this as a python seaborn chart\n\n</pre> <pre> - DB CALL: execute_query(\nSELECT\n    s.first_name,\n    s.last_name,\n    COUNT(o.order_id) AS number_of_orders\nFROM\n    staff s\nJOIN\n    orders o ON s.staff_id = o.staff_id\nGROUP BY\n    s.staff_id;\n)\n</pre> <pre>Okay, I have generated the bar chart using seaborn and displayed it. The chart shows the number of orders handled by each staff member: Alice Smith (6 orders), Bob Johnson (4 orders), and Charlie Williams (1 order).\n\n</pre>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#copyright-2025-google-llc","title":"Copyright 2025 Google LLC.\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#day-3-function-calling-with-the-gemini-api","title":"Day 3 - Function calling with the Gemini API\u00b6","text":"<p>Welcome back to the Kaggle 5-day Generative AI course!</p> <p>In this notebook, you will use the Gemini API's automatic function calling to build a chat interface over a local database. This example is a toy and is missing a number of safety and security constraints you would use in a real-world example, but shows how to add AI chat capabilities to existing applications with ease.</p> <pre>I need to understand the structure of the `orders` table before I can insert new rows. I'll start by querying the database schema.\n</pre> <pre> - DB CALL: execute_query(PRAGMA table_info(orders);)\n</pre> <pre>Okay, the `orders` table has columns `order_id` (INTEGER, primary key), `customer_name` (VARCHAR(255)), `staff_id` (INTEGER), and `product_id` (INTEGER). I'll now insert 5 new rows into this table, making sure to provide values for `customer_name`, `staff_id`, and `product_id`. I will auto-increment the `order_id`.\n</pre> <pre> - DB CALL: execute_query(INSERT INTO orders (customer_name, staff_id, product_id) VALUES ('Alice Smith', 1, 101);)\n</pre> <pre> - DB CALL: execute_query(INSERT INTO orders (customer_name, staff_id, product_id) VALUES ('Bob Johnson', 2, 102);)\n</pre> <pre> - DB CALL: execute_query(INSERT INTO orders (customer_name, staff_id, product_id) VALUES ('Charlie Brown', 1, 103);)\n</pre> <pre> - DB CALL: execute_query(INSERT INTO orders (customer_name, staff_id, product_id) VALUES ('Diana Prince', 3, 104);)\n</pre> <pre> - DB CALL: execute_query(INSERT INTO orders (customer_name, staff_id, product_id) VALUES ('Eve Miller', 2, 105);)\n</pre> <pre>I have successfully inserted 5 new rows into the `orders` table.\n\n</pre>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#for-help","title":"For help\u00b6","text":"<p>Common issues are covered in the FAQ and troubleshooting guide.</p>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#setup","title":"Setup\u00b6","text":"<p>Start by installing and importing the Python SDK.</p>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#set-up-your-api-key","title":"Set up your API key\u00b6","text":"<p>To run the following cell, your API key must be stored it in a Kaggle secret named <code>GOOGLE_API_KEY</code>.</p> <p>If you don't already have an API key, you can grab one from AI Studio. You can find detailed instructions in the docs.</p> <p>To make the key available through Kaggle secrets, choose <code>Secrets</code> from the <code>Add-ons</code> menu and follow the instructions to add your key or enable it for this notebook.</p>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#automated-retry","title":"Automated retry\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#create-a-local-database","title":"Create a local database\u00b6","text":"<p>For this minimal example, you'll create a local SQLite database and add some synthetic data so you have something to query.</p> <p>Load the <code>sql</code> IPython extension so you can interact with the database using magic commands (the <code>%</code> instructions) to create a new, empty SQLite database.</p>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#define-database-functions","title":"Define database functions\u00b6","text":"<p>Function calling with Gemini API's Python SDK can be implemented by defining an OpenAPI schema that is passed to the model. You can also define Python functions and let the SDK inspect them to automatically define the schema. In this latter case, it's important that the functions are type annotated and have accurate docstrings that describe what the functions do - the model has no insight into the function body, so the docs function as the interface.</p> <p>By providing three key pieces of functionality - listing tables, describing a table, and executing a query - the LLM (much like a human user) will have the basic tools needed to understand and interrogate the database.</p> <p>Start with a database connection that will be used across all of the functions.</p>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#implement-function-calls","title":"Implement function calls\u00b6","text":"<p>Now you can put it all together in a call to the Gemini API.</p> <p>Function calling works by adding specific messages to a chat session. When function schemas are defined and made available to the model and a conversation is started, instead of returning a text response, the model may return a <code>function_call</code> instead. When this happens, the client must respond with a <code>function_response</code>, indicating the result of the call, and the conversation can continue on as normal.</p> <p>This function calling interaction normally happens manually, allowing you, the client, to validate and initiate the call. However the Python SDK also supports automatic function calling, where the supplied functions will be automatically invoked. This is a powerful feature and should be used with care, such as when the functions have no side-effects).</p> <p>Here's the state diagram representing the conversation flow with function calling. With automatic function calling, the bottom row is executed automatically by the Python SDK. With manual function calling, you write the code to run each step individually.</p> <p></p>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#inspecting-the-conversation","title":"Inspecting the conversation\u00b6","text":"<p>To see the calls that the model makes, and what the client returns in response, you can inspect the chat history. This helper function will print out each turn along with the relevant fields passed or returned.</p>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#compositional-function-calling","title":"Compositional function calling\u00b6","text":"<p>A powerful new feature in Gemini 2.0 is the model's ability to compose user-provided function calls together while generating code.</p> <p>This means that the model is able to take the available tools, generate code that uses it, and execute it all.</p> <p>The feature requires the Live API, so this step uses different setup code than most of the examples you have seen so far. As the Multimodal Live API is a bi-directional streaming service, everything is set up in advance and then executed. This is a little more complex but the result is quite powerful.</p> <p>First define a function that will handle streaming model output. It will stream text output, handle tool-calling and show the generated code that the model writes and executes to fulfill the task.</p>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#textual-live-database-chat","title":"Textual live database chat\u00b6","text":"<p>Now connect to the model and start a conversation.</p> <p>The Live API is a streaming API, so this example is fully pre-scripted and only has a single user input. Despite this, the request still requires the model to perform a bit of back-and-forth to interrogate the database, and you should see the model generate some code that uses the <code>execute_query</code> tool in a loop.</p>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#text","title":"Text\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#code","title":"Code\u00b6","text":"<pre><code>print(default_api.execute_query(sql=\"PRAGMA table_info(orders);\"))\n\n</code></pre>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#tool-call","title":"Tool call\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#result-outcome_ok","title":"Result: OUTCOME_OK\u00b6","text":"<pre><code>(\"{'result': [[0, 'order_id', 'INTEGER', 0, None, 1], [1, 'customer_name', \"\n \"'VARCHAR(255)', 1, None, 0], [2, 'staff_id', 'INTEGER', 1, None, 0], [3, \"\n \"'product_id', 'INTEGER', 1, None, 0]]}\\n\")\n</code></pre>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#text","title":"Text\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#code","title":"Code\u00b6","text":"<pre><code>insert_statements = [\n    \"INSERT INTO orders (customer_name, staff_id, product_id) VALUES ('Alice Smith', 1, 101);\",\n    \"INSERT INTO orders (customer_name, staff_id, product_id) VALUES ('Bob Johnson', 2, 102);\",\n    \"INSERT INTO orders (customer_name, staff_id, product_id) VALUES ('Charlie Brown', 1, 103);\",\n    \"INSERT INTO orders (customer_name, staff_id, product_id) VALUES ('Diana Prince', 3, 104);\",\n    \"INSERT INTO orders (customer_name, staff_id, product_id) VALUES ('Eve Miller', 2, 105);\"\n]\n\nfor stmt in insert_statements:\n  print(default_api.execute_query(sql=stmt))\n\n</code></pre>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#tool-call","title":"Tool call\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#tool-call","title":"Tool call\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#tool-call","title":"Tool call\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#tool-call","title":"Tool call\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#tool-call","title":"Tool call\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#result-outcome_ok","title":"Result: OUTCOME_OK\u00b6","text":"<pre><code>(\"{'result': []}\\n\"\n \"{'result': []}\\n\"\n \"{'result': []}\\n\"\n \"{'result': []}\\n\"\n \"{'result': []}\\n\")\n</code></pre>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#text","title":"Text\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#plotting-the-database","title":"Plotting the database\u00b6","text":"<p>Try out the built-in agent capability with the next example. You may notice the model try to guess the database schema or environment. Often the model will make mistakes, but you can look through the <code>Text</code> output and watch as the model inspects the error, tries a new approach and learns from its mistakes.</p> <p>If the model doesn't return a plot, try running the cell again.</p>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#code","title":"Code\u00b6","text":"<pre><code>sql_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\nprint(default_api.execute_query(sql=sql_query))\n\n</code></pre>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#tool-call","title":"Tool call\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#result-outcome_ok","title":"Result: OUTCOME_OK\u00b6","text":"<pre><code>\"{'result': [['products'], ['sqlite_sequence'], ['staff'], ['orders']]}\\n\"\n</code></pre>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#text","title":"Text\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#code","title":"Code\u00b6","text":"<pre><code>print(default_api.execute_query(sql=\"PRAGMA table_info(staff);\"))\nprint(default_api.execute_query(sql=\"PRAGMA table_info(orders);\"))\n\n</code></pre>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#tool-call","title":"Tool call\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#tool-call","title":"Tool call\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#result-outcome_ok","title":"Result: OUTCOME_OK\u00b6","text":"<pre><code>(\"{'result': [[0, 'staff_id', 'INTEGER', 0, None, 1], [1, 'first_name', \"\n \"'VARCHAR(255)', 1, None, 0], [2, 'last_name', 'VARCHAR(255)', 1, None, 0]]}\\n\"\n \"{'result': [[0, 'order_id', 'INTEGER', 0, None, 1], [1, 'customer_name', \"\n \"'VARCHAR(255)', 1, None, 0], [2, 'staff_id', 'INTEGER', 1, None, 0], [3, \"\n \"'product_id', 'INTEGER', 1, None, 0]]}\\n\")\n</code></pre>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#text","title":"Text\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#code","title":"Code\u00b6","text":"<pre><code>sql_query = \"\"\"\nSELECT\n    s.first_name,\n    s.last_name,\n    COUNT(o.order_id) AS number_of_orders\nFROM\n    staff s\nJOIN\n    orders o ON s.staff_id = o.staff_id\nGROUP BY\n    s.staff_id;\n\"\"\"\nprint(default_api.execute_query(sql=sql_query))\n\n</code></pre>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#tool-call","title":"Tool call\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#result-outcome_ok","title":"Result: OUTCOME_OK\u00b6","text":"<pre><code>(\"{'result': [['Alice', 'Smith', 6], ['Bob', 'Johnson', 4], ['Charlie', \"\n \"'Williams', 1]]}\\n\")\n</code></pre>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#text","title":"Text\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#code","title":"Code\u00b6","text":"<pre><code>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\nsql_query = \"\"\"\nSELECT\n    s.first_name,\n    s.last_name,\n    COUNT(o.order_id) AS number_of_orders\nFROM\n    staff s\nJOIN\n    orders o ON s.staff_id = o.staff_id\nGROUP BY\n    s.staff_id;\n\"\"\"\nquery_result = default_api.execute_query(sql=sql_query)\ndf = pd.DataFrame(query_result['result'], columns=['first_name', 'last_name', 'number_of_orders'])\ndf['staff_name'] = df['first_name'] + \" \" + df['last_name']\n\nplt.figure(figsize=(8, 6))\nsns.barplot(x='staff_name', y='number_of_orders', data=df)\nplt.title('Number of Orders per Staff Member')\nplt.xlabel('Staff Member')\nplt.ylabel('Number of Orders')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n</code></pre>"},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#tool-call","title":"Tool call\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#text","title":"Text\u00b6","text":""},{"location":"day_03/day-3-function-calling-with-the-gemini-api/#further-reading","title":"Further reading\u00b6","text":"<p>To learn more about what the Gemini API can do with function calling, check out the Function calling cookbook (see <code>Manual function calling</code> to understand how function calling works manually) as well as Function calling config, which gives you fine-grained control over how function calling is triggered.</p> <p>And stay tuned for day 4, where you will explore using function calling with grounding tools.</p> <p>- Mark McD</p>"},{"location":"day_03/extras/","title":"Day 3 Livestream with Paige Bailey \u2013 5-Day Gen AI Intensive Course | Kaggle","text":""},{"location":"day_03/podcast/","title":"Whitepaper Companion Podcast - Agents | 5-Day Gen AI Intensive Course with Google","text":""},{"location":"day_03/whitepaper/","title":"Agents","text":"<p>Authors: Julia Wiesinger, Patrick Marlow and Vladimir Vuskovic</p>"},{"location":"day_03/whitepaper/#introduction","title":"Introduction","text":"<p>Humans are fantastic at messy pattern recognition tasks. However, they often rely on tools - like books, Google Search, or a calculator - to supplement their prior knowledge before arriving at a conclusion. Just like humans, Generative AI models can be trained to use tools to access real-time information or suggest a real-world action. For example, a model can leverage a database retrieval tool to access specific information, like a customer's purchase history, so it can generate tailored shopping recommendations. Alternatively, based on a user's query, a model can make various API calls to send an email response to a colleague or complete a financial transaction on your behalf. To do so, the model must not only have access to a set of external tools, it needs the ability to plan and execute any task in a self-directed fashion. This combination of reasoning, logic, and access to external information that are all connected to a Generative AI model invokes the concept of an agent, or a program that extends beyond the standalone capabilities of a Generative AI model. This whitepaper dives into all these and associated aspects in more detail.</p>"},{"location":"day_03/whitepaper/#read-the-whitepaper-below","title":"Read the whitepaper below","text":""},{"location":"day_04/","title":"Day 4 \u2013 Domain-Specific LLMs","text":"<p>Learn to create and use specialized LLMs like SecLM and Med-PaLM.</p> <p>Assignments:</p> <ol> <li> <p>Unit 4: \u201cDomain-Specific LLMs\u201d </p> <ul> <li>\ud83c\udfa7 Podcast (Optional) </li> <li>\ud83d\udcc4 Whitepaper </li> <li>\ud83d\udcbb Code Labs:<ul> <li>Google Search Grounding </li> <li>Fine-Tune Gemini</li> </ul> </li> </ul> </li> <li> <p>\ud83c\udfa5 YouTube Livestream Recording</p> </li> </ol>"},{"location":"day_04/day-4-fine-tuning-a-custom-model/","title":"Day 4 - Fine tuning a custom model","text":"In\u00a0[\u00a0]: Copied! <pre># @title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # @title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. In\u00a0[4]: Copied! <pre>!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n!pip install -U -q \"google-genai==1.7.0\"\n</pre> !pip uninstall -qqy jupyterlab  # Remove unused conflicting packages !pip install -U -q \"google-genai==1.7.0\" In\u00a0[5]: Copied! <pre>from google import genai\nfrom google.genai import types\n\ngenai.__version__\n</pre> from google import genai from google.genai import types  genai.__version__ Out[5]: <pre>'1.7.0'</pre> In\u00a0[6]: Copied! <pre>from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)\n</pre> from kaggle_secrets import UserSecretsClient  GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")  client = genai.Client(api_key=GOOGLE_API_KEY) <p>If you received an error response along the lines of <code>No user secrets exist for kernel id ...</code>, then you need to add your API key via <code>Add-ons</code>, <code>Secrets</code> and enable it.</p> <p></p> In\u00a0[7]: Copied! <pre>for model in client.models.list():\n    if \"createTunedModel\" in model.supported_actions:\n        print(model.name)\n</pre> for model in client.models.list():     if \"createTunedModel\" in model.supported_actions:         print(model.name) <pre>models/gemini-1.5-flash-001-tuning\n</pre> In\u00a0[8]: Copied! <pre>from sklearn.datasets import fetch_20newsgroups\n\nnewsgroups_train = fetch_20newsgroups(subset=\"train\")\nnewsgroups_test = fetch_20newsgroups(subset=\"test\")\n\n# View list of class names for dataset\nnewsgroups_train.target_names\n</pre> from sklearn.datasets import fetch_20newsgroups  newsgroups_train = fetch_20newsgroups(subset=\"train\") newsgroups_test = fetch_20newsgroups(subset=\"test\")  # View list of class names for dataset newsgroups_train.target_names Out[8]: <pre>['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']</pre> <p>Here's what a single row looks like.</p> In\u00a0[9]: Copied! <pre>print(newsgroups_train.data[0])\n</pre> print(newsgroups_train.data[0]) <pre>From: lerxst@wam.umd.edu (where's my thing)\nSubject: WHAT car is this!?\nNntp-Posting-Host: rac3.wam.umd.edu\nOrganization: University of Maryland, College Park\nLines: 15\n\n I was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n\nThanks,\n- IL\n   ---- brought to you by your neighborhood Lerxst ----\n\n\n\n\n\n</pre> In\u00a0[10]: Copied! <pre>import email\nimport re\n\nimport pandas as pd\n\n\ndef preprocess_newsgroup_row(data):\n    # Extract only the subject and body\n    msg = email.message_from_string(data)\n    text = f\"{msg['Subject']}\\n\\n{msg.get_payload()}\"\n    # Strip any remaining email addresses\n    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)\n    # Truncate the text to fit within the input limits\n    text = text[:40000]\n\n    return text\n\n\ndef preprocess_newsgroup_data(newsgroup_dataset):\n    # Put data points into dataframe\n    df = pd.DataFrame(\n        {\"Text\": newsgroup_dataset.data, \"Label\": newsgroup_dataset.target}\n    )\n    # Clean up the text\n    df[\"Text\"] = df[\"Text\"].apply(preprocess_newsgroup_row)\n    # Match label to target name index\n    df[\"Class Name\"] = df[\"Label\"].map(lambda l: newsgroup_dataset.target_names[l])\n\n    return df\n</pre> import email import re  import pandas as pd   def preprocess_newsgroup_row(data):     # Extract only the subject and body     msg = email.message_from_string(data)     text = f\"{msg['Subject']}\\n\\n{msg.get_payload()}\"     # Strip any remaining email addresses     text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)     # Truncate the text to fit within the input limits     text = text[:40000]      return text   def preprocess_newsgroup_data(newsgroup_dataset):     # Put data points into dataframe     df = pd.DataFrame(         {\"Text\": newsgroup_dataset.data, \"Label\": newsgroup_dataset.target}     )     # Clean up the text     df[\"Text\"] = df[\"Text\"].apply(preprocess_newsgroup_row)     # Match label to target name index     df[\"Class Name\"] = df[\"Label\"].map(lambda l: newsgroup_dataset.target_names[l])      return df In\u00a0[12]: Copied! <pre># Apply preprocessing to training and test datasets\ndf_train = preprocess_newsgroup_data(newsgroups_train)\ndf_test = preprocess_newsgroup_data(newsgroups_test)\n\ndf_train.head()\n</pre> # Apply preprocessing to training and test datasets df_train = preprocess_newsgroup_data(newsgroups_train) df_test = preprocess_newsgroup_data(newsgroups_test)  df_train.head() Out[12]: Text Label Class Name 0 WHAT car is this!?\\n\\n I was wondering if anyo... 7 rec.autos 1 SI Clock Poll - Final Call\\n\\nA fair number of... 4 comp.sys.mac.hardware 2 PB questions...\\n\\nwell folks, my mac plus fin... 4 comp.sys.mac.hardware 3 Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr... 1 comp.graphics 4 Re: Shuttle Launch Question\\n\\nFrom article &lt;&gt;... 14 sci.space <p>Now sample the data. You will keep 50 rows for each category for training. Note that this is even fewer than the Keras example, as this technique (parameter-efficient fine-tuning, or PEFT) updates a relatively small number of parameters and does not require training a new model or updating the large model.</p> In\u00a0[13]: Copied! <pre>def sample_data(df, num_samples, classes_to_keep):\n    # Sample rows, selecting num_samples of each Label.\n    df = (\n        df.groupby(\"Label\")[df.columns]\n        .apply(lambda x: x.sample(num_samples))\n        .reset_index(drop=True)\n    )\n\n    df = df[df[\"Class Name\"].str.contains(classes_to_keep)]\n    df[\"Class Name\"] = df[\"Class Name\"].astype(\"category\")\n\n    return df\n\n\nTRAIN_NUM_SAMPLES = 50\nTEST_NUM_SAMPLES = 10\n# Keep rec.* and sci.*\nCLASSES_TO_KEEP = \"^rec|^sci\"\n\ndf_train = sample_data(df_train, TRAIN_NUM_SAMPLES, CLASSES_TO_KEEP)\ndf_test = sample_data(df_test, TEST_NUM_SAMPLES, CLASSES_TO_KEEP)\n</pre> def sample_data(df, num_samples, classes_to_keep):     # Sample rows, selecting num_samples of each Label.     df = (         df.groupby(\"Label\")[df.columns]         .apply(lambda x: x.sample(num_samples))         .reset_index(drop=True)     )      df = df[df[\"Class Name\"].str.contains(classes_to_keep)]     df[\"Class Name\"] = df[\"Class Name\"].astype(\"category\")      return df   TRAIN_NUM_SAMPLES = 50 TEST_NUM_SAMPLES = 10 # Keep rec.* and sci.* CLASSES_TO_KEEP = \"^rec|^sci\"  df_train = sample_data(df_train, TRAIN_NUM_SAMPLES, CLASSES_TO_KEEP) df_test = sample_data(df_test, TEST_NUM_SAMPLES, CLASSES_TO_KEEP) In\u00a0[14]: Copied! <pre>sample_idx = 0\nsample_row = preprocess_newsgroup_row(newsgroups_test.data[sample_idx])\nsample_label = newsgroups_test.target_names[newsgroups_test.target[sample_idx]]\n\nprint(sample_row)\nprint('---')\nprint('Label:', sample_label)\n</pre> sample_idx = 0 sample_row = preprocess_newsgroup_row(newsgroups_test.data[sample_idx]) sample_label = newsgroups_test.target_names[newsgroups_test.target[sample_idx]]  print(sample_row) print('---') print('Label:', sample_label) <pre>Need info on 88-89 Bonneville\n\n\n I am a little confused on all of the models of the 88-89 bonnevilles.\nI have heard of the LE SE LSE SSE SSEI. Could someone tell me the\ndifferences are far as features or performance. I am also curious to\nknow what the book value is for prefereably the 89 model. And how much\nless than book value can you usually get them for. In other words how\nmuch are they in demand this time of year. I have heard that the mid-spring\nearly summer is the best time to buy.\n\n\t\t\tNeil Gandler\n\n---\nLabel: rec.autos\n</pre> <p>Passing the text directly in as a prompt does not yield the desired results. The model will attempt to respond to the message.</p> In\u00a0[15]: Copied! <pre>response = client.models.generate_content(\n    model=\"gemini-1.5-flash-001\", contents=sample_row)\nprint(response.text)\n</pre> response = client.models.generate_content(     model=\"gemini-1.5-flash-001\", contents=sample_row) print(response.text) <pre>You are right to be confused! Pontiac's Bonneville line in 1988-89 was a bit of a mess. Here's a breakdown:\n\n**Bonneville Models Explained**\n\n* **Base Bonneville:**  The most basic model. Usually had a 3.8L V6 engine and a more basic interior.\n\n* **LE:**  This stood for \"Luxury Edition\" and typically came with a bit more standard equipment, like upgraded upholstery, power options, and possibly a 5.0L V8 engine. \n\n* **SE:**  This stood for \"Sport Edition.\" These cars were geared towards a more sporty persona, with features like a firmer suspension, sport wheels, and sometimes a more powerful engine.\n\n* **LSE:**  This is where things get tricky. LSE could stand for \"Luxury Sport Edition,\" which combined features from the LE and SE trims. It also sometimes meant \"Limited Sport Edition,\" which typically had unique styling cues and more limited production.\n\n* **SSE:**  The \"SSE\" signified \"Special Service Edition.\"  These models had the 5.0L V8 engine and other performance-enhancing features.  The \"SSE\" designation was a bit confusing because sometimes they were just a specific package on a base Bonneville rather than a separate trim level.\n\n* **SSEi:**  This was a new model for 1989 and was intended as a high-performance variant of the Bonneville. It came with a larger 3.8L V6 engine with electronic fuel injection and other enhancements.\n\n**Book Value and Pricing**\n\nIt's impossible to give you a precise book value for an '89 Bonneville without knowing the specific model, mileage, condition, and location. However, you can use websites like Kelley Blue Book or Edmunds to get an estimate based on this information. \n\nGenerally,  you can expect to pay less than book value, especially for cars that are older and have higher mileage.  The demand for classic Bonnevilles can fluctuate depending on the model, condition, and time of year. Mid-spring to early summer is generally a good time to buy, as dealerships may be more willing to negotiate to clear out inventory.\n\n**Negotiating**\n\nHere are some tips for negotiating a good price:\n\n* **Research:**  Know what the fair market value of the car is before you negotiate.\n* **Be Patient:**  Don't rush into a deal. Take your time and shop around.\n* **Be Prepared to Walk Away:**  If you're not happy with the price, be prepared to walk away.\n\n**Important Considerations**\n\n* **Condition:**  A well-maintained Bonneville can be a reliable car, but they can also have issues. Pay close attention to the car's condition and be prepared to spend some money on repairs.\n* **Parts Availability:**  Parts for older Bonnevilles can be harder to find, so make sure you can get the parts you need if something breaks.\n\n**Good luck with your search!** \n\n</pre> <p>You can use the prompt engineering techniques you have learned this week to induce the model to perform the desired task. Try some of your own ideas and see what is effective, or check out the following cells for different approaches. Note that they have different levels of effectiveness!</p> In\u00a0[17]: Copied! <pre># Ask the model directly in a zero-shot prompt.\n\nprompt = \"From what newsgroup does the following message originate?\"\nbaseline_response = client.models.generate_content(\n    model=\"gemini-1.5-flash-001\",\n    contents=[prompt, sample_row])\nprint(baseline_response.text)\n</pre> # Ask the model directly in a zero-shot prompt.  prompt = \"From what newsgroup does the following message originate?\" baseline_response = client.models.generate_content(     model=\"gemini-1.5-flash-001\",     contents=[prompt, sample_row]) print(baseline_response.text) <pre>The message originates from the **alt.autos.pontiac** newsgroup. \n\nThis is evident from the content of the message, which is specifically focused on a Pontiac model, the Bonneville.  Newsgroup names often reflect the topics discussed within them. \n\n</pre> <p>This technique still produces quite a verbose response. You could try and parse out the relevant text, or refine the prompt even further.</p> In\u00a0[18]: Copied! <pre>from google.api_core import retry\n\n# You can use a system instruction to do more direct prompting, and get a\n# more succinct answer.\n\nsystem_instruct = \"\"\"\nYou are a classification service. You will be passed input that represents\na newsgroup post and you must respond with the newsgroup from which the post\noriginates.\n\"\"\"\n\n# Define a helper to retry when per-minute quota is reached.\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n# If you want to evaluate your own technique, replace this body of this function\n# with your model, prompt and other code and return the predicted answer.\n@retry.Retry(predicate=is_retriable)\ndef predict_label(post: str) -&gt; str:\n    response = client.models.generate_content(\n        model=\"gemini-1.5-flash-001\",\n        config=types.GenerateContentConfig(\n            system_instruction=system_instruct),\n        contents=post)\n\n    rc = response.candidates[0]\n\n    # Any errors, filters, recitation, etc we can mark as a general error\n    if rc.finish_reason.name != \"STOP\":\n        return \"(error)\"\n    else:\n        # Clean up the response.\n        return response.text.strip()\n\n\nprediction = predict_label(sample_row)\n\nprint(prediction)\nprint()\nprint(\"Correct!\" if prediction == sample_label else \"Incorrect.\")\n</pre> from google.api_core import retry  # You can use a system instruction to do more direct prompting, and get a # more succinct answer.  system_instruct = \"\"\" You are a classification service. You will be passed input that represents a newsgroup post and you must respond with the newsgroup from which the post originates. \"\"\"  # Define a helper to retry when per-minute quota is reached. is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})  # If you want to evaluate your own technique, replace this body of this function # with your model, prompt and other code and return the predicted answer. @retry.Retry(predicate=is_retriable) def predict_label(post: str) -&gt; str:     response = client.models.generate_content(         model=\"gemini-1.5-flash-001\",         config=types.GenerateContentConfig(             system_instruction=system_instruct),         contents=post)      rc = response.candidates[0]      # Any errors, filters, recitation, etc we can mark as a general error     if rc.finish_reason.name != \"STOP\":         return \"(error)\"     else:         # Clean up the response.         return response.text.strip()   prediction = predict_label(sample_row)  print(prediction) print() print(\"Correct!\" if prediction == sample_label else \"Incorrect.\") <pre>rec.autos.misc\n\nIncorrect.\n</pre> <p>Now run a short evaluation using the function defined above. The test set is further sampled to ensure the experiment runs smoothly on the API's free tier. In practice you would evaluate over the whole set.</p> In\u00a0[19]: Copied! <pre>import tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable tqdm features on Pandas.\ntqdmr.pandas()\n\n# But suppress the experimental warning\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n\n# Further sample the test data to be mindful of the free-tier quota.\ndf_baseline_eval = sample_data(df_test, 2, '.*')\n\n# Make predictions using the sampled data.\ndf_baseline_eval['Prediction'] = df_baseline_eval['Text'].progress_apply(predict_label)\n\n# And calculate the accuracy.\naccuracy = (df_baseline_eval[\"Class Name\"] == df_baseline_eval[\"Prediction\"]).sum() / len(df_baseline_eval)\nprint(f\"Accuracy: {accuracy:.2%}\")\n</pre> import tqdm from tqdm.rich import tqdm as tqdmr import warnings  # Enable tqdm features on Pandas. tqdmr.pandas()  # But suppress the experimental warning warnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)   # Further sample the test data to be mindful of the free-tier quota. df_baseline_eval = sample_data(df_test, 2, '.*')  # Make predictions using the sampled data. df_baseline_eval['Prediction'] = df_baseline_eval['Text'].progress_apply(predict_label)  # And calculate the accuracy. accuracy = (df_baseline_eval[\"Class Name\"] == df_baseline_eval[\"Prediction\"]).sum() / len(df_baseline_eval) print(f\"Accuracy: {accuracy:.2%}\") <pre>Output()</pre> <pre></pre> <pre>\n</pre> <pre>Accuracy: 37.50%\n</pre> <p>Now take a look at the dataframe to compare the predictions with the labels.</p> In\u00a0[20]: Copied! <pre>df_baseline_eval\n</pre> df_baseline_eval Out[20]: Text Label Class Name Prediction 0 Re: Too fast\\n\\n (Dan Day) writes:\\n&gt; In artic... 7 rec.autos rec.autos.sports.cars 1 Re: Information needed...\\n\\n (Youjip Won) wri... 7 rec.autos comp.sys.ibm.pc.hardware 2 Re: Maxima Chain wax (and mail-order)\\n\\nIn ar... 8 rec.motorcycles rec.motorcycles 3 Re: Shaft-drives and Wheelies \\n\\nIn article &lt;... 8 rec.motorcycles rec.motorcycles 4 Re: Yanks over A's George Speaks\\n\\nIn &lt;&gt;  wri... 9 rec.sport.baseball rec.sports.baseball 5 Re: Bosox go down in smoke II (Seattle 7-0) ..... 9 rec.sport.baseball rec.sport.baseball 6 Selfish hockey fans..\\n\\n\\n\\tOn Tuesday, when ... 10 rec.sport.hockey rec.sports.hockey 7 Re: Octopus in Detroit?\\n\\nValerie S. Hammerl ... 10 rec.sport.hockey rec.sport.hockey 8 Re: Let's build software cryptophones for over... 11 sci.crypt (error) 9 Crypto-PenPals\\n\\nI came. I lurked. I read the... 11 sci.crypt sci.crypt 10 Re: Lead Acid batteries &amp; Concrete?\\n\\nIn arti... 12 sci.electronics rec.autos.misc 11 Re: Lead ACid Batteries Part 2!!!\\n\\nIn articl... 12 sci.electronics rec.autos.misc 12 Re: Krillean Photography\\n\\nIn article &lt;&gt;  (Al... 13 sci.med alt.folklore.urban 13 Re: Strain Gage Applications in vivo\\n\\nIn art... 13 sci.med comp.bio.medical 14 Re: HST Servicing Mission Scheduled for 11 Day... 14 sci.space sci.space 15 Re: Why we like DC-X (was Re: Shuttle 0-Defect... 14 sci.space misc.space In\u00a0[21]: Copied! <pre>from collections.abc import Iterable\nimport random\n\n\n# Convert the data frame into a dataset suitable for tuning.\ninput_data = {'examples': \n    df_train[['Text', 'Class Name']]\n      .rename(columns={'Text': 'textInput', 'Class Name': 'output'})\n      .to_dict(orient='records')\n }\n\n# If you are re-running this lab, add your model_id here.\nmodel_id = None\n\n# Or try and find a recent tuning job.\nif not model_id:\n  queued_model = None\n  # Newest models first.\n  for m in reversed(client.tunings.list()):\n    # Only look at newsgroup classification models.\n    if m.name.startswith('tunedModels/newsgroup-classification-model'):\n      # If there is a completed model, use the first (newest) one.\n      if m.state.name == 'JOB_STATE_SUCCEEDED':\n        model_id = m.name\n        print('Found existing tuned model to reuse.')\n        break\n\n      elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:\n        # If there's a model still queued, remember the most recent one.\n        queued_model = m.name\n  else:\n    if queued_model:\n      model_id = queued_model\n      print('Found queued model, still waiting.')\n\n\n# Upload the training data and queue the tuning job.\nif not model_id:\n    tuning_op = client.tunings.tune(\n        base_model=\"models/gemini-1.5-flash-001-tuning\",\n        training_dataset=input_data,\n        config=types.CreateTuningJobConfig(\n            tuned_model_display_name=\"Newsgroup classification model\",\n            batch_size=16,\n            epoch_count=2,\n        ),\n    )\n\n    print(tuning_op.state)\n    model_id = tuning_op.name\n\nprint(model_id)\n</pre> from collections.abc import Iterable import random   # Convert the data frame into a dataset suitable for tuning. input_data = {'examples':      df_train[['Text', 'Class Name']]       .rename(columns={'Text': 'textInput', 'Class Name': 'output'})       .to_dict(orient='records')  }  # If you are re-running this lab, add your model_id here. model_id = None  # Or try and find a recent tuning job. if not model_id:   queued_model = None   # Newest models first.   for m in reversed(client.tunings.list()):     # Only look at newsgroup classification models.     if m.name.startswith('tunedModels/newsgroup-classification-model'):       # If there is a completed model, use the first (newest) one.       if m.state.name == 'JOB_STATE_SUCCEEDED':         model_id = m.name         print('Found existing tuned model to reuse.')         break        elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:         # If there's a model still queued, remember the most recent one.         queued_model = m.name   else:     if queued_model:       model_id = queued_model       print('Found queued model, still waiting.')   # Upload the training data and queue the tuning job. if not model_id:     tuning_op = client.tunings.tune(         base_model=\"models/gemini-1.5-flash-001-tuning\",         training_dataset=input_data,         config=types.CreateTuningJobConfig(             tuned_model_display_name=\"Newsgroup classification model\",             batch_size=16,             epoch_count=2,         ),     )      print(tuning_op.state)     model_id = tuning_op.name  print(model_id) <pre>/tmp/ipykernel_45/3257069937.py:40: ExperimentalWarning: The SDK's tuning implementation is experimental, and may change in future versions.\n  tuning_op = client.tunings.tune(\n</pre> <pre>JobState.JOB_STATE_QUEUED\ntunedModels/newsgroup-classification-model-yyxc6vlgd\n</pre> <p>This has created a tuning job that will run in the background. To inspect the progress of the tuning job, run this cell to plot the current status and loss curve. Once the status reaches <code>ACTIVE</code>, tuning is complete and the model is ready to use.</p> <p>Tuning jobs are queued, so it may look like no training steps have been taken initially but it will progress. Tuning can take anywhere from a few minutes to multiple hours, depending on factors like your dataset size and how busy the tuning infrastrature is. Why not treat yourself to a nice cup of tea while you wait, or come and say \"Hi!\" in the group Discord.</p> <p>It is safe to stop this cell at any point. It will not stop the tuning job.</p> <p>IMPORTANT: Due to the high volume of users doing this course, tuning jobs may be queued for many hours. Take a note of your tuned model ID above (<code>tunedModels/...</code>) so you can come back to it tomorrow. In the meantime, check out the Search grounding codelab. If you want to try tuning a local LLM, check out the fine-tuning guides for tuning a Gemma model.</p> In\u00a0[35]: Copied! <pre>import datetime\nimport time\n\n\nMAX_WAIT = datetime.timedelta(minutes=10)\n\nwhile not (tuned_model := client.tunings.get(name=model_id)).has_ended:\n\n    print(tuned_model.state)\n    time.sleep(60)\n\n    # Don't wait too long. Use a public model if this is going to take a while.\n    if datetime.datetime.now(datetime.timezone.utc) - tuned_model.create_time &gt; MAX_WAIT:\n        print(\"Taking a shortcut, using a previously prepared model.\")\n        model_id = \"tunedModels/newsgroup-classification-model-ltenbi1b\"\n        tuned_model = client.tunings.get(name=model_id)\n        break\n\n\nprint(f\"Done! The model state is: {tuned_model.state.name}\")\n\nif not tuned_model.has_succeeded and tuned_model.error:\n    print(\"Error:\", tuned_model.error)\n</pre> import datetime import time   MAX_WAIT = datetime.timedelta(minutes=10)  while not (tuned_model := client.tunings.get(name=model_id)).has_ended:      print(tuned_model.state)     time.sleep(60)      # Don't wait too long. Use a public model if this is going to take a while.     if datetime.datetime.now(datetime.timezone.utc) - tuned_model.create_time &gt; MAX_WAIT:         print(\"Taking a shortcut, using a previously prepared model.\")         model_id = \"tunedModels/newsgroup-classification-model-ltenbi1b\"         tuned_model = client.tunings.get(name=model_id)         break   print(f\"Done! The model state is: {tuned_model.state.name}\")  if not tuned_model.has_succeeded and tuned_model.error:     print(\"Error:\", tuned_model.error) <pre>JobState.JOB_STATE_RUNNING\nTaking a shortcut, using a previously prepared model.\nDone! The model state is: JOB_STATE_SUCCEEDED\n</pre> In\u00a0[36]: Copied! <pre>new_text = \"\"\"\nFirst-timer looking to get out of here.\n\nHi, I'm writing about my interest in travelling to the outer limits!\n\nWhat kind of craft can I buy? What is easiest to access from this 3rd rock?\n\nLet me know how to do that please.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model=model_id, contents=new_text)\n\nprint(response.text)\n</pre> new_text = \"\"\" First-timer looking to get out of here.  Hi, I'm writing about my interest in travelling to the outer limits!  What kind of craft can I buy? What is easiest to access from this 3rd rock?  Let me know how to do that please. \"\"\"  response = client.models.generate_content(     model=model_id, contents=new_text)  print(response.text) <pre>sci.space\n</pre> In\u00a0[37]: Copied! <pre>@retry.Retry(predicate=is_retriable)\ndef classify_text(text: str) -&gt; str:\n    \"\"\"Classify the provided text into a known newsgroup.\"\"\"\n    response = client.models.generate_content(\n        model=model_id, contents=text)\n    rc = response.candidates[0]\n\n    # Any errors, filters, recitation, etc we can mark as a general error\n    if rc.finish_reason.name != \"STOP\":\n        return \"(error)\"\n    else:\n        return rc.content.parts[0].text\n\n\n# The sampling here is just to minimise your quota usage. If you can, you should\n# evaluate the whole test set with `df_model_eval = df_test.copy()`.\ndf_model_eval = sample_data(df_test, 4, '.*')\n\ndf_model_eval[\"Prediction\"] = df_model_eval[\"Text\"].progress_apply(classify_text)\n\naccuracy = (df_model_eval[\"Class Name\"] == df_model_eval[\"Prediction\"]).sum() / len(df_model_eval)\nprint(f\"Accuracy: {accuracy:.2%}\")\n</pre> @retry.Retry(predicate=is_retriable) def classify_text(text: str) -&gt; str:     \"\"\"Classify the provided text into a known newsgroup.\"\"\"     response = client.models.generate_content(         model=model_id, contents=text)     rc = response.candidates[0]      # Any errors, filters, recitation, etc we can mark as a general error     if rc.finish_reason.name != \"STOP\":         return \"(error)\"     else:         return rc.content.parts[0].text   # The sampling here is just to minimise your quota usage. If you can, you should # evaluate the whole test set with `df_model_eval = df_test.copy()`. df_model_eval = sample_data(df_test, 4, '.*')  df_model_eval[\"Prediction\"] = df_model_eval[\"Text\"].progress_apply(classify_text)  accuracy = (df_model_eval[\"Class Name\"] == df_model_eval[\"Prediction\"]).sum() / len(df_model_eval) print(f\"Accuracy: {accuracy:.2%}\") <pre>Output()</pre> <pre></pre> <pre>\n</pre> <pre>Accuracy: 87.50%\n</pre> In\u00a0[38]: Copied! <pre># Calculate the *input* cost of the baseline model with system instructions.\nsysint_tokens = client.models.count_tokens(\n    model='gemini-1.5-flash-001', contents=[system_instruct, sample_row]\n).total_tokens\nprint(f'System instructed baseline model: {sysint_tokens} (input)')\n\n# Calculate the input cost of the tuned model.\ntuned_tokens = client.models.count_tokens(model=tuned_model.base_model, contents=sample_row).total_tokens\nprint(f'Tuned model: {tuned_tokens} (input)')\n\nsavings = (sysint_tokens - tuned_tokens) / tuned_tokens\nprint(f'Token savings: {savings:.2%}')  # Note that this is only n=1.\n</pre> # Calculate the *input* cost of the baseline model with system instructions. sysint_tokens = client.models.count_tokens(     model='gemini-1.5-flash-001', contents=[system_instruct, sample_row] ).total_tokens print(f'System instructed baseline model: {sysint_tokens} (input)')  # Calculate the input cost of the tuned model. tuned_tokens = client.models.count_tokens(model=tuned_model.base_model, contents=sample_row).total_tokens print(f'Tuned model: {tuned_tokens} (input)')  savings = (sysint_tokens - tuned_tokens) / tuned_tokens print(f'Token savings: {savings:.2%}')  # Note that this is only n=1. <pre>System instructed baseline model: 172 (input)\nTuned model: 136 (input)\nToken savings: 26.47%\n</pre> <p>The earlier verbose model also produced more output tokens than needed for this task.</p> In\u00a0[39]: Copied! <pre>baseline_token_output = baseline_response.usage_metadata.candidates_token_count\nprint('Baseline (verbose) output tokens:', baseline_token_output)\n\ntuned_model_output = client.models.generate_content(\n    model=model_id, contents=sample_row)\ntuned_tokens_output = tuned_model_output.usage_metadata.candidates_token_count\nprint('Tuned output tokens:', tuned_tokens_output)\n</pre> baseline_token_output = baseline_response.usage_metadata.candidates_token_count print('Baseline (verbose) output tokens:', baseline_token_output)  tuned_model_output = client.models.generate_content(     model=model_id, contents=sample_row) tuned_tokens_output = tuned_model_output.usage_metadata.candidates_token_count print('Tuned output tokens:', tuned_tokens_output) <pre>Baseline (verbose) output tokens: 52\nTuned output tokens: 4\n</pre>"},{"location":"day_04/day-4-fine-tuning-a-custom-model/#copyright-2025-google-llc","title":"Copyright 2025 Google LLC.\u00b6","text":""},{"location":"day_04/day-4-fine-tuning-a-custom-model/#day-4-fine-tuning-a-custom-model","title":"Day 4 - Fine tuning a custom model\u00b6","text":"<p>Welcome back to the Kaggle 5-day Generative AI course!</p> <p>In this notebook you will use the Gemini API to fine-tune a custom, task-specific model. Fine-tuning can be used for a variety of tasks from classic NLP problems like entity extraction or summarisation, to creative tasks like stylised generation. You will fine-tune a model to classify the category a piece of text (a newsgroup post) into the category it belongs to (the newsgroup name).</p> <p>This codelab walks you tuning a model with the API. AI Studio also supports creating new tuned models directly in the web UI, allowing you to quickly create and monitor models using data from Google Sheets, Drive or your own files.</p> <p>Note: We recommend doing this codelab first today. There may be a period of waiting while the model tunes, so if you start with this one, you can try the other codelab while you wait.</p>"},{"location":"day_04/day-4-fine-tuning-a-custom-model/#set-up-your-api-key","title":"Set up your API key\u00b6","text":"<p>To run the following cell, your API key must be stored it in a Kaggle secret named <code>GOOGLE_API_KEY</code>.</p> <p>If you don't already have an API key, you can grab one from AI Studio. You can find detailed instructions in the docs.</p> <p>To make the key available through Kaggle secrets, choose <code>Secrets</code> from the <code>Add-ons</code> menu and follow the instructions to add your key or enable it for this notebook.</p>"},{"location":"day_04/day-4-fine-tuning-a-custom-model/#explore-available-models","title":"Explore available models\u00b6","text":"<p>You will be using the <code>TunedModel.create</code> API method to start the fine-tuning job and create your custom model. Find a model that supports it through the <code>models.list</code> endpoint. You can also find more information about tuning models in the model tuning docs.</p>"},{"location":"day_04/day-4-fine-tuning-a-custom-model/#download-the-dataset","title":"Download the dataset\u00b6","text":"<p>In this activity, you will use the same newsgroups dataset that you used to train a classifier in Keras. In this example you will use a fine-tuned Gemini model to achieve the same goal.</p> <p>The 20 Newsgroups Text Dataset contains 18,000 newsgroups posts on 20 topics divided into training and test sets.</p>"},{"location":"day_04/day-4-fine-tuning-a-custom-model/#prepare-the-dataset","title":"Prepare the dataset\u00b6","text":"<p>You'll use the same pre-processing code you used for the custom model on day 2. This pre-processing removes personal information, which can be used to \"shortcut\" to known users of a forum, and formats the text to appear a bit more like regular text and less like a newsgroup post (e.g. by removing the mail headers). This normalisation allows the model to generalise to regular text and not over-depend on specific fields. If your input data is always going to be newsgroup posts, it may be helpful to leave this structure in place if they provide genuine signals.</p>"},{"location":"day_04/day-4-fine-tuning-a-custom-model/#evaluate-baseline-performance","title":"Evaluate baseline performance\u00b6","text":"<p>Before you start tuning a model, it's good practice to perform an evaluation on the available models to ensure you can measure how much the tuning helps.</p> <p>First identify a single sample row to use for visual inspection.</p>"},{"location":"day_04/day-4-fine-tuning-a-custom-model/#tune-a-custom-model","title":"Tune a custom model\u00b6","text":"<p>In this example you'll use tuning to create a model that requires no prompting or system instructions and outputs succinct text from the classes you provide in the training data.</p> <p>The data contains both input text (the processed posts) and output text (the category, or newsgroup), that you can use to start tuning a model.</p> <p>When calling <code>tune()</code>, you can specify model tuning hyperparameters too:</p> <ul> <li><code>epoch_count</code>: defines how many times to loop through the data,</li> <li><code>batch_size</code>: defines how many rows to process in a single step, and</li> <li><code>learning_rate</code>: defines the scaling factor for updating model weights at each step.</li> </ul> <p>You can also choose to omit them and use the defaults. Learn more about these parameters and how they work. For this example these parameters were selected by running some tuning jobs and selecting parameters that converged efficiently.</p> <p>This example will start a new tuning job, but only if one does not already exist. This allows you to leave this codelab and come back later - re-running this step will find your last model.</p>"},{"location":"day_04/day-4-fine-tuning-a-custom-model/#use-the-new-model","title":"Use the new model\u00b6","text":"<p>Now that you have a tuned model, try it out with custom data. You use the same API as a normal Gemini API interaction, but you specify your new model as the model name, which will start with <code>tunedModels/</code>.</p>"},{"location":"day_04/day-4-fine-tuning-a-custom-model/#evaluation","title":"Evaluation\u00b6","text":"<p>You can see that the model outputs labels that correspond to those in the training data, and without any system instructions or prompting, which is already a great improvement. Now see how well it performs on the test set.</p> <p>Note that there is no parallelism in this example; classifying the test sub-set will take a few minutes.</p>"},{"location":"day_04/day-4-fine-tuning-a-custom-model/#compare-token-usage","title":"Compare token usage\u00b6","text":"<p>AI Studio and the Gemini API provide model tuning at no cost, however normal limits and charges apply for use of a tuned model.</p> <p>The size of the input prompt and other generation config like system instructions, as well as the number of generated output tokens, all contribute to the overall cost of a request.</p>"},{"location":"day_04/day-4-fine-tuning-a-custom-model/#next-steps","title":"Next steps\u00b6","text":"<p>Now that you have tuned a classification model, try some other tasks, like tuning a model to respond with a specific tone or style using hand-written examples (or even generated examples!). Kaggle hosts a number of datasets you can try out.</p> <p>Learn about when supervised fine-tuning is most effective.</p> <p>And check out the fine-tuning tutorial for another example that shows a tuned model extending beyond the training data to new, unseen inputs.</p> <p>- Mark McD</p>"},{"location":"day_04/day-4-google-search-grounding/","title":"Day 4 - Google Search grounding with the Gemini API","text":"In\u00a0[1]: Copied! <pre># @title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # @title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. In\u00a0[2]: Copied! <pre># Uninstall packages from Kaggle base image that are not needed.\n!pip uninstall -qy jupyterlab jupyterlab-lsp\n# Install the google-genai SDK for this codelab.\n!pip install -qU 'google-genai==1.7.0'\n</pre> # Uninstall packages from Kaggle base image that are not needed. !pip uninstall -qy jupyterlab jupyterlab-lsp # Install the google-genai SDK for this codelab. !pip install -qU 'google-genai==1.7.0' In\u00a0[3]: Copied! <pre>from google import genai\nfrom google.genai import types\n\nfrom IPython.display import Markdown, HTML, display\n\ngenai.__version__\n</pre> from google import genai from google.genai import types  from IPython.display import Markdown, HTML, display  genai.__version__ Out[3]: <pre>'1.7.0'</pre> In\u00a0[4]: Copied! <pre>from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)\n</pre> from kaggle_secrets import UserSecretsClient  GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")  client = genai.Client(api_key=GOOGLE_API_KEY) <p>If you received an error response along the lines of <code>No user secrets exist for kernel id ...</code>, then you need to add your API key via <code>Add-ons</code>, <code>Secrets</code> and enable it.</p> <p></p> In\u00a0[5]: Copied! <pre># Define a retry policy. The model might make multiple consecutive calls automatically\n# for a complex query, this ensures the client retries if it hits quota limits.\nfrom google.api_core import retry\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\nif not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n  genai.models.Models.generate_content = retry.Retry(\n      predicate=is_retriable)(genai.models.Models.generate_content)\n</pre> # Define a retry policy. The model might make multiple consecutive calls automatically # for a complex query, this ensures the client retries if it hits quota limits. from google.api_core import retry  is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})  if not hasattr(genai.models.Models.generate_content, '__wrapped__'):   genai.models.Models.generate_content = retry.Retry(       predicate=is_retriable)(genai.models.Models.generate_content) In\u00a0[6]: Copied! <pre># Ask for information without search grounding.\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=\"When and where is Billie Eilish's next concert?\")\n\nMarkdown(response.text)\n</pre> # Ask for information without search grounding. response = client.models.generate_content(     model='gemini-2.0-flash',     contents=\"When and where is Billie Eilish's next concert?\")  Markdown(response.text) Out[6]: <p>Unfortunately, Billie Eilish doesn't have any upcoming concerts planned for 2024. She wrapped up her \"Happier Than Ever\" tour in 2023 and hasn't announced any new tour dates. For up-to-date information, I recommend checking her official website and social media channels.</p> <p>Now try with grounding enabled.</p>  Open in AI Studio In\u00a0[59]: Copied! <pre># And now re-run the same query with search grounding enabled.\nconfig_with_search = types.GenerateContentConfig(\n    tools=[types.Tool(google_search=types.GoogleSearch())],\n)\n\ndef query_with_grounding():\n    response = client.models.generate_content(\n        model='gemini-2.0-flash',\n        contents=\"When and where is Billie Eilish's next concert?\",\n        config=config_with_search,\n    )\n    return response.candidates[0]\n\n\nrc = query_with_grounding()\nMarkdown(rc.content.parts[0].text)\n</pre> # And now re-run the same query with search grounding enabled. config_with_search = types.GenerateContentConfig(     tools=[types.Tool(google_search=types.GoogleSearch())], )  def query_with_grounding():     response = client.models.generate_content(         model='gemini-2.0-flash',         contents=\"When and where is Billie Eilish's next concert?\",         config=config_with_search,     )     return response.candidates[0]   rc = query_with_grounding() Markdown(rc.content.parts[0].text) Out[59]: <p>Billie Eilish is currently on her \"HIT ME HARD AND SOFT: THE TOUR\". According to available information, her next concerts are:</p> <ul> <li>April 23, 2025: Avicii Arena, Stockholm, Sweden</li> <li>April 24, 2025: Avicii Arena, Stockholm, Sweden</li> <li>April 26, 2025: Unity Arena, Fornebu, Norway</li> <li>April 28, 2025: Royal Arena, Copenhagen, Denmark</li> <li>April 29, 2025: Royal Arena, Copenhagen, Denmark</li> </ul> <p>The tour continues through Europe and concludes on July 27, 2025, at the 3Arena in Dublin.</p> In\u00a0[60]: Copied! <pre>while not rc.grounding_metadata.grounding_supports or not rc.grounding_metadata.grounding_chunks:\n    # If incomplete grounding data was returned, retry.\n    rc = query_with_grounding()\n\nchunks = rc.grounding_metadata.grounding_chunks\nfor chunk in chunks:\n    print(f'{chunk.web.title}: {chunk.web.uri}')\n</pre> while not rc.grounding_metadata.grounding_supports or not rc.grounding_metadata.grounding_chunks:     # If incomplete grounding data was returned, retry.     rc = query_with_grounding()  chunks = rc.grounding_metadata.grounding_chunks for chunk in chunks:     print(f'{chunk.web.title}: {chunk.web.uri}') <pre>theo2.co.uk: https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWQVqAIhSi4TFtQiZvzhIp25MzmBl-IucHKcCxP0DNLMBC_CYMPAORUppqIbT8ntzsb96HYNKNWqGCCLVnDJBj5Vl7ZYVAnk3qiKhmEzTU49g9fkW3UA6MxGVMUiThZu0-O6f7AjDGVFSdCiVFBvzyKucbkCea4erKKU0l9QUb6bkxDeaRcfiV4-_5nqhW8z\nticketmaster.co.uk: https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWQVqAJDHV4V-uxBpPDD6ZwD56bOf3HMLV3iKJof-v0rM1cxqEVCCf6zxQvW2ctOwYtwx7014qORj6CJmR-N9alL6TNBHa_H0JUY5hLsN4hLD_oljsvAIXcRGNpDh7XU_rVXOYpM-k6j0T0JOX7FTn5hVBPIBHDnARMd2se78ZJcIA==\nticketmaster.com: https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWQVqAKeOZxsSUyYugFxkkWDj-EbsetgfMn-Ircws4BKF8UtCYZaE6_AX3DBFYefGMcpGZ1HXPEs14z9GLk78MEhQ1QbdYGeoPnbk7iQ4IFR06Z7s30wsT8oNeQ0396aEAHJdSRv5jT2jPevF9qnlpwOnDBheo8kjlDBgsfaM4A=\nwikipedia.org: https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWQVqALJOCS2gzwt609gBLiuDuASljmZcY9RO1bNt5y6IKL9048GywRSpz813EINqAmpiXIwtPpGs0aeAuE_TSnHtgdpua3R31DNds8cIjhMRD6OdwD1-d5bo37ZhQNzGxOwsOnM0KC83BtjITHEwNvHyaVaT8Iqj20G\nbillieeilish.com: https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWQVqAJEDrl0vHGQ73UHwtVawwB4bcIrk4FWN09mI-aSVjONPDGddKnwNHt54r8FR5bJN8IK4ZBgSK_2kt50koeW7TmCfJUhdP4-K_Qkto_5MN1vEk6i5jXEIs5cg9G_qYWvIv2QhKo=\nsongkick.com: https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWQVqALXgsS4nJNMDq7OJwub17E6zQdldorOjaKHa9wkW4fBeOqP0ItpnISvzfA03NWDuKoJxda31xyA46N6kR0Yi6pzaqldCnBY4IqMgWXvfu_MuM6LUHOm7cDfqNqAf5nymrdiSo3dCKHFunlrkeSSQ2-I\n</pre> <p>As part of the response, there is a standalone styled HTML content block that you use to link back to relevant search suggestions related to the generation.</p> In\u00a0[\u00a0]: Copied! <pre>HTML(rc.grounding_metadata.search_entry_point.rendered_content)\n</pre> HTML(rc.grounding_metadata.search_entry_point.rendered_content) <p>The <code>grounding_supports</code> in the metadata provide a way for you to correlate the grounding chunks used to the generated output text.</p> In\u00a0[61]: Copied! <pre>from pprint import pprint\n\nsupports = rc.grounding_metadata.grounding_supports\nfor support in supports:\n    pprint(support.to_json_dict())\n</pre> from pprint import pprint  supports = rc.grounding_metadata.grounding_supports for support in supports:     pprint(support.to_json_dict()) <pre>{'confidence_scores': [0.9238536, 0.63184303, 0.7186428, 0.7688701],\n 'grounding_chunk_indices': [0, 1, 2, 3],\n 'segment': {'end_index': 67,\n             'text': 'Billie Eilish is currently on her \"HIT ME HARD AND SOFT: '\n                     'THE TOUR\".'}}\n{'confidence_scores': [0.65632933],\n 'grounding_chunk_indices': [2],\n 'segment': {'end_index': 183,\n             'start_index': 128,\n             'text': '*   **April 23, 2025:** Avicii Arena, Stockholm, Sweden'}}\n{'confidence_scores': [0.7851307, 0.69624656, 0.6140977],\n 'grounding_chunk_indices': [4, 5, 2],\n 'segment': {'end_index': 292,\n             'start_index': 240,\n             'text': '*   **April 26, 2025:** Unity Arena, Fornebu, Norway'}}\n{'confidence_scores': [0.604772],\n 'grounding_chunk_indices': [2],\n 'segment': {'end_index': 349,\n             'start_index': 293,\n             'text': '*   **April 28, 2025:** Royal Arena, Copenhagen, '\n                     'Denmark'}}\n{'confidence_scores': [0.63103974, 0.6163921],\n 'grounding_chunk_indices': [5, 2],\n 'segment': {'end_index': 406,\n             'start_index': 350,\n             'text': '*   **April 29, 2025:** Royal Arena, Copenhagen, '\n                     'Denmark'}}\n{'confidence_scores': [0.9062436, 0.60820603],\n 'grounding_chunk_indices': [3, 1],\n 'segment': {'end_index': 498,\n             'start_index': 408,\n             'text': 'The tour continues through Europe and concludes on July '\n                     '27, 2025, at the 3Arena in Dublin.'}}\n</pre> <p>These supports can be used to highlight text in the response, or build tables of footnotes.</p> In\u00a0[62]: Copied! <pre>import io\n\nmarkdown_buffer = io.StringIO()\n\n# Print the text with footnote markers.\nmarkdown_buffer.write(\"Supported text:\\n\\n\")\nfor support in supports:\n    markdown_buffer.write(\" * \")\n    markdown_buffer.write(\n        rc.content.parts[0].text[support.segment.start_index : support.segment.end_index]\n    )\n\n    for i in support.grounding_chunk_indices:\n        chunk = chunks[i].web\n        markdown_buffer.write(f\"&lt;sup&gt;[{i+1}]&lt;/sup&gt;\")\n\n    markdown_buffer.write(\"\\n\\n\")\n\n\n# And print the footnotes.\nmarkdown_buffer.write(\"Citations:\\n\\n\")\nfor i, chunk in enumerate(chunks, start=1):\n    markdown_buffer.write(f\"{i}. [{chunk.web.title}]({chunk.web.uri})\\n\")\n\n\nMarkdown(markdown_buffer.getvalue())\n</pre> import io  markdown_buffer = io.StringIO()  # Print the text with footnote markers. markdown_buffer.write(\"Supported text:\\n\\n\") for support in supports:     markdown_buffer.write(\" * \")     markdown_buffer.write(         rc.content.parts[0].text[support.segment.start_index : support.segment.end_index]     )      for i in support.grounding_chunk_indices:         chunk = chunks[i].web         markdown_buffer.write(f\"<sup>[{i+1}]</sup>\")      markdown_buffer.write(\"\\n\\n\")   # And print the footnotes. markdown_buffer.write(\"Citations:\\n\\n\") for i, chunk in enumerate(chunks, start=1):     markdown_buffer.write(f\"{i}. [{chunk.web.title}]({chunk.web.uri})\\n\")   Markdown(markdown_buffer.getvalue()) Out[62]: <p>Supported text:</p> <ul> <li><p>Billie Eilish is currently on her \"HIT ME HARD AND SOFT: THE TOUR\".<sup>[1]</sup><sup>[2]</sup><sup>[3]</sup><sup>[4]</sup></p> </li> <li><ul> <li>April 23, 2025: Avicii Arena, Stockholm, Sweden<sup>[3]</sup></li> </ul> </li> <li><ul> <li>April 26, 2025: Unity Arena, Fornebu, Norway<sup>[5]</sup><sup>[6]</sup><sup>[3]</sup></li> </ul> </li> <li><ul> <li>April 28, 2025: Royal Arena, Copenhagen, Denmark<sup>[3]</sup></li> </ul> </li> <li><ul> <li>April 29, 2025: Royal Arena, Copenhagen, Denmark<sup>[6]</sup><sup>[3]</sup></li> </ul> </li> <li><p>The tour continues through Europe and concludes on July 27, 2025, at the 3Arena in Dublin.<sup>[4]</sup><sup>[2]</sup></p> </li> </ul> <p>Citations:</p> <ol> <li>theo2.co.uk</li> <li>ticketmaster.co.uk</li> <li>ticketmaster.com</li> <li>wikipedia.org</li> <li>billieeilish.com</li> <li>songkick.com</li> </ol> In\u00a0[63]: Copied! <pre>from IPython.display import display, Image, Markdown\n\ndef show_response(response):\n    for p in response.candidates[0].content.parts:\n        if p.text:\n            display(Markdown(p.text))\n        elif p.inline_data:\n            display(Image(p.inline_data.data))\n        else:\n            print(p.to_json_dict())\n    \n        display(Markdown('----'))\n</pre> from IPython.display import display, Image, Markdown  def show_response(response):     for p in response.candidates[0].content.parts:         if p.text:             display(Markdown(p.text))         elif p.inline_data:             display(Image(p.inline_data.data))         else:             print(p.to_json_dict())              display(Markdown('----')) <p>Now start a chat asking for some information. Here you provide the Google Search tool so that the model can look up data from Google's Search index.</p> In\u00a0[67]: Copied! <pre>config_with_search = types.GenerateContentConfig(\n    tools=[types.Tool(google_search=types.GoogleSearch())],\n    temperature=0.0,\n)\n\nchat = client.chats.create(model='gemini-2.0-flash')\n\nresponse = chat.send_message(\n    message=\"What were the medal tallies, by top-10 countries, for the 2024 olympics?\",\n    config=config_with_search,\n)\n\nshow_response(response)\n</pre> config_with_search = types.GenerateContentConfig(     tools=[types.Tool(google_search=types.GoogleSearch())],     temperature=0.0, )  chat = client.chats.create(model='gemini-2.0-flash')  response = chat.send_message(     message=\"What were the medal tallies, by top-10 countries, for the 2024 olympics?\",     config=config_with_search, )  show_response(response) <p>The top 10 countries in the medal tally for the 2024 Paris Olympics were:</p> <ol> <li>United States: 40 Gold, 44 Silver, 42 Bronze (126 total)</li> <li>China: 40 Gold, 27 Silver, 24 Bronze (91 total)</li> <li>Japan: 20 Gold, 12 Silver, 13 Bronze (45 total)</li> <li>Australia: 18 Gold, 19 Silver, 16 Bronze (53 total)</li> <li>France: 16 Gold, 26 Silver, 22 Bronze (64 total)</li> <li>Netherlands: 15 Gold, 7 Silver, 12 Bronze (34 total)</li> <li>Great Britain: 14 Gold, 22 Silver, 29 Bronze (65 total)</li> <li>Republic of Korea: 13 Gold, 9 Silver, 10 Bronze (32 total)</li> <li>Italy: 12 Gold, 13 Silver, 15 Bronze (40 total)</li> <li>Germany: 12 Gold, 13 Silver, 8 Bronze (33 total)</li> </ol> <p>Continuing the chat, now ask the model to convert the data into a chart. The <code>code_execution</code> tool is able to generate code to draw charts, execute that code and return the image. You can see the executed code in the <code>executable_code</code> part of the response.</p> <p>Combining results from Google Search with tools like live plotting can enable very powerful use cases that require very little code to run.</p> In\u00a0[68]: Copied! <pre>config_with_code = types.GenerateContentConfig(\n    tools=[types.Tool(code_execution=types.ToolCodeExecution())],\n    temperature=0.0,\n)\n\nresponse = chat.send_message(\n    message=\"Now plot this as a seaborn chart. Break out the medals too.\",\n    config=config_with_code,\n)\n\nshow_response(response)\n</pre> config_with_code = types.GenerateContentConfig(     tools=[types.Tool(code_execution=types.ToolCodeExecution())],     temperature=0.0, )  response = chat.send_message(     message=\"Now plot this as a seaborn chart. Break out the medals too.\",     config=config_with_code, )  show_response(response) <p>Okay, I can create a Seaborn chart visualizing the medal tallies for the top 10 countries in the 2024 Olympics. I'll break out the gold, silver, and bronze medals for each country.</p> <pre>{'executable_code': {'code': \"import pandas as pd\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# Data for the medal tally\\ndata = {\\n    'Country': ['United States', 'China', 'Japan', 'Australia', 'France', 'Netherlands', 'Great Britain', 'Republic of Korea', 'Italy', 'Germany'],\\n    'Gold': [40, 40, 20, 18, 16, 15, 14, 13, 12, 12],\\n    'Silver': [44, 27, 12, 19, 26, 7, 22, 9, 13, 13],\\n    'Bronze': [42, 24, 13, 16, 22, 12, 29, 10, 15, 8],\\n    'Total': [126, 91, 45, 53, 64, 34, 65, 32, 40, 33]\\n}\\n\\ndf = pd.DataFrame(data)\\n\\n# Melt the dataframe to have a 'Medal' column\\ndf_melted = pd.melt(df, id_vars=['Country'], value_vars=['Gold', 'Silver', 'Bronze'], var_name='Medal', value_name='Count')\\n\\n# Create the bar plot\\nplt.figure(figsize=(14, 8))\\nsns.barplot(x='Country', y='Count', hue='Medal', data=df_melted, palette=['#FFD700', '#C0C0C0', '#CD7F32']) # Gold, Silver, Bronze colors\\nplt.title('2024 Olympics Medal Tally (Top 10 Countries)')\\nplt.xlabel('Country')\\nplt.ylabel('Medal Count')\\nplt.xticks(rotation=45, ha='right')\\nplt.tight_layout()\\nplt.show()\\n\", 'language': 'PYTHON'}}\n</pre> <p>The chart displays the medal distribution (Gold, Silver, and Bronze) for the top 10 countries at the 2024 Olympics. The United States and China lead in Gold medals. Great Britain has a high number of Bronze medals. The chart effectively visualizes the breakdown of medals for each country.</p>"},{"location":"day_04/day-4-google-search-grounding/#copyright-2025-google-llc","title":"Copyright 2025 Google LLC.\u00b6","text":""},{"location":"day_04/day-4-google-search-grounding/#day-4-google-search-grounding-with-the-gemini-api","title":"Day 4 - Google Search grounding with the Gemini API\u00b6","text":"<p>Welcome back to the Kaggle 5-day Generative AI course!</p> <p>In this optional notebook, you will use Google Search results with the Gemini API in a technique called grounding, where the model is connected to verifiable sources of information. Using search grounding is similar to using the RAG system you implemented earlier in the week, but the Gemini API automates a lot of it for you. The model generates Google Search queries and invokes the searches automatically, retrieving relevant data from Google's index of the web and providing links to search suggestions that support the query, so your users can verify the sources.</p>"},{"location":"day_04/day-4-google-search-grounding/#new-in-gemini-20","title":"New in Gemini 2.0\u00b6","text":"<p>Gemini 2.0 Flash provides a generous Google Search quota as part of the free tier. If you switch models back to 1.5, you will need to enable billing to use Grounding with Google Search, or you can try it out in AI Studio. See the earlier versions of this notebook for guidance.</p>"},{"location":"day_04/day-4-google-search-grounding/#optional-use-google-ai-studio","title":"Optional: Use Google AI Studio\u00b6","text":"<p>If you wish to try out grounding with Google Search, follow this section to try it out using the AI Studio interface. Or skip ahead to the <code>API</code> section to try the feature here in your notebook.</p>"},{"location":"day_04/day-4-google-search-grounding/#open-ai-studio","title":"Open AI Studio\u00b6","text":"<p>Start by going to AI Studio. You should be in the \"New chat\" interface.</p> <p>Search Grounding is best with <code>gemini-2.0-flash</code>, but try out <code>gemini-1.5-flash</code> too.</p> <p></p>"},{"location":"day_04/day-4-google-search-grounding/#ask-a-question","title":"Ask a question\u00b6","text":"<p>Now enter a prompt into the chat interface. Try asking something that is timely and might require recent information to answer, like a recent sport score. For this query, grounding will be disabled by default.</p> <p>This screenshow shows the response for <code>What were the top halloween costumes this year?</code>. Every execution will be different but typically the model talks about 2023, and hedges its responses saying it doesn't have access to specific information resulting in a general comment, rather than specific answers.</p> <p></p>"},{"location":"day_04/day-4-google-search-grounding/#enable-grounding","title":"Enable grounding\u00b6","text":"<p>On the right-hand sidebar, under the <code>Tools</code> section. Find and enable the <code>Grounding</code> option.</p> <p></p> <p>Now re-run your question by hovering over the user prompt in the chat history, and pressing the Gemini \u2728 icon to re-run your prompt.</p> <p></p> <p>You should now see a response generated that references sources from Google Search.</p> <p></p>"},{"location":"day_04/day-4-google-search-grounding/#try-your-own-queries","title":"Try your own queries\u00b6","text":"<p>Explore this interface and try some other queries. Share what works well in the Discord! You can start from this blank template that has search grounding enabled.</p> <p>The remaining steps require an API key with billing enabled. They are not required to complete this course; if you have tried grounding in AI Studio you are done for this notebook.</p>"},{"location":"day_04/day-4-google-search-grounding/#use-the-api","title":"Use the API\u00b6","text":"<p>Start by installing and importing the Gemini API Python SDK.</p>"},{"location":"day_04/day-4-google-search-grounding/#set-up-your-api-key","title":"Set up your API key\u00b6","text":"<p>To run the following cell, your API key must be stored it in a Kaggle secret named <code>GOOGLE_API_KEY</code>.</p> <p>If you don't already have an API key, you can grab one from AI Studio. You can find detailed instructions in the docs.</p> <p>To make the key available through Kaggle secrets, choose <code>Secrets</code> from the <code>Add-ons</code> menu and follow the instructions to add your key or enable it for this notebook.</p>"},{"location":"day_04/day-4-google-search-grounding/#automated-retry","title":"Automated retry\u00b6","text":""},{"location":"day_04/day-4-google-search-grounding/#use-search-grounding","title":"Use search grounding\u00b6","text":""},{"location":"day_04/day-4-google-search-grounding/#model-support","title":"Model support\u00b6","text":"<p>Search grounding is available in a limited set of models. Find a model that supports it on the models page.</p> <p>In this guide, you'll use <code>gemini-2.0-flash</code>.</p>"},{"location":"day_04/day-4-google-search-grounding/#make-a-request","title":"Make a request\u00b6","text":"<p>To enable search grounding, you specify it as a tool: <code>google_search</code>. Like other tools, this is supplied as a parameter in <code>GenerateContentConfig</code>, and can be passed to <code>generate_content</code> calls as well as <code>chats.create</code> (for all chat turns) or <code>chat.send_message</code> (for specific turns).</p>  Open in AI Studio"},{"location":"day_04/day-4-google-search-grounding/#response-metadata","title":"Response metadata\u00b6","text":"<p>When search grounding is used, the model returns extra metadata that includes links to search suggestions, supporting documents and information on how the supporting documents were used.</p> <p>Each \"grounding chunk\" represents information retrieved from Google Search that was used in the grounded generation request. Following the URI will take you to the source.</p>"},{"location":"day_04/day-4-google-search-grounding/#search-with-tools","title":"Search with tools\u00b6","text":"<p>In this example, you'll use enable the Google Search grounding tool and the code generation tool across two steps. In the first step, the model will use Google Search to find the requested information and then in the follow-up question, it generates code to plot the results.</p> <p>This usage includes textual, visual and code parts, so first define a function to help visualise these.</p>"},{"location":"day_04/day-4-google-search-grounding/#further-reading","title":"Further reading\u00b6","text":"<p>When using search grounding, there are some specific requirements that you must follow, including when and how to show search suggestions, and how to use the grounding links.  Be sure to read and follow the details in the search grounding capability guide and the search suggestions guide.</p> <p>Also check out some more compelling examples of using search grounding with the Live API in the cookbook, like this example that uses Google Maps to plot Search results on a map in an audio conversation, or this example that builds a comprehensive research report.</p> <p>- Mark McD</p>"},{"location":"day_04/extras/","title":"Day 4 Livestream with Paige Bailey \u2013 5-Day Gen AI Intensive Course | Kaggle","text":""},{"location":"day_04/podcast/","title":"Whitepaper Companion Podcast - Solving Domain-Specific Problems Using LLMs","text":""},{"location":"day_04/whitepaper/","title":"Solving Domain-Specific Problems Using LLMs","text":"<p>Authors: Christopher Semturs, Shekoofeh Azizi, Scott Coull, Umesh Shankar and Wieland Holfelder </p>"},{"location":"day_04/whitepaper/#introduction","title":"Introduction","text":"<p>Large language models (LLMs) have emerged as powerful tools for tackling complex challenges in numerous domains. While early iterations focused on general-purpose tasks, recent developments have highlighted the potential of fine-tuning LLMs to address specific problems within specialized fields. This whitepaper explores these concepts in two distinct domains: cybersecurity and medicine. Each showcases the unique ability of LLMs to enhance existing workflows and unlock new possibilities. Cybersecurity presents a number of unique challenges for LLMs, including a scarcity of publicly available data, a wide diversity of highly technical concepts, and information about threats that change on a daily basis. Additionally, sensitive use cases, like malware analysis, necessitate specific considerations for model development. We address these challenges by focusing on cybersecurity-specific content and tasks, pairing security-focused language models with a suite of supporting techniques to offer improved performance for vital tasks like threat identification and risk analysis.</p> <p>In the field of medicine, LLMs face a different set of obstacles, such as the vast and ever-evolving nature of medical knowledge and the need to apply said knowledge in a context-dependent manner that makes accurate diagnosis and treatment a continual challenge.</p> <p>LLMs like Med-PaLM, customized for medical applications, demonstrate the ability to answer complex medical questions and provide insightful interpretations of medical data, showing potential for supporting both clinicians and patients. Through the lens of these two distinct domains, in this whitepaper we will explore the challenges and opportunities presented by specialized data, technical language, and sensitive use cases. By examining the unique paths taken by SecLM and Med-PaLM, we provide insights into the potential of LLMs to revolutionize various areas of expertise.</p>"},{"location":"day_04/whitepaper/#read-the-whitepaper-below","title":"Read the whitepaper below","text":""},{"location":"day_05/","title":"Day 5 \u2013 MLOps for Generative AI","text":"<p>Adapt MLOps for Gen AI using tools like Vertex AI.</p> <p>Assignments:</p> <ol> <li> <p>Unit 5: \u201cMLOps for Generative AI\u201d </p> <ul> <li>\ud83c\udfa7 Podcast (Optional) </li> <li>\ud83d\udcc4 Whitepaper </li> <li>\ud83d\udca1 Walkthrough Resource</li> </ul> </li> <li> <p>\ud83c\udfa5 YouTube Livestream Recording</p> </li> </ol>"},{"location":"day_05/extras/","title":"Day 5 Livestream with Paige Bailey \u2013 5-Day Gen AI Intensive Course | Kaggle","text":""},{"location":"day_05/podcast/","title":"Whitepaper Companion Podcast - Operationalizing Generative AI on Vertex AI using MLOps","text":""},{"location":"day_05/whitepaper/","title":"Operationalizing Generative AI on Vertex AI using MLOps","text":"<p>Authors: Anant Nawalgaria, Gabriela Hernandez Larios, Elia Secchi, Mike Styer, Christos Aniftos, Onofrio Petragallo and Sokratis Kartakis </p>"},{"location":"day_05/whitepaper/#introduction","title":"Introduction","text":"<p>The emergence of foundation models and generative AI (gen AI) has introduced a new era for building AI systems. Selecting the right model from a diverse range of architectures and sizes, curating data, engineering optimal prompts, tuning models for specific tasks, grounding model outputs in real-world data, optimizing hardware - these are just a few of the novel challenges that large models introduce.</p> <p>This whitepaper delves into the fundamental tenets of MLOps and the necessary adaptations required for the domain of gen AI and Foundation Models. We also examine the diverse range of Vertex AI products, specifically tailored to address the unique demands of foundation models and gen AI-based applications. Through this exploration we uncover how Vertex AI, with its solid foundations of AI infrastructure and MLOps tools, expands its capabilities to provide a comprehensive MLOps platform for gen AI.</p>"},{"location":"day_05/whitepaper/#read-the-whitepaper-below","title":"Read the whitepaper below","text":""}]}